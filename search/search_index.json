{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>chromatic</code>","text":"<p>The <code>chromatic-lightcurves</code> package is a friendly python tool for working with spectroscopic light curves. Inspired by the <code>lightkurve</code> package, we aim for this tool to provide an easy way interact with datasets representing the brightness of a star as a function of both time and wavelength.</p> <p>This package defines a spectroscopic light curve object (a <code>Rainbow</code> = \ud83c\udf08) to provide easy access to wavelength, time, flux, and uncertainty attributes. It allows reading and writing these datasets from/to a variety of formats, simplifies many common calculations, and provides a plethora of visualizations. Read on for quick introductions to how these little \ud83c\udf08s work!</p>"},{"location":"actions/","title":"\ud83c\udf08 Actions","text":"<p>There are number of methods or actions that any <code>Rainbow</code> (=\ud83c\udf08) object can do. By learning the vocabulary of just a few of these methods, you can build up some fairly complicated stories for working with data. In general, most of these actions return a \ud83c\udf08 object that has been modified in one way or another, so you can keep adding actions after actions after actions. To show how these work, we'll create some simulated \ud83c\udf08 objects and try a few:</p> In\u00a0[1]: Copied! <pre>from chromatic import SimulatedRainbow, version\nfrom chromatic import plt, np, u\n\nplt.matplotlib.rcParams[\"figure.figsize\"] = (8, 3)\n</pre> from chromatic import SimulatedRainbow, version from chromatic import plt, np, u  plt.matplotlib.rcParams[\"figure.figsize\"] = (8, 3) In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> <p>By itself, the <code>SimulatedRainbow()</code> object just creates an empty \ud83c\udf08 with a particular wavelength and time grid. However, we can use a suite of actions that inject signals and/or noise sources to build of semi-realistic simulated datasets.</p> In\u00a0[3]: Copied! <pre>empty = SimulatedRainbow(wlim=[0.1, 5] * u.micron, tlim=[-0.1, 0.1] * u.day)\n</pre> empty = SimulatedRainbow(wlim=[0.1, 5] * u.micron, tlim=[-0.1, 0.1] * u.day) <p>It's useful to have a way to inject a simulated transit of an imaginary exoplanet. If you're trying out some new fitting algorithm, you can inject a transit with known properties and make sure you recover it.</p> In\u00a0[4]: Copied! <pre>with_transit = empty.inject_transit()\nwith_transit.paint();\n</pre> with_transit = empty.inject_transit() with_transit.paint(); <p>You can customize some of the model parameters. See the docstring for more details, but the most basic would be to provide an array of planet radii corresponding to each wavelength.</p> In\u00a0[5]: Copied! <pre>with_transit = empty.inject_transit(planet_radius=np.linspace(0.2, 0.1, empty.nwave))\nwith_transit.paint();\n</pre> with_transit = empty.inject_transit(planet_radius=np.linspace(0.2, 0.1, empty.nwave)) with_transit.paint(); <p>The parameters used for the transit model will be stored in <code>.metadata['injected_transit_parameters']</code>.</p> In\u00a0[6]: Copied! <pre>with_spectrum = with_transit.inject_spectrum(\n    temperature=5800 * u.K, logg=4.43, metallicity=0.0\n)\nwith_spectrum.paint();\n</pre> with_spectrum = with_transit.inject_spectrum(     temperature=5800 * u.K, logg=4.43, metallicity=0.0 ) with_spectrum.paint(); <pre>\ud83c\udf08\ud83e\udd16 None of the pre-existing flux values were 1,\nwhich hints at the possibility that there\nmight already be a spectrum in them. Please\nwatch out for weird units or values!\n\n</pre> <p>This function injects imaginary systematic noise sources into a \ud83c\udf08. High-precision observations of transiting exoplanets often encounter systematics that are correlated in complicated ways in time or wavelength or with various hidden quantities. This function injects a linear combination of real or imagined time-like, wave-like, and/or flux-like quantities (see its docstring for details).</p> In\u00a0[7]: Copied! <pre>with_systematics = with_spectrum.inject_systematics()\nwith_systematics.normalize().paint();\n</pre> with_systematics = with_spectrum.inject_systematics() with_systematics.normalize().paint(); <p>The parameters and equations used for the systematics model will be stored in <code>.metadata['systematics_components']</code> and <code>.metadata['systematics_equation']</code>. The independent variables needed for these equations are stored in the \ud83c\udf08, so it should be possible to perfectly recreate the systematic model.</p> <p>We can inject noise, for example to simulate the observing the same system at a greater distance or with a smaller telescope. One way to set the noise level is with the <code>signal_to_noise</code> keyword argument.</p> In\u00a0[8]: Copied! <pre>with_noise = with_systematics.inject_noise(signal_to_noise=100)\nwith_noise.normalize().paint();\n</pre> with_noise = with_systematics.inject_noise(signal_to_noise=100) with_noise.normalize().paint(); <p>Another way to set the noise level is to specify a typical number of photons expected per bin with the <code>number_of_photons</code> keyword argument.</p> In\u00a0[9]: Copied! <pre>with_noise = with_systematics.normalize().inject_noise(number_of_photons=1e4)\nwith_noise.paint();\n</pre> with_noise = with_systematics.normalize().inject_noise(number_of_photons=1e4) with_noise.paint(); <p>Since each \ud83c\udf08 action returns a \ud83c\udf08, multiple actions can be linked togther into a single command.</p> In\u00a0[10]: Copied! <pre>r = (\n    SimulatedRainbow()\n    .inject_transit()\n    .inject_spectrum()\n    .inject_systematics()\n    .inject_noise()\n    .normalize()\n)\n</pre> r = (     SimulatedRainbow()     .inject_transit()     .inject_spectrum()     .inject_systematics()     .inject_noise()     .normalize() ) <pre>\ud83c\udf08\ud83e\udd16 None of the pre-existing flux values were 1,\nwhich hints at the possibility that there\nmight already be a spectrum in them. Please\nwatch out for weird units or values!\n\n</pre> In\u00a0[11]: Copied! <pre>def summarize(x):\n    print(\n        f\"\"\"\n    {x} is a {type(x).__name__}.\n    It has a {x.nwave} wavelengths and {x.ntime} times. \n\n    Its 5 first wavelengths:{np.round(x.wavelength[:5], 2)}\n    Its 5 first times:{np.round(x.time[:5].to('hour'), 2)}\n    \"\"\"\n    )\n</pre> def summarize(x):     print(         f\"\"\"     {x} is a {type(x).__name__}.     It has a {x.nwave} wavelengths and {x.ntime} times.       Its 5 first wavelengths:{np.round(x.wavelength[:5], 2)}     Its 5 first times:{np.round(x.time[:5].to('hour'), 2)}     \"\"\"     ) In\u00a0[12]: Copied! <pre>summarize(r)\n</pre> summarize(r) <pre>\n    &lt;Simulated\ud83c\udf08(231w, 150t)&gt; is a SimulatedRainbow.\n    It has a 231 wavelengths and 150 times. \n\n    Its 5 first wavelengths:[0.5  0.51 0.51 0.52 0.52] micron\n    Its 5 first times:[-2.5  -2.47 -2.43 -2.4  -2.37] h\n    \n</pre> <p>To bin in wavelength, it can take the following inputs:</p> <ul> <li><code>dw=</code> to bin in wavelength to a particular $d\\lambda$ width. This will create a linear grid in wavelength.</li> <li><code>R=</code> to bin in wavelength to particular $R = \\lambda/d\\lambda$. This will create a logarithmic grid in wavelength.</li> <li><code>wavelength=</code> to bin to any custom wavelength grid specified by its centers; the edges will be guessed from the spacing between these edges.</li> <li><code>wavelength_edges=</code> to bin to any custom wavelength grid specified by its edges; the centers will be guessed as the midpoints between these edges. (The number of binned wavelengths will be 1 fewer than the number of edges.)</li> <li><code>nwavelengths=</code> to bin by a fixed number of adjacent wavelengths (as in \"bin every N wavelengths together\"), starting from the first wavelength.</li> </ul> In\u00a0[13]: Copied! <pre>b = r.bin(dw=0.5 * u.micron)\nsummarize(b)\n</pre> b = r.bin(dw=0.5 * u.micron) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(10w, 150t)&gt; is a SimulatedRainbow.\n    It has a 10 wavelengths and 150 times. \n\n    Its 5 first wavelengths:[0.5 1.  1.5 2.  2.5] micron\n    Its 5 first times:[-2.5  -2.47 -2.43 -2.4  -2.37] h\n    \n</pre> In\u00a0[14]: Copied! <pre>b = r.bin(R=10)\nsummarize(b)\n</pre> b = r.bin(R=10) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(24w, 150t)&gt; is a SimulatedRainbow.\n    It has a 24 wavelengths and 150 times. \n\n    Its 5 first wavelengths:[0.5  0.55 0.61 0.67 0.75] micron\n    Its 5 first times:[-2.5  -2.47 -2.43 -2.4  -2.37] h\n    \n</pre> In\u00a0[15]: Copied! <pre>b = r.bin(wavelength=np.linspace(1, 2, 6) * u.micron)\nsummarize(b)\n</pre> b = r.bin(wavelength=np.linspace(1, 2, 6) * u.micron) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(6w, 150t)&gt; is a SimulatedRainbow.\n    It has a 6 wavelengths and 150 times. \n\n    Its 5 first wavelengths:[1.  1.2 1.4 1.6 1.8] micron\n    Its 5 first times:[-2.5  -2.47 -2.43 -2.4  -2.37] h\n    \n</pre> In\u00a0[16]: Copied! <pre>b = r.bin(wavelength_edges=np.linspace(1, 2, 6) * u.micron)\nsummarize(b)\n</pre> b = r.bin(wavelength_edges=np.linspace(1, 2, 6) * u.micron) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(5w, 150t)&gt; is a SimulatedRainbow.\n    It has a 5 wavelengths and 150 times. \n\n    Its 5 first wavelengths:[1.1 1.3 1.5 1.7 1.9] micron\n    Its 5 first times:[-2.5  -2.47 -2.43 -2.4  -2.37] h\n    \n</pre> In\u00a0[17]: Copied! <pre>b = r.bin(nwavelengths=10)\nsummarize(b)\n</pre> b = r.bin(nwavelengths=10) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(23w, 150t)&gt; is a SimulatedRainbow.\n    It has a 23 wavelengths and 150 times. \n\n    Its 5 first wavelengths:[0.52 0.58 0.64 0.71 0.78] micron\n    Its 5 first times:[-2.5  -2.47 -2.43 -2.4  -2.37] h\n    \n</pre> <p>To bin in time, it can take the following inputs:</p> <ul> <li><code>dt=</code> to bin in time to a particular $dt$ width. This will create a linear grid in time.</li> <li><code>time=</code> to bin to any custom time grid specified by its centers; the edges will be guessed from the spacing between these edges.</li> <li><code>time_edges=</code> to bin to any custom time grid specified by its edges; the centers will be guessed as the midpoints between these edges. (The number of binned times will be 1 fewer than the number of edges.)</li> <li><code>ntimes=</code> to bin by a fixed number of adjacent times (as in \"bin every N times together\"), starting from the first time.</li> </ul> In\u00a0[18]: Copied! <pre>b = r.bin(dt=0.25 * u.hour)\nsummarize(b)\n</pre> b = r.bin(dt=0.25 * u.hour) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(231w, 21t)&gt; is a SimulatedRainbow.\n    It has a 231 wavelengths and 21 times. \n\n    Its 5 first wavelengths:[0.5  0.51 0.51 0.52 0.52] micron\n    Its 5 first times:[-2.5  -2.25 -2.   -1.75 -1.5 ] h\n    \n</pre> In\u00a0[19]: Copied! <pre>b = r.bin(time=np.linspace(-1, 1, 6) * u.hour)\nsummarize(b)\n</pre> b = r.bin(time=np.linspace(-1, 1, 6) * u.hour) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(231w, 6t)&gt; is a SimulatedRainbow.\n    It has a 231 wavelengths and 6 times. \n\n    Its 5 first wavelengths:[0.5  0.51 0.51 0.52 0.52] micron\n    Its 5 first times:[-1.  -0.6 -0.2  0.2  0.6] h\n    \n</pre> In\u00a0[20]: Copied! <pre>b = r.bin(time_edges=np.linspace(-1, 1, 6) * u.hour)\nsummarize(b)\n</pre> b = r.bin(time_edges=np.linspace(-1, 1, 6) * u.hour) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(231w, 5t)&gt; is a SimulatedRainbow.\n    It has a 231 wavelengths and 5 times. \n\n    Its 5 first wavelengths:[0.5  0.51 0.51 0.52 0.52] micron\n    Its 5 first times:[-0.8 -0.4  0.   0.4  0.8] h\n    \n</pre> In\u00a0[21]: Copied! <pre>b = r.bin(ntimes=10)\nsummarize(b)\n</pre> b = r.bin(ntimes=10) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(231w, 15t)&gt; is a SimulatedRainbow.\n    It has a 231 wavelengths and 15 times. \n\n    Its 5 first wavelengths:[0.5  0.51 0.51 0.52 0.52] micron\n    Its 5 first times:[-2.35 -2.02 -1.68 -1.35 -1.02] h\n    \n</pre> <p>You can combine to bin in both wavelength and time in the same step.</p> In\u00a0[22]: Copied! <pre>b = r.bin(dw=100 * u.nm, dt=10 * u.minute)\nsummarize(b)\n</pre> b = r.bin(dw=100 * u.nm, dt=10 * u.minute) summarize(b) <pre>\n    &lt;Simulated\ud83c\udf08(46w, 31t)&gt; is a SimulatedRainbow.\n    It has a 46 wavelengths and 31 times. \n\n    Its 5 first wavelengths:[0.5 0.6 0.7 0.8 0.9] micron\n    Its 5 first times:[-2.5  -2.33 -2.17 -2.   -1.83] h\n    \n</pre> In\u00a0[23]: Copied! <pre>fi, ax = plt.subplots(1, 2, sharex=True, figsize=(8, 3), constrained_layout=True)\nr.paint(ax=ax[0], vmin=0.98, vmax=1.02)\nplt.title(\"Unbinned\")\nplt.xlabel(\"\")\nb.paint(ax=ax[1], vmin=0.98, vmax=1.02)\nplt.title(\"Binned\");\n</pre> fi, ax = plt.subplots(1, 2, sharex=True, figsize=(8, 3), constrained_layout=True) r.paint(ax=ax[0], vmin=0.98, vmax=1.02) plt.title(\"Unbinned\") plt.xlabel(\"\") b.paint(ax=ax[1], vmin=0.98, vmax=1.02) plt.title(\"Binned\"); In\u00a0[24]: Copied! <pre>from astropy.time import Time\n\nr.time += Time.now().jd * u.day\n</pre> from astropy.time import Time  r.time += Time.now().jd * u.day In\u00a0[25]: Copied! <pre>r.time\n</pre> r.time Out[25]:  $[2460784.3,~2460784.3,~2460784.3,~\\dots,~2460784.5,~2460784.5,~2460784.5] \\; \\mathrm{d}$  In\u00a0[26]: Copied! <pre>r.fold(period=1.234 * u.day, t0=Time.now().jd * u.day).time\n</pre> r.fold(period=1.234 * u.day, t0=Time.now().jd * u.day).time Out[26]:  $[-0.10416674,~-0.10277785,~-0.10138896,~\\dots,~0.099999931,~0.10138882,~0.10277771] \\; \\mathrm{d}$  In\u00a0[27]: Copied! <pre>r.get_average_lightcurve_as_rainbow()\n</pre> r.get_average_lightcurve_as_rainbow() Out[27]: <pre>&lt;Simulated\ud83c\udf08(1w, 150t)&gt;</pre> In\u00a0[28]: Copied! <pre>r.get_average_spectrum_as_rainbow()\n</pre> r.get_average_spectrum_as_rainbow() Out[28]: <pre>&lt;Simulated\ud83c\udf08(231w, 1t)&gt;</pre> In\u00a0[29]: Copied! <pre>r.normalize().paint();\n</pre> r.normalize().paint(); <p>In some cases, you might also be curious to divide by the median light curve, to look for small variations away from the overall transit shape. You can customize whether you want to normalize in wavelength or time by suppyling the <code>axis=</code> keyword when you call the <code>.normalize()</code> function, which defaults to <code>axis='wavelength'</code>.</p> In\u00a0[30]: Copied! <pre>r.normalize(axis=\"time\").paint();\n</pre> r.normalize(axis=\"time\").paint(); <p>(In this case, there were no transit depth or limb-darkening variations injected into the simulated \ud83c\udf08, so normalizing through time entirely erases any hint of the transit.)</p> In\u00a0[31]: Copied! <pre>fi, ax = plt.subplots(2, 2, sharey=True, figsize=(8, 5), constrained_layout=True)\nr.paint(ax=ax[0, 0])\nplt.title(\"original\")\nr.remove_trends(method=\"median_filter\", size=(11, 5)).paint(ax=ax[0, 1])\nplt.title(\"median-filtered\")\nr.remove_trends(method=\"savgol_filter\", window_length=11, polyorder=2).paint(\n    ax=ax[1, 0]\n)\nplt.title(\"savgol-filtered\")\nr.remove_trends(method=\"differences\").paint(ax=ax[1, 1])\nplt.title(\"first-differences\");\n</pre> fi, ax = plt.subplots(2, 2, sharey=True, figsize=(8, 5), constrained_layout=True) r.paint(ax=ax[0, 0]) plt.title(\"original\") r.remove_trends(method=\"median_filter\", size=(11, 5)).paint(ax=ax[0, 1]) plt.title(\"median-filtered\") r.remove_trends(method=\"savgol_filter\", window_length=11, polyorder=2).paint(     ax=ax[1, 0] ) plt.title(\"savgol-filtered\") r.remove_trends(method=\"differences\").paint(ax=ax[1, 1]) plt.title(\"first-differences\"); In\u00a0[32]: Copied! <pre>unshifted = SimulatedRainbow(wlim=[470, 560] * u.nm, R=5000).inject_spectrum()\nshifted = unshifted.shift(300 * u.km / u.s)\nfor x, label in zip([unshifted, shifted], [\"unshifted\", \"shifted\"]):\n    plt.plot(x.wavelength, x.flux[:, 0], label=label)\nplt.xlabel(f\"Wavelength ({unshifted.wavelength.unit.to_string('latex_inline')})\")\nplt.ylabel(f\"Flux ({unshifted.flux.unit.to_string('latex_inline')})\")\nplt.legend(frameon=False);\n</pre> unshifted = SimulatedRainbow(wlim=[470, 560] * u.nm, R=5000).inject_spectrum() shifted = unshifted.shift(300 * u.km / u.s) for x, label in zip([unshifted, shifted], [\"unshifted\", \"shifted\"]):     plt.plot(x.wavelength, x.flux[:, 0], label=label) plt.xlabel(f\"Wavelength ({unshifted.wavelength.unit.to_string('latex_inline')})\") plt.ylabel(f\"Flux ({unshifted.flux.unit.to_string('latex_inline')})\") plt.legend(frameon=False); <pre>\n            Downloading pre-processed grid for R=10000, metallicity=0.0 from\n            https://casa.colorado.edu/~bertathompson/chromatic/phoenix_photons_metallicity=0.0_R=10000.npy\n            Because the resolution is R&gt;1000, this might be annoyingly slow.\n            \n</pre> <pre>\ud83c\udf08\ud83e\udd16 The progress bar for this download is not being shown\nbecause `astropy.utils.data.download_file` is being run\nfrom a jupyter notebook instead of from the terminal.\n\n</pre> In\u00a0[33]: Copied! <pre>r.wavelike[\"ok\"] = np.random.uniform(0, 1, r.nwave) &lt; 0.9\nr.wavelike[\"ok\"][:20] = False\n\nfi, ax = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\nr.trim().paint(ax=ax[1])\nr.paint(ax=ax[0]);\n</pre> r.wavelike[\"ok\"] = np.random.uniform(0, 1, r.nwave) &lt; 0.9 r.wavelike[\"ok\"][:20] = False  fi, ax = plt.subplots(1, 2, figsize=(8, 3), sharey=True) r.trim().paint(ax=ax[1]) r.paint(ax=ax[0]); In\u00a0[34]: Copied! <pre>a = SimulatedRainbow().inject_noise()\nb = SimulatedRainbow().inject_noise()\n</pre> a = SimulatedRainbow().inject_noise() b = SimulatedRainbow().inject_noise() In\u00a0[35]: Copied! <pre>(a + b).paint();\n</pre> (a + b).paint(); In\u00a0[36]: Copied! <pre>(a - b)\n</pre> (a - b) Out[36]: <pre>&lt;Simulated\ud83c\udf08(231w, 150t)&gt;</pre> In\u00a0[37]: Copied! <pre>(a * b)\n</pre> (a * b) Out[37]: <pre>&lt;Simulated\ud83c\udf08(231w, 150t)&gt;</pre> In\u00a0[38]: Copied! <pre>(a / b)\n</pre> (a / b) Out[38]: <pre>&lt;Simulated\ud83c\udf08(231w, 150t)&gt;</pre> In\u00a0[39]: Copied! <pre>a * 2 + b / 4 - 0.1 * a.model\n</pre> a * 2 + b / 4 - 0.1 * a.model Out[39]: <pre>&lt;Simulated\ud83c\udf08(231w, 150t)&gt;</pre> In\u00a0[40]: Copied! <pre># the original array\nsummarize(r)\n</pre> # the original array summarize(r) <pre>\n    &lt;Simulated\ud83c\udf08(231w, 150t)&gt; is a SimulatedRainbow.\n    It has a 231 wavelengths and 150 times. \n\n    Its 5 first wavelengths:[0.5  0.51 0.51 0.52 0.52] micron\n    Its 5 first times:[59058822.21 59058822.24 59058822.27 59058822.31 59058822.34] h\n    \n</pre> In\u00a0[41]: Copied! <pre># using slices\nsummarize(r[5:10, ::2])\n</pre> # using slices summarize(r[5:10, ::2]) <pre>\n    &lt;Simulated\ud83c\udf08(5w, 75t)&gt; is a SimulatedRainbow.\n    It has a 5 wavelengths and 75 times. \n\n    Its 5 first wavelengths:[0.53 0.53 0.54 0.54 0.55] micron\n    Its 5 first times:[59058822.21 59058822.27 59058822.34 59058822.41 59058822.47] h\n    \n</pre> In\u00a0[42]: Copied! <pre># using indices\ni_wavelengths = np.arange(5, 10)\ni_times = np.arange(0, r.ntime, 2)\nsummarize(r[i_wavelengths, i_times])\n</pre> # using indices i_wavelengths = np.arange(5, 10) i_times = np.arange(0, r.ntime, 2) summarize(r[i_wavelengths, i_times]) <pre>\n    &lt;Simulated\ud83c\udf08(5w, 75t)&gt; is a SimulatedRainbow.\n    It has a 5 wavelengths and 75 times. \n\n    Its 5 first wavelengths:[0.53 0.53 0.54 0.54 0.55] micron\n    Its 5 first times:[59058822.21 59058822.27 59058822.34 59058822.41 59058822.47] h\n    \n</pre> In\u00a0[43]: Copied! <pre># using boolean masks\nok_wavelengths = r.wavelength &lt; 2 * u.micron\nok_times = np.abs(r.time) &lt; 1 * u.hour\nsummarize(r[ok_wavelengths, ok_times])\n</pre> # using boolean masks ok_wavelengths = r.wavelength &lt; 2 * u.micron ok_times = np.abs(r.time) &lt; 1 * u.hour summarize(r[ok_wavelengths, ok_times]) <pre>\n    &lt;Simulated\ud83c\udf08(139w, 0t)&gt; is a SimulatedRainbow.\n    It has a 139 wavelengths and 0 times. \n\n    Its 5 first wavelengths:[0.5  0.51 0.51 0.52 0.52] micron\n    Its 5 first times:[] h\n    \n</pre> In\u00a0[44]: Copied! <pre>x = (\n    SimulatedRainbow()\n    .inject_noise()\n    .inject_transit()\n    .bin(R=5, dt=5 * u.minute)\n    .normalize()\n)\nh = x.history()\nprint(h)\n</pre> x = (     SimulatedRainbow()     .inject_noise()     .inject_transit()     .bin(R=5, dt=5 * u.minute)     .normalize() ) h = x.history() print(h) <pre>(\nSimulatedRainbow(\n   tlim=u.Quantity(np.array([-2.5,  2.5]))*u.Unit('h'),\n   dt=u.Quantity(np.float64(2.0))*u.Unit('min'),\n   wlim=u.Quantity(np.array([0.5, 5. ]))*u.Unit('micron'),\n   R=100)\n.inject_noise(\n   signal_to_noise=100)\n.inject_transit(\n   planet_radius=0.1,\n   method='exoplanet',\n   **{})\n.bin_in_time(\n   dt=u.Quantity(np.float64(5.0))*u.Unit('min'),\n   minimum_acceptable_ok=1,\n   trim=True)\n.trim_times(\n   just_edges=True,\n   when_to_give_up=1,\n   minimum_acceptable_ok=1)\n.bin_in_wavelength(\n   R=5,\n   minimum_acceptable_ok=1,\n   trim=True,\n   starting_wavelengths='1D')\n.trim_wavelengths(\n   just_edges=True,\n   when_to_give_up=1,\n   minimum_acceptable_ok=1)\n.normalize(\n   axis='wavelength',\n   percentile=50)\n)\n</pre> <p>By default, it gives an almost copy-paste-able string version of the history of the <code>Rainbow</code>. If you look closely, you'll see that <code>.bin</code> has gotten split out into four steps (<code>.bin_in_time</code>, <code>.trim_times</code>, <code>.bin_in_wavelength</code>, <code>.trim_wavelengths</code>). In many cases, you should be able to approximately reproduce the actions that have gone into a <code>Rainbow</code> by just copying, pasting, and running the set of commands returned by <code>.history()</code> (or running <code>eval</code> to evaluate a string as Python commands).</p> In\u00a0[45]: Copied! <pre>eval(h)\n</pre> eval(h) Out[45]: <pre>&lt;Simulated\ud83c\udf08(13w, 61t)&gt;</pre>"},{"location":"actions/#actions","title":"\ud83c\udf08 Actions\u00b6","text":""},{"location":"actions/#inject_transit","title":"<code>\ud83c\udf08.inject_transit()</code>\u00b6","text":""},{"location":"actions/#inject_spectrum","title":"<code>\ud83c\udf08.inject_spectrum()</code>\u00b6","text":"<p>This function injects a static stellar spectrum into all the fluxes for a \ud83c\udf08, using the Husser et al. (2013) PHOENIX model grid. It will try to automatically download all the files you need, when you need them.</p>"},{"location":"actions/#inject_systematics","title":"<code>\ud83c\udf08.inject_systematics()</code>\u00b6","text":""},{"location":"actions/#inject_noise","title":"<code>\ud83c\udf08.inject_noise()</code>\u00b6","text":""},{"location":"actions/#bin","title":"<code>\ud83c\udf08.bin()</code>\u00b6","text":"<p>While we should generally try to avoid fitting to binned data when possible, there will often be times where it's helpful to bin to particular grid of wavelengths and/or times. You can do this using the <code>.bin()</code> function, which we'll apply here to the simulated \ud83c\udf08 we just created.</p>"},{"location":"actions/#fold","title":"<code>\ud83c\udf08.fold()</code>\u00b6","text":"<p>If our times are in units of BJD, it might be helpful to phase-fold them to a particular transit period and epoch. This function is a small wrapper to convert time to being measured relative to the center of some periodic event (like a transit).</p>"},{"location":"actions/#get_average_lightcurve_as_rainbow","title":"<code>\ud83c\udf08.get_average_lightcurve_as_rainbow()</code>\u00b6","text":"<p>Binning all wavelengths together produces an uncertainty-weighted average light curve.</p>"},{"location":"actions/#get_average_spectrum_as_rainbow","title":"<code>\ud83c\udf08.get_average_spectrum_as_rainbow()</code>\u00b6","text":"<p>Binning all times together produces an uncertainty-weighted average spectrum.</p>"},{"location":"actions/#normalize","title":"<code>\ud83c\udf08.normalize()</code>\u00b6","text":"<p>If we're starting in units of photons detected at our telescope per wavelength, we may want to normalize a \ud83c\udf08 by dividing through by its median spectrum. That's effectively all that the <code>.normalize()</code> action does.</p>"},{"location":"actions/#remove_trends","title":"<code>\ud83c\udf08.remove_trends()</code>\u00b6","text":"<p>Often we need a quick way to remove smooth trends from a \ud83c\udf08.</p>"},{"location":"actions/#shift","title":"<code>\ud83c\udf08.shift()</code>\u00b6","text":"<p>This function Doppler shifts all the wavelengths in a \ud83c\udf08 by a given velocity.</p>"},{"location":"actions/#trim","title":"<code>\ud83c\udf08.trim()</code>\u00b6","text":"<p>Often datasets may have blocks of wavelengths or times that are entirely bad. This function trims bad wavelengths or times off the edges of a \ud83c\udf08.</p>"},{"location":"actions/","title":"<code>\ud83c\udf08 + \ud83c\udf08</code>\u00b6","text":"<p>We can perform mathematical operations on \ud83c\udf08 objects. This will apply the requested mathematical operation between the <code>flux</code> and <code>model</code> arrays, and try its best to figure out what to do with the <code>uncertainty</code>.</p>"},{"location":"actions/","title":"<code>\ud83c\udf08[:,:]</code>\u00b6","text":"<p>We can trim a Rainbow in wavelength (first dimension) and/or time (second dimension) by slicing it in a similar way that you might to any other 2D array.</p>"},{"location":"actions/#viewing-a-s-history","title":"Viewing a \ud83c\udf08's History\u00b6","text":"<p>Most actions that return <code>Rainbow</code> objects be recorded in that object's <code>metadata['history']</code> entry. This is meant to be an approximate summary of the steps that led up to the creation of the current \ud83c\udf08. You can view this history by calling the <code>.history()</code> method.</p>"},{"location":"api/","title":"Reference","text":"<p>A friendly wrapper to load time-series spectra and/or multiwavelength light curves into a <code>chromatic</code> Rainbow object. It will try its best to pick the best reader and return the most useful kind of object. \ud83e\udd8b\ud83c\udf052\ufe0f\u20e3\ud83e\ude9c\ud83c\udfac\ud83d\udc40\ud83c\uddee\ud83c\uddf9\ud83d\udcd5\ud83e\uddd1\u200d\ud83c\udfeb\ud83c\udf08</p> <p><code>Rainbow</code> (\ud83c\udf08) objects represent brightness as a function of both wavelength and time.</p> <p>These objects are useful for reading or writing multiwavelength time-series datasets in a variety of formats, visualizing these data with simple commands, and performing basic calculations. <code>RainbowWithModel</code> and <code>SimulatedRainbow</code> objects inherit from <code>Rainbow</code>, so all basically all methods and attributes described below are available for them too.</p> <p>               Bases: <code>Rainbow</code></p> <p><code>RainbowWithModel</code> objects have a fluxlike <code>model</code> attached to them, meaning that they can</p> <p>This class definition inherits from <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/withmodel.py</code> <pre><code>class RainbowWithModel(Rainbow):\n    \"\"\"\n    `RainbowWithModel` objects have a fluxlike `model`\n    attached to them, meaning that they can\n\n    This class definition inherits from `Rainbow`.\n    \"\"\"\n\n    # which fluxlike keys will respond to math between objects\n    _keys_that_respond_to_math = [\"flux\", \"model\"]\n\n    # which keys get uncertainty weighting during binning\n    _keys_that_get_uncertainty_weighting = [\"flux\", \"model\", \"uncertainty\"]\n\n    @property\n    def residuals(self):\n        \"\"\"\n        Calculate the residuals on the fly,\n        to make sure they're always up to date.\n\n        The residuals are calculated simply\n        as the `.flux` - `.model`, so they are\n        in whatever units those arrays have.\n\n        Returns\n        -------\n        residuals : array, Quantity\n            The 2D array of residuals (nwave, ntime).\n        \"\"\"\n        return self.flux - self.model\n\n    @property\n    def chi_squared(self):\n        \"\"\"\n        Calculate chi-squared.\n\n        This calculates the sum of the squares of\n        the uncertainty-normalized residuals,\n        sum(((flux - model)/uncertainty)**2)\n\n        Data points marked as not OK are ignored.\n\n        Returns\n        -------\n        chi_squared : float\n            The chi-squared value.\n        \"\"\"\n        r = (self.flux - self.model) / self.uncertainty\n        return np.sum(r[self.ok] ** 2)\n\n    @property\n    def residuals_plus_one(self):\n        \"\"\"\n        A tiny wrapper to get the residuals + 1.\n\n        Returns\n        -------\n        residuals_plus_one : array, Quantity\n            The 2D array of residuals + 1 (nwave, ntime).\n        \"\"\"\n        return self.flux - self.model + 1\n\n    @property\n    def ones(self):\n        \"\"\"\n        Generate an array of ones that looks like the flux.\n        (A tiny wrapper needed for `plot_with_model`)\n\n        Returns\n        -------\n        ones : array, Quantity\n            The 2D array ones (nwave, ntime).\n        \"\"\"\n        return np.ones_like(self.flux)\n\n    def _validate_core_dictionaries(self):\n        super()._validate_core_dictionaries()\n        try:\n            model = self.get(\"model\")\n            assert np.shape(model) == np.shape(self.flux)\n        except (AttributeError, AssertionError):\n            message = \"\"\"\n            No fluxlike 'model' was found attached to this\n            `RainbowWithModel` object. The poor thing,\n            its name is a lie! Please connect a model.\n            The simplest way to do so might look like...\n            `rainbow.model = np.ones(rainbow.shape)`\n            ...or similarly with a more interesting array.\n            \"\"\"\n            cheerfully_suggest(message)\n\n    from .visualizations import (\n        plot_with_model,\n        plot_with_model_and_residuals,\n        paint_with_models,\n        plot_one_wavelength_with_models,\n        animate_with_models,\n    )\n</code></pre> <p>               Bases: <code>RainbowWithModel</code></p> <p><code>SimulatedRainbow</code> objects are created from scratch within <code>chromatic</code>, with options for various different wavelength grids, time grids, noise sources, and injected models. They can be useful for generating quick simulated dataset for testing analysis and visualization tools.</p> <p>This class definition inherits from <code>RainbowWithModel</code>, which itself inherits from <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/simulated.py</code> <pre><code>class SimulatedRainbow(RainbowWithModel):\n    \"\"\"\n    `SimulatedRainbow` objects are created from scratch\n    within `chromatic`, with options for various different\n    wavelength grids, time grids, noise sources, and injected\n    models. They can be useful for generating quick simulated\n    dataset for testing analysis and visualization tools.\n\n    This class definition inherits from `RainbowWithModel`,\n    which itself inherits from `Rainbow`.\n    \"\"\"\n\n    def __init__(\n        self,\n        tlim=[-2.5, 2.5] * u.hour,\n        dt=2 * u.minute,\n        time=None,\n        wlim=[0.5, 5] * u.micron,\n        R=100,\n        dw=None,\n        wavelength=None,\n        star_flux=None,\n        name=None,\n        signal_to_noise=None,\n    ):\n        \"\"\"\n        Initialize a `SimulatedRainbow` object from some parameters.\n\n        This sets up an effectively empty `Rainbow` with defined\n        wavelengths and times. For making more interesting\n        simulated datasets, this will often be paired with\n        some combination of the `.inject...` actions that inject\n        various astrophysical, instrumental, or noise signatures\n        into the dataset.\n\n        The time-setting order of precendence is:\n            1) time\n            2) tlim + dt\n\n        The wavelength-setting order of precendence is:\n            1) wavelength\n            2) wlim + dw\n            3) wlim + R\n\n        Parameters\n        ----------\n        tlim : list or Quantity\n            The pip install -e '.[develop]'[min, max] times for creating the time grid.\n            These should have astropy units of time.\n        dt : Quantity\n            The d(time) bin size for creating a grid\n            that is uniform in linear space.\n        time : Quantity\n            An array of times, if you just want to give\n            it an entirely custom array.\n        wlim : list or Quantity\n            The [min, max] wavelengths for creating the grid.\n            These should have astropy units of wavelength.\n        R : float\n            The spectral resolution for creating a grid\n            that is uniform in logarithmic space.\n        dw : Quantity\n            The d(wavelength) bin size for creating a grid\n            that is uniform in linear space.\n        wavelength : Quantity\n            An array of wavelengths, if you just want to give\n            it an entirely custom array.\n        star_flux : numpy 1D array\n            An array of fluxes corresponding to the supplied wavelengths.\n            If left blank, the code assumes a normalized flux of\n            flux(wavelength) = 1 for all wavelengths.\n        \"\"\"\n        Rainbow.__init__(self)\n\n        # (remove the history entry from creating the Rainbow)\n        self._remove_last_history_entry()\n\n        # create a history entry for this action (before other variables are defined)\n        h = self._create_history_entry(\"SimulatedRainbow\", locals())\n\n        # set up the wavelength grid\n        self._setup_fake_wavelength_grid(wlim=wlim, R=R, dw=dw, wavelength=wavelength)\n\n        # set up the time grid\n        self._setup_fake_time_grid(tlim=tlim, dt=dt, time=time)\n\n        # save the basic inputs that aren't stored elsewhere\n        self.metadata[\"name\"] = name\n\n        # If the flux of the star is not given,\n        # assume a continuum-normlized flux where fx=1 at all wavelengths.\n        if star_flux is None:\n            model = np.ones(self.shape)\n\n        # If the flux vs wavelength of the star is supplied,\n        # include it in the model.\n        else:\n            # Check to make sure the flux and wavelengths\n            # have the same shape.\n            if len(star_flux) == len(self.wavelike[\"wavelength\"]):\n                model = np.transpose([star_flux] * self.shape[1])\n            elif len(star_flux) == 1:\n                model = star_flux * np.ones(self.shape)\n\n        # Set uncertainty.\n        self.fluxlike[\"flux\"] = model * 1\n        self.fluxlike[\"model\"] = model * 1\n        self.fluxlike[\"uncertainty\"] = np.zeros(self.shape)\n\n        # make sure everything is defined and sorted\n        self._validate_core_dictionaries()\n\n        if signal_to_noise is not None:\n            message = f\"\"\"\n            You tried to specify the noise level with\n            `SimulatedRainbow(signal_to_noise={signal_to_noise})`,\n            but that functionality is going away soon.\n            Please replace it right now with\n            `SimulatedRainbow().inject_noise(signal_to_noise={signal_to_noise})`\n            so that your code will continue to work.\n            You're getting away with it this time,\n            but it won't work for much longer!\n            \"\"\"\n            cheerfully_suggest(message)\n            new = self.inject_noise()\n            for k in [\"flux\", \"uncertainty\", \"model\"]:\n                self.fluxlike[k] = new.fluxlike[k]\n\n        # append the history entry to the new Rainbow\n        self._record_history_entry(h)\n\n    def _setup_fake_time_grid(\n        self, tlim=[-2.5 * u.hour, 2.5 * u.hour], dt=1 * u.minute, time=None\n    ):\n        \"\"\"\n        Create a fake time grid.\n\n        Parameters\n        ----------\n\n        tlim : list or Quantity\n            The [min, max] times for creating the time grid.\n            These should have astropy units of time.\n        dt : Quantity\n            The d(time) bin size for creating a grid\n            that is uniform in linear space.\n        time : Quantity\n            An array of times, if you just want to give\n            it an entirely custom array.\n\n        The time-setting order of precendence is:\n            1) time\n            2) tlim + dt\n        \"\"\"\n        # check we're trying to do exactly one thing\n        if (tlim is None) and (time is None):\n            raise RuntimeError(\"Please specify either `tlim` or `time`.\")\n\n        if time is None:\n            t_unit = tlim[0].unit\n            t_unit.to(\"s\")\n            time = np.arange(tlim[0] / t_unit, tlim[1] / t_unit, dt / t_unit) * t_unit\n        else:\n            t_unit = time.unit\n\n        self.timelike[\"time\"] = u.Quantity(time).to(u.day)\n        # TODO, make this match up better with astropy time\n\n        self._guess_tscale()\n\n    def _setup_fake_wavelength_grid(\n        self, wlim=[0.5 * u.micron, 5 * u.micron], R=100, dw=None, wavelength=None\n    ):\n        \"\"\"\n        Create a fake wavelength grid.\n\n        Parameters\n        ----------\n\n        wlim : list or Quantity\n            The [min, max] wavelengths for creating the grid.\n            These should have astropy units of wavelength.\n        R : float\n            The spectral resolution for creating a grid\n            that is uniform in logarithmic space.\n        dw : Quantity\n            The d(wavelength) bin size for creating a grid\n            that is uniform in linear space.\n        wavelength : Quantity\n            An array of wavelengths, if you just want to give\n            it an entirely custom array.\n\n        The wavelength-setting order of precendence is:\n            1) wavelength\n            2) wlim + dw\n            3) wlim + R\n        \"\"\"\n\n        # check we're trying to do exactly one thing\n        if (wlim is None) and (wavelength is None):\n            raise RuntimeError(\"Please specify either `wlim` or `wavelength`.\")\n\n        # create a linear or logarithmic grid\n        if wavelength is None:\n            # check that we're\n            if (R is None) and (dw is None):\n                raise RuntimeError(\"Please specify either `R` or `dw`.\")\n\n            w_unit = wlim[0].unit\n            if dw is None:\n                self.metadata[\"R\"] = R\n                # self.metadata[\"wscale\"] = \"log\"\n\n                logw_min = np.log(wlim[0] / w_unit)\n                logw_max = np.log(wlim[1] / w_unit)\n                logw = np.arange(logw_min, logw_max, 1 / R)\n                wavelength = np.exp(logw) * w_unit\n\n            elif dw is not None:\n                self.metadata[\"dw\"] = dw\n                # self.metadata[\"wscale\"] = \"linear\"\n                wavelength = (\n                    np.arange(wlim[0] / w_unit, wlim[1] / w_unit, self.dw / w_unit)\n                    * w_unit\n                )\n\n        # or just make sure the wavelength grid has units\n        elif wavelength is not None:\n            w_unit = wavelength.unit\n\n        # make sure the wavelength array has units\n        self.wavelike[\"wavelength\"] = u.Quantity(wavelength).to(u.micron)\n        self._guess_wscale()\n</code></pre>"},{"location":"api/#chromatic.rainbows.read_rainbow--parameters","title":"Parameters","text":"<p>filepath : str, list     The file or files to open. **kw : dict     All other keyword arguments will be passed to     the <code>Rainbow</code> initialization.</p>"},{"location":"api/#chromatic.rainbows.read_rainbow--returns","title":"Returns","text":"<p>rainbow : Rainbow, RainbowWithModel     The loaded data!</p> Source code in <code>chromatic/rainbows/__init__.py</code> <pre><code>def read_rainbow(filepath, **kw):\n    \"\"\"\n    A friendly wrapper to load time-series spectra and/or\n    multiwavelength light curves into a `chromatic` Rainbow\n    object. It will try its best to pick the best reader\n    and return the most useful kind of object.\n    \ud83e\udd8b\ud83c\udf052\ufe0f\u20e3\ud83e\ude9c\ud83c\udfac\ud83d\udc40\ud83c\uddee\ud83c\uddf9\ud83d\udcd5\ud83e\uddd1\u200d\ud83c\udfeb\ud83c\udf08\n\n    Parameters\n    ----------\n    filepath : str, list\n        The file or files to open.\n    **kw : dict\n        All other keyword arguments will be passed to\n        the `Rainbow` initialization.\n\n    Returns\n    -------\n    rainbow : Rainbow, RainbowWithModel\n        The loaded data!\n    \"\"\"\n    r = Rainbow(filepath, **kw)\n    if \"model\" in r.fluxlike:\n        return RainbowWithModel(**r._get_core_dictionaries())\n    else:\n        return r\n</code></pre>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow--attributes","title":"Attributes","text":"<p>wavelike : dict     A dictionary for quantities with shape <code>(nwave,),     for which there's one value for each wavelength. timelike : dict     A dictionary for quantities with shape</code>(ntime,),     for which there's one value for each time. fluxlike : dict     A dictionary for quantities with shape <code>(nwave,ntime),     for which there's one value for each wavelength and time. metadata : dict     A dictionary containing all other useful information     that should stay connected to the</code>Rainbow<code>, in any format. wavelength : Quantity     The 1D array of wavelengths for this</code>Rainbow<code>.     (This is a property, not an actual attribute.) time : Quantity     The 1D array of times for this</code>Rainbow<code>.     (This is a property, not an actual attribute.) flux : array, Quantity     The 2D array of fluxes for this</code>Rainbow<code>.     (This is a property, not an actual attribute.) uncertainty : array, Quantity     The 2D array of flux uncertainties for this</code>Rainbow<code>.     (This is a property, not an actual attribute.) ok : array     The 2D array of \"ok-ness\" for this</code>Rainbow<code>.     (This is a property, not an actual attribute.) shape : tuple     The shape of this</code>Rainbow<code>'s flux array.     (This is a property, not an actual attribute.) nwave : int     The number of wavelengths in this</code>Rainbow<code>'.     (This is a property, not an actual attribute.) ntime : int     The number of times in this</code>Rainbow<code>'.     (This is a property, not an actual attribute.) nflux : int     The total number of fluxes in this</code>Rainbow<code>' (=</code>nwave*ntime<code>).     (This is a property, not an actual attribute.) dt : Quantity     The typical time offset between adjacent times in this</code>Rainbow<code>.     (This is a property, not an actual attribute.) name : array, Quantity     The name of this</code>Rainbow`, if one has been set.     (This is a property, not an actual attribute.)</p> Source code in <code>chromatic/rainbows/rainbow.py</code> <pre><code>class Rainbow:\n    \"\"\"\n    `Rainbow` (\ud83c\udf08) objects represent brightness as a function\n    of both wavelength and time.\n\n    These objects are useful for reading or writing multiwavelength\n    time-series datasets in a variety of formats, visualizing these\n    data with simple commands, and performing basic calculations.\n    `RainbowWithModel` and `SimulatedRainbow` objects inherit from\n    `Rainbow`, so all basically all methods and attributes described\n    below are available for them too.\n\n    Attributes\n    ----------\n    wavelike : dict\n        A dictionary for quantities with shape `(nwave,),\n        for which there's one value for each wavelength.\n    timelike : dict\n        A dictionary for quantities with shape `(ntime,),\n        for which there's one value for each time.\n    fluxlike : dict\n        A dictionary for quantities with shape `(nwave,ntime),\n        for which there's one value for each wavelength and time.\n    metadata : dict\n        A dictionary containing all other useful information\n        that should stay connected to the `Rainbow`, in any format.\n    wavelength : Quantity\n        The 1D array of wavelengths for this `Rainbow`.\n        (This is a property, not an actual attribute.)\n    time : Quantity\n        The 1D array of times for this `Rainbow`.\n        (This is a property, not an actual attribute.)\n    flux : array, Quantity\n        The 2D array of fluxes for this `Rainbow`.\n        (This is a property, not an actual attribute.)\n    uncertainty : array, Quantity\n        The 2D array of flux uncertainties for this `Rainbow`.\n        (This is a property, not an actual attribute.)\n    ok : array\n        The 2D array of \"ok-ness\" for this `Rainbow`.\n        (This is a property, not an actual attribute.)\n    shape : tuple\n        The shape of this `Rainbow`'s flux array.\n        (This is a property, not an actual attribute.)\n    nwave : int\n        The number of wavelengths in this `Rainbow`'.\n        (This is a property, not an actual attribute.)\n    ntime : int\n        The number of times in this `Rainbow`'.\n        (This is a property, not an actual attribute.)\n    nflux : int\n        The total number of fluxes in this `Rainbow`' (= `nwave*ntime`).\n        (This is a property, not an actual attribute.)\n    dt : Quantity\n        The typical time offset between adjacent times in this `Rainbow`.\n        (This is a property, not an actual attribute.)\n    name : array, Quantity\n        The name of this `Rainbow`, if one has been set.\n        (This is a property, not an actual attribute.)\n    \"\"\"\n\n    # all Rainbows must contain these core dictionaries\n    _core_dictionaries = [\"fluxlike\", \"timelike\", \"wavelike\", \"metadata\"]\n\n    # define which axis is which\n    waveaxis = 0\n    timeaxis = 1\n\n    # which fluxlike keys will respond to math between objects\n    _keys_that_respond_to_math = [\"flux\"]\n\n    # which keys get uncertainty weighting during binning\n    _keys_that_get_uncertainty_weighting = [\"flux\", \"uncertainty\"]\n\n    def __init__(\n        self,\n        filepath=None,\n        format=None,\n        wavelength=None,\n        time=None,\n        flux=None,\n        uncertainty=None,\n        wavelike=None,\n        timelike=None,\n        fluxlike=None,\n        metadata=None,\n        name=None,\n        **kw,\n    ):\n        \"\"\"\n        Initialize a `Rainbow` object.\n\n        The `__init__` function is called when a new `Rainbow` is\n        instantiated as `r = Rainbow(some, kinds, of=inputs)`.\n\n        The options for inputs are flexible, including the possibility\n        to initialize from a file, from arrays with appropriate units,\n        from dictionaries with appropriate ingredients, or simply as\n        an empty object if no arguments are given.\n\n        Parameters\n        ----------\n        filepath : str, optional\n            The filepath pointing to the file or group of files\n            that should be read.\n        format : str, optional\n            The file format of the file to be read. If None,\n            the format will be guessed automatically from the\n            filepath.\n        wavelength : Quantity, optional\n            A 1D array of wavelengths, in any unit.\n        time : Quantity, Time, optional\n            A 1D array of times, in any unit.\n        flux : array, optional\n            A 2D array of flux values.\n        uncertainty : array, optional\n            A 2D array of uncertainties, associated with the flux.\n        wavelike : dict, optional\n            A dictionary containing 1D arrays with the same\n            shape as the wavelength axis. It must at least\n            contain the key 'wavelength', which should have\n            astropy units of wavelength associated with it.\n        timelike : dict, optional\n            A dictionary containing 1D arrays with the same\n            shape as the time axis. It must at least\n            contain the key 'time', which should have\n            astropy units of time associated with it.\n        fluxlike : dict, optional\n            A dictionary containing 2D arrays with the shape\n            of (nwave, ntime), like flux. It must at least\n            contain the key 'flux'.\n        metadata : dict, optional\n            A dictionary containing all other metadata\n            associated with the dataset, generally lots of\n            individual parameters or comments.\n        **kw : dict, optional\n            Additional keywords will be passed along to\n            the function that initializes the rainbow.\n            If initializing from arrays (`time=`, `wavelength=`,\n            ...), these keywords will be interpreted as\n            additional arrays that should be sorted by their\n            shape into the appropriate dictionary. If\n            initializing from files, the keywords will\n            be passed on to the reader.\n\n        Examples\n        --------\n        Initialize from a file. While this works, a more robust\n        solution is probably to use `read_rainbow`, which will\n        automatically choose the best of `Rainbow` and `RainbowWithModel`\n        ```\n        r1 = Rainbow('my-neat-file.abc', format='abcdefgh')\n        ```\n\n        Initalize from arrays. The wavelength and time must have\n        appropriate units, and the shape of the flux array must\n        match the size of the wavelength and time arrays. Other\n        arrays that match the shape of any of these quantities\n        will be stored in the appropriate location. Other inputs\n        not matching any of these will be stored as `metadata.`\n        ```\n        r2 = Rainbow(\n                wavelength=np.linspace(1, 5, 50)*u.micron,\n                time=np.linspace(-0.5, 0.5, 100)*u.day,\n                flux=np.random.normal(0, 1, (50, 100)),\n                some_other_array=np.ones((50,100)),\n                some_metadata='wow!'\n        )\n        ```\n        Initialize from dictionaries. The dictionaries must contain\n        at least `wavelike['wavelength']`, `timelike['time']`, and\n        `fluxlike['flux']`, but any other additional inputs can be\n        provided.\n        ```\n        r3 = Rainbow(\n                wavelike=dict(wavelength=np.linspace(1, 5, 50)*u.micron),\n                timelike=dict(time=np.linspace(-0.5, 0.5, 100)*u.day),\n                fluxlike=dict(flux=np.random.normal(0, 1, (50, 100)))\n        )\n        ```\n        \"\"\"\n        # create a history entry for this action (before other variables are defined)\n        h = self._create_history_entry(\"Rainbow\", locals())\n\n        # metadata are arbitrary types of information we need\n        self.metadata = {\"name\": name}\n\n        # wavelike quanities are 1D arrays with nwave elements\n        self.wavelike = {}\n\n        # timelike quantities are 1D arrays with ntime elements\n        self.timelike = {}\n\n        # fluxlike quantities are 2D arrays with nwave x time elements\n        self.fluxlike = {}\n\n        # try to intialize from the exact dictionaries needed\n        if (\n            (type(wavelike) == dict)\n            and (type(timelike) == dict)\n            and (type(fluxlike) == dict)\n        ):\n            self._initialize_from_dictionaries(\n                wavelike=wavelike,\n                timelike=timelike,\n                fluxlike=fluxlike,\n                metadata=metadata,\n            )\n        # then try to initialize from arrays\n        elif (wavelength is not None) and (time is not None) and (flux is not None):\n            self._initialize_from_arrays(\n                wavelength=wavelength,\n                time=time,\n                flux=flux,\n                uncertainty=uncertainty,\n                **kw,\n            )\n            if metadata is not None:\n                self.metadata.update(**metadata)\n        # then try to initialize from a file\n        elif isinstance(filepath, (str, list, Column)):\n            self._initialize_from_file(filepath=filepath, format=format, **kw)\n\n        # finally, tidy up by guessing the scales\n        self._guess_wscale()\n        self._guess_tscale()\n\n        # append the history entry to this Rainbow\n        self._setup_history()\n        self._record_history_entry(h)\n\n    def _sort(self):\n        \"\"\"\n        Sort the wavelengths and times, from lowest to highest.\n        Attach the unsorted indices to be able to work backwards.\n        This sorts the object in-place (not returning a new Rainbow.)\n\n        Returns\n        -------\n        sorted : Rainbow\n            The sorted Rainbow.\n        \"\"\"\n\n        # figure out the indices to sort from low to high\n        i_wavelength = np.argsort(self.wavelength)\n        i_time = np.argsort(self.time)\n\n        if np.shape(self.flux) != (len(i_wavelength), len(i_time)):\n            message = \"\"\"\n            Wavelength, time, and flux arrays don't match;\n            the `._sort()` step is being skipped.\n            \"\"\"\n            cheerfully_suggest(message)\n            return\n\n        if np.any(np.diff(i_wavelength) &lt; 0):\n            message = f\"\"\"\n            The {self.nwave} input wavelengths were not monotonically increasing.\n            {self} has been sorted from lowest to highest wavelength.\n            If you want to recover the original wavelength order, the original\n            wavelength indices are available in `rainbow.original_wave_index`.\n            \"\"\"\n            cheerfully_suggest(message)\n\n        if np.any(np.diff(i_time) &lt; 0):\n            message = f\"\"\"\n            The {self.ntime} input times were not monotonically increasing.\n            {self} has been sorted from lowest to highest time.\n            If you want to recover the original time order, the original\n            time indices are available in `rainbow.original_time_index`.\n            \"\"\"\n            cheerfully_suggest(message)\n\n        # attach unsorted indices to this array, if the don't exist\n        if \"original_wave_index\" not in self.wavelike:\n            self.wavelike[\"original_wave_index\"] = np.arange(self.nwave)\n        if \"original_time_index\" not in self.timelike:\n            self.timelike[\"original_time_index\"] = np.arange(self.ntime)\n\n        # sort that copy by wavelength and time\n        for k in self.wavelike:\n            if self.wavelike[k] is not None:\n                self.wavelike[k] = self.wavelike[k][i_wavelength]\n        for k in self.timelike:\n            if self.timelike[k] is not None:\n                self.timelike[k] = self.timelike[k][i_time]\n        for k in self.fluxlike:\n            if self.fluxlike[k] is not None:\n                wave_sorted = self.fluxlike[k][i_wavelength, :]\n                self.fluxlike[k][:, :] = wave_sorted[:, i_time]\n\n    def _validate_uncertainties(self):\n        \"\"\"\n        Do some checks on the uncertainty values.\n        \"\"\"\n        if self.uncertainty is None and len(self.fluxlike) &gt; 0:\n            message = f\"\"\"\n            Hmmm...it's not clear which column corresponds to the\n            flux uncertainties for this Rainbow object. The\n            available `fluxlike` columns are:\n                {self.fluxlike.keys()}\n            A long-term solution might be to fix the `from_?!?!?!?`\n            reader, but a short-term solution would be to pick one\n            of the columns listed above and say something like\n\n            x.fluxlike['uncertainty'] = x.fluxlike['some-other-relevant-error-column']\n\n            where `x` is the Rainbow you just created.\n            \"\"\"\n            cheerfully_suggest(message)\n            return\n\n        # kludge to replace zero uncertainties\n        # if np.all(self.uncertainty == 0):\n        #    cheerfully_suggest(\"\\nUncertainties were all 0, replacing them with 1!\")\n        #        self.fluxlike[\"uncertainty\"] = np.ones_like(self.flux)\n\n    def _initialize_from_dictionaries(\n        self, wavelike={}, timelike={}, fluxlike={}, metadata={}\n    ):\n        \"\"\"\n        Populate from dictionaries in the correct format.\n\n        Parameters\n        ----------\n        wavelike : dict\n            A dictionary containing 1D arrays with the same\n            shape as the wavelength axis. It must at least\n            contain the key 'wavelength', which should have\n            astropy units of wavelength associated with it.\n        timelike : dict\n            A dictionary containing 1D arrays with the same\n            shape as the time axis. It must at least\n            contain the key 'time', which should have\n            astropy units of time associated with it.\n        fluxlike : dict\n            A dictionary containing 2D arrays with the shape\n            of (nwave, ntime), like flux. It must at least\n            contain the key 'flux'.\n        metadata : dict\n            A dictionary containing all other metadata\n            associated with the dataset, generally lots of\n            individual parameters or comments.\n        \"\"\"\n\n        # update the three core dictionaries of arrays\n        for k in wavelike:\n            self.wavelike[k] = wavelike[k] * 1\n        for k in timelike:\n            self.timelike[k] = timelike[k] * 1\n        for k in fluxlike:\n            self.fluxlike[k] = fluxlike[k] * 1\n        # multiplying by 1 is a kludge to prevent accidental links\n\n        # update the metadata\n        self.metadata.update(**metadata)\n\n        # validate that something reasonable got populated\n        self._validate_core_dictionaries()\n\n    def _get_core_dictionaries(self):\n        \"\"\"\n        Get the core dictionaries of this Rainbow.\n\n        Returns\n        -------\n        core : dict\n            Dictionary containing the keys\n            ['wavelike', 'timelike', 'fluxlike', 'metadata']\n        \"\"\"\n        return {k: vars(self)[k] for k in self._core_dictionaries}\n\n    def _initialize_from_arrays(\n        self, wavelength=None, time=None, flux=None, uncertainty=None, **kw\n    ):\n        \"\"\"\n        Populate from arrays.\n\n        Parameters\n        ----------\n        wavelength : Quantity, optional\n            A 1D array of wavelengths, in any unit.\n        time : Quantity, Time, optional\n            A 1D array of times, in any unit.\n        flux : array, optional\n            A 2D array of flux values.\n        uncertainty : array, optional\n            A 2D array of uncertainties, associated with the flux.\n        **kw : dict, optional\n            Additional keywords will be interpreted as arrays\n            that should be sorted into the appropriate location\n            based on their size.\n        \"\"\"\n\n        # store the wavelength\n        self.wavelike[\"wavelength\"] = wavelength * 1\n\n        # store the time\n        self.timelike[\"time\"] = time * 1\n\n        # store the flux and uncertainty\n        self.fluxlike[\"flux\"] = flux * 1\n        if uncertainty is None:\n            self.fluxlike[\"uncertainty\"] = np.ones_like(flux) * np.nan\n        else:\n            self.fluxlike[\"uncertainty\"] = uncertainty * 1\n\n        # sort other arrays by shape\n        for k, v in kw.items():\n            self._put_array_in_right_dictionary(k, v)\n\n        # validate that something reasonable got populated\n        self._validate_core_dictionaries()\n\n    def _put_array_in_right_dictionary(self, k, v):\n        \"\"\"\n        Sort an input into the right core dictionary\n        (timelike, wavelike, fluxlike) based on its shape.\n\n        Parameters\n        ----------\n        k : str\n            The key for the (appropriate) dictionary.\n        v : array\n            The quantity to sort.\n        \"\"\"\n        if np.shape(v) == self.shape:\n            self.fluxlike[k] = v * 1\n        elif np.shape(v) == (self.nwave,):\n            self.wavelike[k] = v * 1\n        elif np.shape(v) == (self.ntime,):\n            self.timelike[k] = v * 1\n        else:\n            raise ValueError(f\"'{k}' doesn't fit anywhere!\")\n\n    def _initialize_from_file(self, filepath=None, format=None, **kw):\n        \"\"\"\n        Populate from a filename or group of files.\n\n        Parameters\n        ----------\n        filepath : str\n            The filepath pointing to the file or group of files\n            that should be read.\n        format : str, function, (optional)\n            The file format of the file to be read.\n            If None, guess format from filepath.\n            If str, pull reader from dictionary of readers.\n            If function, treat as a `from_???` reader function.\n        **kw : dict,  (optional)\n            Additional keywords will be passed on to the reader.\n        \"\"\"\n\n        # make sure we're dealing with a real filename\n        assert filepath is not None\n\n        # pick the appropriate reader\n        reader = guess_reader(filepath=filepath, format=format)\n        reader(self, filepath, **kw)\n\n        # validate that something reasonable got populated\n        self._validate_core_dictionaries()\n        self._validate_uncertainties()\n        self._guess_wscale()\n        self._guess_tscale()\n\n    def _create_copy(self):\n        \"\"\"\n        Create a copy of self, with the core dictionaries copied.\n        \"\"\"\n        new = type(self)()\n        new._initialize_from_dictionaries(\n            **copy.deepcopy(self._get_core_dictionaries())\n        )\n        return new\n\n    def _guess_wscale(self, relative_tolerance=0.05):\n        \"\"\"\n        Try to guess the wscale from the wavelengths.\n\n        Parameters\n        ----------\n\n        relative_tolerance : float\n            The fractional difference to which the differences\n            between wavelengths should match in order for a\n            linear or logarithmic wavelength scale to be\n            assigned. For example, the default value of 0.01\n            means that the differences between all wavelength\n            must be within 1% of each other for the wavelength\n            scale to be called linear.\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n\n            # give up if there's no wavelength array\n            if self.wavelength is None:\n                return \"?\"\n\n            # calculate difference arrays\n            w = self.wavelength.value\n            dw = np.diff(w)\n            dlogw = np.diff(np.log(w))\n\n            # test the three options\n            if np.allclose(dw, np.median(dw), rtol=relative_tolerance):\n                self.metadata[\"wscale\"] = \"linear\"\n            elif np.allclose(dlogw, np.median(dlogw), rtol=relative_tolerance):\n                self.metadata[\"wscale\"] = \"log\"\n            else:\n                self.metadata[\"wscale\"] = \"?\"\n\n    def _guess_tscale(self, relative_tolerance=0.05):\n        \"\"\"\n        Try to guess the tscale from the times.\n\n        Parameters\n        ----------\n\n        relative_tolerance : float\n            The fractional difference to which the differences\n            between times should match in order for us to call\n            the times effectively uniform, or for us to treat\n            them more carefully as an irregular or gappy grid.\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n\n            # give up if there's no time array\n            if self.time is None:\n                return \"?\"\n\n            # calculate difference arrays\n            t = self.time.value\n            dt = np.diff(t)\n            with warnings.catch_warnings():\n                # (don't complain about negative time)\n                warnings.simplefilter(\"ignore\")\n                dlogt = np.diff(np.log(t))\n\n            # test the three options\n            if np.allclose(dt, np.median(dt), rtol=relative_tolerance):\n                self.metadata[\"tscale\"] = \"linear\"\n            # elif np.allclose(dlogt, np.median(dlogt), rtol=relative_tolerance):\n            #    self.metadata[\"tscale\"] = \"log\"\n            else:\n                self.metadata[\"tscale\"] = \"?\"\n\n    @property\n    def name(self):\n        \"\"\"\n        The name of this `Rainbow` object.\n        \"\"\"\n        return self.metadata.get(\"name\", None)\n\n    @property\n    def wavelength(self):\n        \"\"\"\n        The 1D array of wavelengths (with astropy units of length).\n        \"\"\"\n        return self.wavelike.get(\"wavelength\", None)\n\n    @property\n    def time(self):\n        \"\"\"\n        The 1D array of time (with astropy units of time).\n        \"\"\"\n        return self.timelike.get(\"time\", None)\n\n    @property\n    def flux(self):\n        \"\"\"\n        The 2D array of fluxes (row = wavelength, col = time).\n        \"\"\"\n        return self.fluxlike.get(\"flux\", None)\n\n    @property\n    def uncertainty(self):\n        \"\"\"\n        The 2D array of uncertainties on the fluxes.\n        \"\"\"\n        return self.fluxlike.get(\"uncertainty\", None)\n\n    @property\n    def ok(self):\n        \"\"\"\n        The 2D array of whether data is OK (row = wavelength, col = time).\n        \"\"\"\n\n        # assemble from three possible arrays\n        ok = self.fluxlike.get(\"ok\", np.ones(self.shape).astype(bool))\n        ok = (\n            ok\n            * self.wavelike.get(\"ok\", np.ones(self.nwave).astype(bool))[:, np.newaxis]\n        )\n        ok = (\n            ok\n            * self.timelike.get(\"ok\", np.ones(self.ntime).astype(bool))[np.newaxis, :]\n        )\n\n        # make sure flux is finite\n        if self.flux is not None:\n            ok = ok * np.isfinite(self.flux)\n\n        # weird kludge to deal with rounding errors (particularly in two-step .bin)\n        if ok.dtype == bool:\n            return ok\n        elif np.all((ok == 1) | (ok == 0)):\n            return ok.astype(bool)\n        else:\n            return np.round(ok, decimals=12)\n\n    @property\n    def _time_label(self):\n        return self.metadata.get(\"time_label\", \"Time\")\n\n    @property\n    def _wave_label(self):\n        return self.metadata.get(\"wave_label\", \"Wavelength\")\n\n    def __getattr__(self, key):\n        \"\"\"\n        If an attribute/method isn't explicitly defined,\n        try to pull it from one of the core dictionaries.\n\n        Let's say you want to get the 2D uncertainty array\n        but don't want to type `self.fluxlike['uncertainty']`.\n        You could instead type `self.uncertainty`, and this\n        would try to search through the four standard\n        dictionaries to pull out the first `uncertainty`\n        it finds.\n\n        Parameters\n        ----------\n        key : str\n            The attribute we're trying to get.\n        \"\"\"\n        if key not in self._core_dictionaries:\n            for dictionary_name in self._core_dictionaries:\n                try:\n                    return self.__dict__[dictionary_name][key]\n                except KeyError:\n                    pass\n        message = f\"\ud83c\udf08.{key} does not exist for this Rainbow\"\n        raise AttributeError(message)\n\n    def __setattr__(self, key, value):\n        \"\"\"\n        When setting a new attribute, try to sort it into the\n        appropriate core directory based on its size.\n\n        Let's say you have some quantity that has the same\n        shape as the wavelength array and you'd like to attach\n        it to this Rainbow object. This will try to save it\n        in the most relevant core dictionary (of the choices\n        timelike, wavelike, fluxlike).\n\n        Parameters\n        ----------\n        key : str\n            The attribute we're trying to get.\n        value : array\n            The quantity we're trying to attach to that name.\n        \"\"\"\n        try:\n            if key in self._core_dictionaries:\n                raise ValueError(\"Trying to set a core dictionary.\")\n            elif key == \"wavelength\":\n                self.wavelike[\"wavelength\"] = value * 1\n                self._validate_core_dictionaries()\n            elif key == \"time\":\n                self.timelike[\"time\"] = value * 1\n                self._validate_core_dictionaries()\n            elif key in [\"flux\", \"uncertainty\", \"ok\"]:\n                self.fluxlike[key] = value * 1\n                self._validate_core_dictionaries()\n            elif isinstance(value, str):\n                self.metadata[key] = value\n            else:\n                self._put_array_in_right_dictionary(key, value)\n        except (AttributeError, ValueError):\n            self.__dict__[key] = value\n\n    @property\n    def _nametag(self):\n        \"\"\"\n        This short phrase will preface everything\n        said with `self.speak()`.\n        \"\"\"\n        return f\"\ud83c\udf08({self.nwave}w, {self.ntime}t)\"\n\n    @property\n    def shape(self):\n        \"\"\"\n        The shape of the flux array (nwave, ntime).\n        \"\"\"\n        return (self.nwave, self.ntime)\n\n    @property\n    def nwave(self):\n        \"\"\"\n        The number of wavelengths.\n        \"\"\"\n        if self.wavelength is None:\n            return 0\n        else:\n            return len(self.wavelength)\n\n    @property\n    def ntime(self):\n        \"\"\"\n        The number of times.\n        \"\"\"\n        if self.time is None:\n            return 0\n        else:\n            return len(self.time)\n\n    @property\n    def dt(self):\n        \"\"\"\n        The typical timestep.\n        \"\"\"\n        if self.time is None:\n            return None\n        else:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                return np.nanmedian(np.diff(self.time)).to(u.minute)\n\n    @property\n    def nflux(self):\n        \"\"\"\n        The total number of fluxes.\n        \"\"\"\n        return np.prod(self.shape)\n\n    def _validate_core_dictionaries(self):\n        \"\"\"\n        Do some simple checks to make sure this Rainbow\n        is populated with the minimal data needed to do anything.\n        It shouldn't be run before the Rainbow is fully\n        initialized; otherwise, it might complain about\n        a half-populated object.\n        \"\"\"\n\n        # make sure there are some times + wavelengths defined\n        if self.ntime is None:\n            cheerfully_suggest(\n                f\"\"\"\n            No times are defined for this Rainbow.\n            \"\"\"\n            )\n        if self.nwave is None:\n            cheerfully_suggest(\n                f\"\"\"\n            No wavelengths are defined for this Rainbow.\n            \"\"\"\n            )\n\n        # warn if the times and wavelengths are the same size\n        if (self.nwave == self.ntime) and (self.ntime is not None) and (self.ntime &gt; 1):\n            cheerfully_suggest(\n                f\"\"\"\n            The number of wavelengths ({self.nwave}) is the same as the\n            number of times ({self.ntime}). This is fine, we suppose\n            (&lt;mock exasperated sigh&gt;), but here are few reasons you might\n            want to reconsider letting them have the same size:\n                (1) Mathemetical operations and variabile assignment\n                    inside this Rainbow make guesses about whether a quantity\n                    is wavelike or timelike based on its shape; these features\n                    will fail (or even worse do something mysterious) if\n                    there are the same numbers of wavelengths and times.\n                (2) For your own darn sake, if your fluxlike arrays are\n                    all square, it's going to be very easy for you to accidentally\n                    transpose them and not realize it.\n                (3) It's very unlikely that your real data had exactly the same\n                    number of times and wavelengths, so we're guessing that you\n                    probably just created these arrays from scratch, which\n                    hopefully means it's not too annoying to just make them\n                    have different numbers of wavelengths and times.\n            Thanks!\n            \"\"\"\n            )\n\n        # does the flux have the right shape?\n        if self.shape != np.shape(self.flux):\n            message = f\"\"\"\n            Something doesn't line up!\n            The flux array has a shape of {np.shape(self.flux)}.\n            The wavelength array has {self.nwave} wavelengths.\n            The time array has {self.ntime} times.\n            \"\"\"\n            if self.shape == np.shape(self.flux)[::-1]:\n                cheerfully_suggest(\n                    f\"\"\"{message}\n                    Any chance your flux array is transposed?\n                    \"\"\"\n                )\n            else:\n                cheerfully_suggest(message)\n\n        for n in [\"uncertainty\", \"ok\"]:\n            x = getattr(self, n)\n            if x is not None:\n                if x.shape != np.shape(self.flux):\n                    message = f\"\"\"\n                    Watch out! The '{n}' array has\n                    a shape of {x.shape}, which doesn't match the\n                    flux array's shape of {np.shape(self.flux)}.\n                    \"\"\"\n                    cheerfully_suggest(message)\n\n        # make sure 2D arrays are uniquely named from 1D\n        for k in tuple(self.fluxlike.keys()):\n            if (k in self.wavelike) or (k in self.timelike):\n                self.fluxlike[f\"{k}_2d\"] = self.fluxlike.pop(k)\n\n        if \"ok\" in self.fluxlike:\n            is_nan = np.isnan(self.fluxlike[\"flux\"])\n            self.fluxlike[\"ok\"][is_nan] = 0\n\n        # make sure no arrays are accidentally pointed to each other\n        # (if they are, sorting will get really messed up!)\n        for d in [\"fluxlike\", \"wavelike\", \"timelike\"]:\n            core_dictionary = self.get(d)\n            for k1, v1 in core_dictionary.items():\n                for k2, v2 in core_dictionary.items():\n                    if k1 != k2:\n                        assert v1 is not v2\n\n        self._sort()\n\n    def _make_sure_wavelength_edges_are_defined(self):\n        \"\"\"\n        Make sure there are some wavelength edges defined.\n        \"\"\"\n        if self.nwave &lt;= 1:\n            return\n        if (\"wavelength_lower\" not in self.wavelike) or (\n            \"wavelength_upper\" not in self.wavelike\n        ):\n            if self.metadata.get(\"wscale\", None) == \"log\":\n                l, u = calculate_bin_leftright(np.log(self.wavelength.value))\n                self.wavelike[\"wavelength_lower\"] = np.exp(l) * self.wavelength.unit\n                self.wavelike[\"wavelength_upper\"] = np.exp(u) * self.wavelength.unit\n            elif self.metadata.get(\"wscale\", None) == \"linear\":\n                l, u = calculate_bin_leftright(self.wavelength)\n                self.wavelike[\"wavelength_lower\"] = l\n                self.wavelike[\"wavelength_upper\"] = u\n            else:\n                l, u = calculate_bin_leftright(self.wavelength)\n                self.wavelike[\"wavelength_lower\"] = l\n                self.wavelike[\"wavelength_upper\"] = u\n\n    def _make_sure_time_edges_are_defined(self, redo=True):\n        \"\"\"\n        Make sure there are some time edges defined.\n        \"\"\"\n        if self.ntime &lt;= 1:\n            return\n        if (\n            (\"time_lower\" not in self.timelike)\n            or (\"time_upper\" not in self.timelike)\n            or redo\n        ):\n            if self.metadata.get(\"tscale\", None) == \"log\":\n                lower, upper = calculate_bin_leftright(np.log(self.time.value))\n                self.timelike[\"time_lower\"] = np.exp(lower) * self.time.unit\n                self.timelike[\"time_upper\"] = np.exp(upper) * self.time.unit\n            else:\n                lower, upper = calculate_bin_leftright(self.time)\n                self.timelike[\"time_lower\"] = lower\n                self.timelike[\"time_upper\"] = upper\n\n    def __getitem__(self, key):\n        \"\"\"\n        Trim a rainbow by indexing, slicing, or masking.\n        Two indices must be provided (`[:,:]`).\n\n        Examples\n        --------\n        ```\n        r[:,:]\n        r[10:20, :]\n        r[np.arange(10,20), :]\n        r[r.wavelength &gt; 1*u.micron, :]\n        r[:, np.abs(r.time) &lt; 1*u.hour]\n        r[r.wavelength &gt; 1*u.micron, np.abs(r.time) &lt; 1*u.hour]\n        ```\n\n        Parameters\n        ----------\n        key : tuple\n            The (wavelength, time) slices, indices, or masks.\n        \"\"\"\n\n        i_wavelength, i_time = key\n        # create a history entry for this action (before other variables are defined)\n        h = self._create_history_entry(\"__getitem__\", locals())\n\n        # create a copy\n        new = self._create_copy()\n\n        # make sure we don't drop down to 1D arrays\n        if isinstance(i_wavelength, int):\n            i_wavelength = [i_wavelength]\n\n        if isinstance(i_time, int):\n            i_time = [i_time]\n\n        # do indexing of wavelike\n        for w in self.wavelike:\n            new.wavelike[w] = self.wavelike[w][i_wavelength]\n\n        # do indexing of timelike\n        for t in self.timelike:\n            new.timelike[t] = self.timelike[t][i_time]\n\n        # do indexing of fluxlike\n        for f in self.fluxlike:\n            # (indexing step by step seems more stable)\n            if self.fluxlike[f] is None:\n                continue\n            temporary = self.fluxlike[f][i_wavelength, :]\n            new.fluxlike[f] = temporary[:, i_time]\n\n        # finalize the new rainbow\n        new._validate_core_dictionaries()\n        new._guess_wscale()\n        new._guess_tscale()\n\n        # append the history entry to the new Rainbow\n        new._record_history_entry(h)\n\n        return new\n\n    def __repr__(self):\n        \"\"\"\n        How should this object be represented as a string?\n        \"\"\"\n        n = self.__class__.__name__.replace(\"Rainbow\", \"\ud83c\udf08\")\n        if self.name is not None:\n            n += f\"'{self.name}'\"\n        return f\"&lt;{n}({self.nwave}w, {self.ntime}t)&gt;\"\n\n    # import the basic operations for Rainbows\n    from .actions.operations import (\n        _apply_operation,\n        _broadcast_to_fluxlike,\n        _raise_ambiguous_shape_error,\n        __add__,\n        __sub__,\n        __mul__,\n        __truediv__,\n        __eq__,\n        diff,\n    )\n\n    # import other actions that return other Rainbows\n    from .actions import (\n        normalize,\n        _is_probably_normalized,\n        bin,\n        bin_in_time,\n        bin_in_wavelength,\n        trim,\n        trim_times,\n        trim_wavelengths,\n        shift,\n        _create_shared_wavelength_axis,\n        align_wavelengths,\n        inject_transit,\n        inject_systematics,\n        inject_noise,\n        inject_spectrum,\n        inject_outliers,\n        flag_outliers,\n        fold,\n        mask_transit,\n        compare,\n        get_average_lightcurve_as_rainbow,\n        get_average_spectrum_as_rainbow,\n        _create_fake_wavelike_quantity,\n        _create_fake_timelike_quantity,\n        _create_fake_fluxlike_quantity,\n        remove_trends,\n        attach_model,\n        inflate_uncertainty,\n        concatenate_in_time,\n        concatenate_in_wavelength,\n    )\n\n    # import summary statistics for each wavelength\n    from .get.wavelike import (\n        get_average_spectrum,\n        get_median_spectrum,\n        get_spectral_resolution,\n        get_expected_uncertainty,\n        get_measured_scatter,\n        get_measured_scatter_in_bins,\n        get_for_wavelength,\n        get_ok_data_for_wavelength,\n    )\n\n    # import summary statistics for each time\n    from .get.timelike import (\n        get_average_lightcurve,\n        get_median_lightcurve,\n        get_for_time,\n        get_ok_data_for_time,\n        get_times_as_astropy,\n        set_times_from_astropy,\n    )\n\n    # import summary statistics for each time\n    from .get.fluxlike import (\n        get_ok_data,\n    )\n\n    # import visualizations that can act on Rainbows\n    from .visualizations import (\n        paint,\n        imshow,\n        pcolormesh,\n        scatter,\n        plot_lightcurves,\n        _setup_animate_lightcurves,\n        animate_lightcurves,\n        _setup_animate_spectra,\n        animate_spectra,\n        _setup_animated_scatter,\n        setup_wavelength_colors,\n        _make_sure_cmap_is_defined,\n        get_wavelength_color,\n        paint_quantities,\n        plot_quantities,\n        imshow_interact,\n        plot_spectra,\n        plot,\n        plot_histogram,\n        _scatter_timelike_or_wavelike,\n        _get_plot_directory,\n        _label_plot_file,\n        savefig,\n    )\n\n    from .visualizations.wavelike import (\n        plot_spectral_resolution,\n        plot_noise_comparison,\n        plot_noise_comparison_in_bins,\n        plot_average_spectrum,\n        plot_median_spectrum,\n    )\n\n    from .visualizations.timelike import plot_average_lightcurve, plot_median_lightcurve\n\n    from .converters import (\n        to_nparray,\n        to_df,\n    )\n\n    from .helpers import (\n        _setup_history,\n        _record_history_entry,\n        _remove_last_history_entry,\n        _create_history_entry,\n        history,\n        help,\n        save,\n        get,\n    )\n</code></pre>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.dt","title":"<code>dt</code>  <code>property</code>","text":"<p>The typical timestep.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.flux","title":"<code>flux</code>  <code>property</code>","text":"<p>The 2D array of fluxes (row = wavelength, col = time).</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.name","title":"<code>name</code>  <code>property</code>","text":"<p>The name of this <code>Rainbow</code> object.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.nflux","title":"<code>nflux</code>  <code>property</code>","text":"<p>The total number of fluxes.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.ntime","title":"<code>ntime</code>  <code>property</code>","text":"<p>The number of times.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.nwave","title":"<code>nwave</code>  <code>property</code>","text":"<p>The number of wavelengths.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.ok","title":"<code>ok</code>  <code>property</code>","text":"<p>The 2D array of whether data is OK (row = wavelength, col = time).</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>The shape of the flux array (nwave, ntime).</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.time","title":"<code>time</code>  <code>property</code>","text":"<p>The 1D array of time (with astropy units of time).</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.uncertainty","title":"<code>uncertainty</code>  <code>property</code>","text":"<p>The 2D array of uncertainties on the fluxes.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.wavelength","title":"<code>wavelength</code>  <code>property</code>","text":"<p>The 1D array of wavelengths (with astropy units of length).</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__getattr__","title":"<code>__getattr__(key)</code>","text":"<p>If an attribute/method isn't explicitly defined, try to pull it from one of the core dictionaries.</p> <p>Let's say you want to get the 2D uncertainty array but don't want to type <code>self.fluxlike['uncertainty']</code>. You could instead type <code>self.uncertainty</code>, and this would try to search through the four standard dictionaries to pull out the first <code>uncertainty</code> it finds.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__getattr__--parameters","title":"Parameters","text":"<p>key : str     The attribute we're trying to get.</p> Source code in <code>chromatic/rainbows/rainbow.py</code> <pre><code>def __getattr__(self, key):\n    \"\"\"\n    If an attribute/method isn't explicitly defined,\n    try to pull it from one of the core dictionaries.\n\n    Let's say you want to get the 2D uncertainty array\n    but don't want to type `self.fluxlike['uncertainty']`.\n    You could instead type `self.uncertainty`, and this\n    would try to search through the four standard\n    dictionaries to pull out the first `uncertainty`\n    it finds.\n\n    Parameters\n    ----------\n    key : str\n        The attribute we're trying to get.\n    \"\"\"\n    if key not in self._core_dictionaries:\n        for dictionary_name in self._core_dictionaries:\n            try:\n                return self.__dict__[dictionary_name][key]\n            except KeyError:\n                pass\n    message = f\"\ud83c\udf08.{key} does not exist for this Rainbow\"\n    raise AttributeError(message)\n</code></pre>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Trim a rainbow by indexing, slicing, or masking. Two indices must be provided (<code>[:,:]</code>).</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__getitem__--examples","title":"Examples","text":"<pre><code>r[:,:]\nr[10:20, :]\nr[np.arange(10,20), :]\nr[r.wavelength &gt; 1*u.micron, :]\nr[:, np.abs(r.time) &lt; 1*u.hour]\nr[r.wavelength &gt; 1*u.micron, np.abs(r.time) &lt; 1*u.hour]\n</code></pre>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__getitem__--parameters","title":"Parameters","text":"<p>key : tuple     The (wavelength, time) slices, indices, or masks.</p> Source code in <code>chromatic/rainbows/rainbow.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"\n    Trim a rainbow by indexing, slicing, or masking.\n    Two indices must be provided (`[:,:]`).\n\n    Examples\n    --------\n    ```\n    r[:,:]\n    r[10:20, :]\n    r[np.arange(10,20), :]\n    r[r.wavelength &gt; 1*u.micron, :]\n    r[:, np.abs(r.time) &lt; 1*u.hour]\n    r[r.wavelength &gt; 1*u.micron, np.abs(r.time) &lt; 1*u.hour]\n    ```\n\n    Parameters\n    ----------\n    key : tuple\n        The (wavelength, time) slices, indices, or masks.\n    \"\"\"\n\n    i_wavelength, i_time = key\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"__getitem__\", locals())\n\n    # create a copy\n    new = self._create_copy()\n\n    # make sure we don't drop down to 1D arrays\n    if isinstance(i_wavelength, int):\n        i_wavelength = [i_wavelength]\n\n    if isinstance(i_time, int):\n        i_time = [i_time]\n\n    # do indexing of wavelike\n    for w in self.wavelike:\n        new.wavelike[w] = self.wavelike[w][i_wavelength]\n\n    # do indexing of timelike\n    for t in self.timelike:\n        new.timelike[t] = self.timelike[t][i_time]\n\n    # do indexing of fluxlike\n    for f in self.fluxlike:\n        # (indexing step by step seems more stable)\n        if self.fluxlike[f] is None:\n            continue\n        temporary = self.fluxlike[f][i_wavelength, :]\n        new.fluxlike[f] = temporary[:, i_time]\n\n    # finalize the new rainbow\n    new._validate_core_dictionaries()\n    new._guess_wscale()\n    new._guess_tscale()\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__init__","title":"<code>__init__(filepath=None, format=None, wavelength=None, time=None, flux=None, uncertainty=None, wavelike=None, timelike=None, fluxlike=None, metadata=None, name=None, **kw)</code>","text":"<p>Initialize a <code>Rainbow</code> object.</p> <p>The <code>__init__</code> function is called when a new <code>Rainbow</code> is instantiated as <code>r = Rainbow(some, kinds, of=inputs)</code>.</p> <p>The options for inputs are flexible, including the possibility to initialize from a file, from arrays with appropriate units, from dictionaries with appropriate ingredients, or simply as an empty object if no arguments are given.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__init__--parameters","title":"Parameters","text":"<p>filepath : str, optional     The filepath pointing to the file or group of files     that should be read. format : str, optional     The file format of the file to be read. If None,     the format will be guessed automatically from the     filepath. wavelength : Quantity, optional     A 1D array of wavelengths, in any unit. time : Quantity, Time, optional     A 1D array of times, in any unit. flux : array, optional     A 2D array of flux values. uncertainty : array, optional     A 2D array of uncertainties, associated with the flux. wavelike : dict, optional     A dictionary containing 1D arrays with the same     shape as the wavelength axis. It must at least     contain the key 'wavelength', which should have     astropy units of wavelength associated with it. timelike : dict, optional     A dictionary containing 1D arrays with the same     shape as the time axis. It must at least     contain the key 'time', which should have     astropy units of time associated with it. fluxlike : dict, optional     A dictionary containing 2D arrays with the shape     of (nwave, ntime), like flux. It must at least     contain the key 'flux'. metadata : dict, optional     A dictionary containing all other metadata     associated with the dataset, generally lots of     individual parameters or comments. **kw : dict, optional     Additional keywords will be passed along to     the function that initializes the rainbow.     If initializing from arrays (<code>time=</code>, <code>wavelength=</code>,     ...), these keywords will be interpreted as     additional arrays that should be sorted by their     shape into the appropriate dictionary. If     initializing from files, the keywords will     be passed on to the reader.</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__init__--examples","title":"Examples","text":"<p>Initialize from a file. While this works, a more robust solution is probably to use <code>read_rainbow</code>, which will automatically choose the best of <code>Rainbow</code> and <code>RainbowWithModel</code></p> <pre><code>r1 = Rainbow('my-neat-file.abc', format='abcdefgh')\n</code></pre> <p>Initalize from arrays. The wavelength and time must have appropriate units, and the shape of the flux array must match the size of the wavelength and time arrays. Other arrays that match the shape of any of these quantities will be stored in the appropriate location. Other inputs not matching any of these will be stored as <code>metadata.</code></p> <pre><code>r2 = Rainbow(\n        wavelength=np.linspace(1, 5, 50)*u.micron,\n        time=np.linspace(-0.5, 0.5, 100)*u.day,\n        flux=np.random.normal(0, 1, (50, 100)),\n        some_other_array=np.ones((50,100)),\n        some_metadata='wow!'\n)\n</code></pre> <p>Initialize from dictionaries. The dictionaries must contain at least <code>wavelike['wavelength']</code>, <code>timelike['time']</code>, and <code>fluxlike['flux']</code>, but any other additional inputs can be provided.</p> <pre><code>r3 = Rainbow(\n        wavelike=dict(wavelength=np.linspace(1, 5, 50)*u.micron),\n        timelike=dict(time=np.linspace(-0.5, 0.5, 100)*u.day),\n        fluxlike=dict(flux=np.random.normal(0, 1, (50, 100)))\n)\n</code></pre> Source code in <code>chromatic/rainbows/rainbow.py</code> <pre><code>def __init__(\n    self,\n    filepath=None,\n    format=None,\n    wavelength=None,\n    time=None,\n    flux=None,\n    uncertainty=None,\n    wavelike=None,\n    timelike=None,\n    fluxlike=None,\n    metadata=None,\n    name=None,\n    **kw,\n):\n    \"\"\"\n    Initialize a `Rainbow` object.\n\n    The `__init__` function is called when a new `Rainbow` is\n    instantiated as `r = Rainbow(some, kinds, of=inputs)`.\n\n    The options for inputs are flexible, including the possibility\n    to initialize from a file, from arrays with appropriate units,\n    from dictionaries with appropriate ingredients, or simply as\n    an empty object if no arguments are given.\n\n    Parameters\n    ----------\n    filepath : str, optional\n        The filepath pointing to the file or group of files\n        that should be read.\n    format : str, optional\n        The file format of the file to be read. If None,\n        the format will be guessed automatically from the\n        filepath.\n    wavelength : Quantity, optional\n        A 1D array of wavelengths, in any unit.\n    time : Quantity, Time, optional\n        A 1D array of times, in any unit.\n    flux : array, optional\n        A 2D array of flux values.\n    uncertainty : array, optional\n        A 2D array of uncertainties, associated with the flux.\n    wavelike : dict, optional\n        A dictionary containing 1D arrays with the same\n        shape as the wavelength axis. It must at least\n        contain the key 'wavelength', which should have\n        astropy units of wavelength associated with it.\n    timelike : dict, optional\n        A dictionary containing 1D arrays with the same\n        shape as the time axis. It must at least\n        contain the key 'time', which should have\n        astropy units of time associated with it.\n    fluxlike : dict, optional\n        A dictionary containing 2D arrays with the shape\n        of (nwave, ntime), like flux. It must at least\n        contain the key 'flux'.\n    metadata : dict, optional\n        A dictionary containing all other metadata\n        associated with the dataset, generally lots of\n        individual parameters or comments.\n    **kw : dict, optional\n        Additional keywords will be passed along to\n        the function that initializes the rainbow.\n        If initializing from arrays (`time=`, `wavelength=`,\n        ...), these keywords will be interpreted as\n        additional arrays that should be sorted by their\n        shape into the appropriate dictionary. If\n        initializing from files, the keywords will\n        be passed on to the reader.\n\n    Examples\n    --------\n    Initialize from a file. While this works, a more robust\n    solution is probably to use `read_rainbow`, which will\n    automatically choose the best of `Rainbow` and `RainbowWithModel`\n    ```\n    r1 = Rainbow('my-neat-file.abc', format='abcdefgh')\n    ```\n\n    Initalize from arrays. The wavelength and time must have\n    appropriate units, and the shape of the flux array must\n    match the size of the wavelength and time arrays. Other\n    arrays that match the shape of any of these quantities\n    will be stored in the appropriate location. Other inputs\n    not matching any of these will be stored as `metadata.`\n    ```\n    r2 = Rainbow(\n            wavelength=np.linspace(1, 5, 50)*u.micron,\n            time=np.linspace(-0.5, 0.5, 100)*u.day,\n            flux=np.random.normal(0, 1, (50, 100)),\n            some_other_array=np.ones((50,100)),\n            some_metadata='wow!'\n    )\n    ```\n    Initialize from dictionaries. The dictionaries must contain\n    at least `wavelike['wavelength']`, `timelike['time']`, and\n    `fluxlike['flux']`, but any other additional inputs can be\n    provided.\n    ```\n    r3 = Rainbow(\n            wavelike=dict(wavelength=np.linspace(1, 5, 50)*u.micron),\n            timelike=dict(time=np.linspace(-0.5, 0.5, 100)*u.day),\n            fluxlike=dict(flux=np.random.normal(0, 1, (50, 100)))\n    )\n    ```\n    \"\"\"\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"Rainbow\", locals())\n\n    # metadata are arbitrary types of information we need\n    self.metadata = {\"name\": name}\n\n    # wavelike quanities are 1D arrays with nwave elements\n    self.wavelike = {}\n\n    # timelike quantities are 1D arrays with ntime elements\n    self.timelike = {}\n\n    # fluxlike quantities are 2D arrays with nwave x time elements\n    self.fluxlike = {}\n\n    # try to intialize from the exact dictionaries needed\n    if (\n        (type(wavelike) == dict)\n        and (type(timelike) == dict)\n        and (type(fluxlike) == dict)\n    ):\n        self._initialize_from_dictionaries(\n            wavelike=wavelike,\n            timelike=timelike,\n            fluxlike=fluxlike,\n            metadata=metadata,\n        )\n    # then try to initialize from arrays\n    elif (wavelength is not None) and (time is not None) and (flux is not None):\n        self._initialize_from_arrays(\n            wavelength=wavelength,\n            time=time,\n            flux=flux,\n            uncertainty=uncertainty,\n            **kw,\n        )\n        if metadata is not None:\n            self.metadata.update(**metadata)\n    # then try to initialize from a file\n    elif isinstance(filepath, (str, list, Column)):\n        self._initialize_from_file(filepath=filepath, format=format, **kw)\n\n    # finally, tidy up by guessing the scales\n    self._guess_wscale()\n    self._guess_tscale()\n\n    # append the history entry to this Rainbow\n    self._setup_history()\n    self._record_history_entry(h)\n</code></pre>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__repr__","title":"<code>__repr__()</code>","text":"<p>How should this object be represented as a string?</p> Source code in <code>chromatic/rainbows/rainbow.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    How should this object be represented as a string?\n    \"\"\"\n    n = self.__class__.__name__.replace(\"Rainbow\", \"\ud83c\udf08\")\n    if self.name is not None:\n        n += f\"'{self.name}'\"\n    return f\"&lt;{n}({self.nwave}w, {self.ntime}t)&gt;\"\n</code></pre>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__setattr__","title":"<code>__setattr__(key, value)</code>","text":"<p>When setting a new attribute, try to sort it into the appropriate core directory based on its size.</p> <p>Let's say you have some quantity that has the same shape as the wavelength array and you'd like to attach it to this Rainbow object. This will try to save it in the most relevant core dictionary (of the choices timelike, wavelike, fluxlike).</p>"},{"location":"api/#chromatic.rainbows.rainbow.Rainbow.__setattr__--parameters","title":"Parameters","text":"<p>key : str     The attribute we're trying to get. value : array     The quantity we're trying to attach to that name.</p> Source code in <code>chromatic/rainbows/rainbow.py</code> <pre><code>def __setattr__(self, key, value):\n    \"\"\"\n    When setting a new attribute, try to sort it into the\n    appropriate core directory based on its size.\n\n    Let's say you have some quantity that has the same\n    shape as the wavelength array and you'd like to attach\n    it to this Rainbow object. This will try to save it\n    in the most relevant core dictionary (of the choices\n    timelike, wavelike, fluxlike).\n\n    Parameters\n    ----------\n    key : str\n        The attribute we're trying to get.\n    value : array\n        The quantity we're trying to attach to that name.\n    \"\"\"\n    try:\n        if key in self._core_dictionaries:\n            raise ValueError(\"Trying to set a core dictionary.\")\n        elif key == \"wavelength\":\n            self.wavelike[\"wavelength\"] = value * 1\n            self._validate_core_dictionaries()\n        elif key == \"time\":\n            self.timelike[\"time\"] = value * 1\n            self._validate_core_dictionaries()\n        elif key in [\"flux\", \"uncertainty\", \"ok\"]:\n            self.fluxlike[key] = value * 1\n            self._validate_core_dictionaries()\n        elif isinstance(value, str):\n            self.metadata[key] = value\n        else:\n            self._put_array_in_right_dictionary(key, value)\n    except (AttributeError, ValueError):\n        self.__dict__[key] = value\n</code></pre>"},{"location":"api/#chromatic.rainbows.withmodel.RainbowWithModel.chi_squared","title":"<code>chi_squared</code>  <code>property</code>","text":"<p>Calculate chi-squared.</p> <p>This calculates the sum of the squares of the uncertainty-normalized residuals, sum(((flux - model)/uncertainty)**2)</p> <p>Data points marked as not OK are ignored.</p>"},{"location":"api/#chromatic.rainbows.withmodel.RainbowWithModel.chi_squared--returns","title":"Returns","text":"<p>chi_squared : float     The chi-squared value.</p>"},{"location":"api/#chromatic.rainbows.withmodel.RainbowWithModel.ones","title":"<code>ones</code>  <code>property</code>","text":"<p>Generate an array of ones that looks like the flux. (A tiny wrapper needed for <code>plot_with_model</code>)</p>"},{"location":"api/#chromatic.rainbows.withmodel.RainbowWithModel.ones--returns","title":"Returns","text":"<p>ones : array, Quantity     The 2D array ones (nwave, ntime).</p>"},{"location":"api/#chromatic.rainbows.withmodel.RainbowWithModel.residuals","title":"<code>residuals</code>  <code>property</code>","text":"<p>Calculate the residuals on the fly, to make sure they're always up to date.</p> <p>The residuals are calculated simply as the <code>.flux</code> - <code>.model</code>, so they are in whatever units those arrays have.</p>"},{"location":"api/#chromatic.rainbows.withmodel.RainbowWithModel.residuals--returns","title":"Returns","text":"<p>residuals : array, Quantity     The 2D array of residuals (nwave, ntime).</p>"},{"location":"api/#chromatic.rainbows.withmodel.RainbowWithModel.residuals_plus_one","title":"<code>residuals_plus_one</code>  <code>property</code>","text":"<p>A tiny wrapper to get the residuals + 1.</p>"},{"location":"api/#chromatic.rainbows.withmodel.RainbowWithModel.residuals_plus_one--returns","title":"Returns","text":"<p>residuals_plus_one : array, Quantity     The 2D array of residuals + 1 (nwave, ntime).</p>"},{"location":"api/#chromatic.rainbows.simulated.SimulatedRainbow.__init__","title":"<code>__init__(tlim=[-2.5, 2.5] * u.hour, dt=2 * u.minute, time=None, wlim=[0.5, 5] * u.micron, R=100, dw=None, wavelength=None, star_flux=None, name=None, signal_to_noise=None)</code>","text":"<p>Initialize a <code>SimulatedRainbow</code> object from some parameters.</p> <p>This sets up an effectively empty <code>Rainbow</code> with defined wavelengths and times. For making more interesting simulated datasets, this will often be paired with some combination of the <code>.inject...</code> actions that inject various astrophysical, instrumental, or noise signatures into the dataset.</p> The time-setting order of precendence is <p>1) time 2) tlim + dt</p> The wavelength-setting order of precendence is <p>1) wavelength 2) wlim + dw 3) wlim + R</p>"},{"location":"api/#chromatic.rainbows.simulated.SimulatedRainbow.__init__--parameters","title":"Parameters","text":"<p>tlim : list or Quantity     The pip install -e '.[develop]'[min, max] times for creating the time grid.     These should have astropy units of time. dt : Quantity     The d(time) bin size for creating a grid     that is uniform in linear space. time : Quantity     An array of times, if you just want to give     it an entirely custom array. wlim : list or Quantity     The [min, max] wavelengths for creating the grid.     These should have astropy units of wavelength. R : float     The spectral resolution for creating a grid     that is uniform in logarithmic space. dw : Quantity     The d(wavelength) bin size for creating a grid     that is uniform in linear space. wavelength : Quantity     An array of wavelengths, if you just want to give     it an entirely custom array. star_flux : numpy 1D array     An array of fluxes corresponding to the supplied wavelengths.     If left blank, the code assumes a normalized flux of     flux(wavelength) = 1 for all wavelengths.</p> Source code in <code>chromatic/rainbows/simulated.py</code> <pre><code>def __init__(\n    self,\n    tlim=[-2.5, 2.5] * u.hour,\n    dt=2 * u.minute,\n    time=None,\n    wlim=[0.5, 5] * u.micron,\n    R=100,\n    dw=None,\n    wavelength=None,\n    star_flux=None,\n    name=None,\n    signal_to_noise=None,\n):\n    \"\"\"\n    Initialize a `SimulatedRainbow` object from some parameters.\n\n    This sets up an effectively empty `Rainbow` with defined\n    wavelengths and times. For making more interesting\n    simulated datasets, this will often be paired with\n    some combination of the `.inject...` actions that inject\n    various astrophysical, instrumental, or noise signatures\n    into the dataset.\n\n    The time-setting order of precendence is:\n        1) time\n        2) tlim + dt\n\n    The wavelength-setting order of precendence is:\n        1) wavelength\n        2) wlim + dw\n        3) wlim + R\n\n    Parameters\n    ----------\n    tlim : list or Quantity\n        The pip install -e '.[develop]'[min, max] times for creating the time grid.\n        These should have astropy units of time.\n    dt : Quantity\n        The d(time) bin size for creating a grid\n        that is uniform in linear space.\n    time : Quantity\n        An array of times, if you just want to give\n        it an entirely custom array.\n    wlim : list or Quantity\n        The [min, max] wavelengths for creating the grid.\n        These should have astropy units of wavelength.\n    R : float\n        The spectral resolution for creating a grid\n        that is uniform in logarithmic space.\n    dw : Quantity\n        The d(wavelength) bin size for creating a grid\n        that is uniform in linear space.\n    wavelength : Quantity\n        An array of wavelengths, if you just want to give\n        it an entirely custom array.\n    star_flux : numpy 1D array\n        An array of fluxes corresponding to the supplied wavelengths.\n        If left blank, the code assumes a normalized flux of\n        flux(wavelength) = 1 for all wavelengths.\n    \"\"\"\n    Rainbow.__init__(self)\n\n    # (remove the history entry from creating the Rainbow)\n    self._remove_last_history_entry()\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"SimulatedRainbow\", locals())\n\n    # set up the wavelength grid\n    self._setup_fake_wavelength_grid(wlim=wlim, R=R, dw=dw, wavelength=wavelength)\n\n    # set up the time grid\n    self._setup_fake_time_grid(tlim=tlim, dt=dt, time=time)\n\n    # save the basic inputs that aren't stored elsewhere\n    self.metadata[\"name\"] = name\n\n    # If the flux of the star is not given,\n    # assume a continuum-normlized flux where fx=1 at all wavelengths.\n    if star_flux is None:\n        model = np.ones(self.shape)\n\n    # If the flux vs wavelength of the star is supplied,\n    # include it in the model.\n    else:\n        # Check to make sure the flux and wavelengths\n        # have the same shape.\n        if len(star_flux) == len(self.wavelike[\"wavelength\"]):\n            model = np.transpose([star_flux] * self.shape[1])\n        elif len(star_flux) == 1:\n            model = star_flux * np.ones(self.shape)\n\n    # Set uncertainty.\n    self.fluxlike[\"flux\"] = model * 1\n    self.fluxlike[\"model\"] = model * 1\n    self.fluxlike[\"uncertainty\"] = np.zeros(self.shape)\n\n    # make sure everything is defined and sorted\n    self._validate_core_dictionaries()\n\n    if signal_to_noise is not None:\n        message = f\"\"\"\n        You tried to specify the noise level with\n        `SimulatedRainbow(signal_to_noise={signal_to_noise})`,\n        but that functionality is going away soon.\n        Please replace it right now with\n        `SimulatedRainbow().inject_noise(signal_to_noise={signal_to_noise})`\n        so that your code will continue to work.\n        You're getting away with it this time,\n        but it won't work for much longer!\n        \"\"\"\n        cheerfully_suggest(message)\n        new = self.inject_noise()\n        for k in [\"flux\", \"uncertainty\", \"model\"]:\n            self.fluxlike[k] = new.fluxlike[k]\n\n    # append the history entry to the new Rainbow\n    self._record_history_entry(h)\n</code></pre>"},{"location":"api/#helpers","title":"\ud83c\udf08 Helpers","text":"<p>Retrieve an attribute by its string name. (This is a friendlier wrapper for <code>getattr()</code>).</p> <p><code>r.get('flux')</code> is identical to <code>r.flux</code></p> <p>This is different from indexing directly into a core dictionary (for example, <code>r.fluxlike['flux']</code>), because it can also be used to get the results of properties that do calculations on the fly (for example, <code>r.residuals</code> in the <code>RainbowWithModel</code> class).</p> <p>Print a quick reference of key actions available for this <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/helpers/help.py</code> <pre><code>def help(self):\n    \"\"\"\n    Print a quick reference of key actions available for this `Rainbow`.\n    \"\"\"\n    print(\n        textwrap.dedent(\n            \"\"\"\n    Hooray for you! You asked for help on what you can do\n    with this \ud83c\udf08 object. Here's a quick reference of a few\n    available options for things to try.\"\"\"\n        )\n    )\n\n    base_directory = pkg_resources.resource_filename(\"chromatic\", \"rainbows\")\n    descriptions_files = []\n    for level in [\"*\", \"*/*\"]:\n        descriptions_files += glob.glob(\n            os.path.join(base_directory, level, \"descriptions.txt\")\n        )\n    categories = [\n        d.replace(base_directory + \"/\", \"\").replace(\"/descriptions.txt\", \"\")\n        for d in descriptions_files\n    ]\n    for i in np.argsort(categories):\n        c, d = categories[i], descriptions_files[i]\n        header = (\n            \"\\n\" + \"-\" * (len(c) + 4) + \"\\n\" + f\"| {c} |\\n\" + \"-\" * (len(c) + 4) + \"\\n\"\n        )\n\n        table = ascii.read(d)\n        items = []\n        for row in table:\n            name = row[\"name\"]\n            if hasattr(self, name) or (name in [\"+-*/\", \"[:,:]\"]):\n                if name in \"+-*/\":\n                    function_call = f\"{name}\"\n                else:\n                    function_call = f\".{name}()\"\n\n                item = (\n                    f\"{row['cartoon']} | {function_call:&lt;28} \\n   {row['description']}\"\n                )\n                items.append(item)\n        if len(items) &gt; 0:\n            print(header)\n            print(\"\\n\".join(items))\n</code></pre> <p>Return a summary of the history of actions that have gone into this <code>Rainbow</code>.</p> <p>Save this <code>Rainbow</code> out to a file.</p>"},{"location":"api/#chromatic.rainbows.helpers.get--parameters","title":"Parameters","text":"<p>key : str     The name of the attribute, property, or core dictionary item to get. default : any, optional     What to return if the attribute can't be found.</p>"},{"location":"api/#chromatic.rainbows.helpers.get--returns","title":"Returns","text":"<p>thing : any     The thing you were trying to get. If unavailable,     return the <code>default</code> (which by default is <code>None</code>)</p> Source code in <code>chromatic/rainbows/helpers/get.py</code> <pre><code>def get(self, key, default=None):\n    \"\"\"\n    Retrieve an attribute by its string name.\n    (This is a friendlier wrapper for `getattr()`).\n\n    `r.get('flux')` is identical to `r.flux`\n\n    This is different from indexing directly into\n    a core dictionary (for example, `r.fluxlike['flux']`),\n    because it can also be used to get the results of\n    properties that do calculations on the fly (for example,\n    `r.residuals` in the `RainbowWithModel` class).\n\n    Parameters\n    ----------\n    key : str\n        The name of the attribute, property, or core dictionary item to get.\n    default : any, optional\n        What to return if the attribute can't be found.\n\n    Returns\n    -------\n    thing : any\n        The thing you were trying to get. If unavailable,\n        return the `default` (which by default is `None`)\n    \"\"\"\n    try:\n        return getattr(self, key)\n    except AttributeError:\n        return default\n</code></pre>"},{"location":"api/#chromatic.rainbows.helpers.history--returns","title":"Returns","text":"<p>history : str     A string that does its best to try to summarize     all the actions that have been applied to this     <code>Rainbow</code> object from the moment it was created.     In some (but not all) cases, it may be possible     to copy, paste, and rerun this code to recreate     the <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/helpers/history.py</code> <pre><code>def history(self):\n    \"\"\"\n    Return a summary of the history of actions that have gone into this `Rainbow`.\n\n    Returns\n    -------\n    history : str\n        A string that does its best to try to summarize\n        all the actions that have been applied to this\n        `Rainbow` object from the moment it was created.\n        In some (but not all) cases, it may be possible\n        to copy, paste, and rerun this code to recreate\n        the `Rainbow`.\n    \"\"\"\n\n    calls = self.metadata[\"history\"]\n    return \"(\\n\" + \"\\n\".join(calls) + \"\\n)\"\n</code></pre>"},{"location":"api/#chromatic.rainbows.helpers.save--parameters","title":"Parameters","text":"<p>filepath : str     The filepath pointing to the file to be written.     (For now, it needs a <code>.rainbow.npy</code> extension.) format : str, optional     The file format of the file to be written. If <code>None</code>,     the format will be guessed automatically from the     filepath. **kw : dict, optional     All other keywords will be passed to the writer.</p> Source code in <code>chromatic/rainbows/helpers/save.py</code> <pre><code>def save(self, filepath=\"test.rainbow.npy\", format=None, **kw):\n    \"\"\"\n    Save this `Rainbow` out to a file.\n\n    Parameters\n    ----------\n    filepath : str\n        The filepath pointing to the file to be written.\n        (For now, it needs a `.rainbow.npy` extension.)\n    format : str, optional\n        The file format of the file to be written. If `None`,\n        the format will be guessed automatically from the\n        filepath.\n    **kw : dict, optional\n        All other keywords will be passed to the writer.\n    \"\"\"\n    # figure out the best writer\n    writer = guess_writer(filepath, format=format)\n\n    # use that writer to save the file\n    writer(self, filepath, **kw)\n</code></pre>"},{"location":"api/#actions","title":"\ud83c\udf08 Actions","text":"<p>Use 2D wavelength information to align onto a single 1D wavelength array.</p> <p>This relies on the existence of a <code>.fluxlike['wavelength_2d']</code> array, expressing the wavelength associated with each flux element. Those wavelengths will be used to (a) establish a new compromise wavelength grid and (b) bin the individual timepoints onto that new grid, effectively shifting the wavelengths to align.</p> <p>Attach a <code>fluxlike</code> model, thus making a new <code>RainbowWithModel.</code></p> <p>Having a model attached makes it possible to make calculations (residuals, chi^2) and visualizations comparing data to model.</p> <p>The <code>model</code> array will be stored in <code>.fluxlike['model']</code>. After running this to make a <code>RainbowWithModel</code> it's OK (and faster) to simply update <code>.fluxlike['model']</code> or <code>.model</code>.</p> <p>Compare this <code>Rainbow</code> to others.</p> <p>(still in development) This connects the current <code>Rainbow</code> to a collection of other <code>Rainbow</code> objects, which can then be visualized side-by-side in a uniform way.</p> <p>Flag outliers as not <code>ok</code>.</p> <p>This examines the flux array, identifies significant outliers, and marks them 0 in the <code>ok</code> array. The default procedure is to use a median filter to remove temporal trends (<code>remove_trends</code>), inflate the uncertainties based on the median-absolute-deviation scatter (<code>inflate_uncertainty</code>), and call points outliers if they deviate by more than a certain number of sigma (<code>how_many_sigma</code>) from the median-filtered level.</p> <p>The returned <code>Rainbow</code> object should be identical to the input one, except for the possibility that some elements in <code>ok</code> array will have been marked as zero. (The filtering or inflation are not applied to the returned object.)</p> <p>Fold this <code>Rainbow</code> to a period and reference epoch.</p> <p>This changes the times from some original time into a phased time, for example the time within an orbital period, relative to the time of mid-transit. This is mostly a convenience function for plotting data relative to mid-transit and/or trimming data based on orbital phase.</p> <p>Inflate uncertainties to match observed scatter.</p> <p>This is a quick and approximate tool for inflating the flux uncertainties in a <code>Rainbow</code> to match the observed scatter. With defaults, this will estimate the scatter using a robust median-absolute-deviation estimate of the standard deviation (<code>method='MAD'</code>), applied to time-series from which temporal trends have been removed (<code>remove_trends=True</code>), and inflate the uncertainties on a per-wavelength basis. The trend removal, by default by subtracting off local medians (<code>remove_trends_method='median_filter'</code>), will squash many types of both astrophysical and systematic trends, so this function should be used with caution in applicants where precise and reliable uncertainties are needed.</p> <p>Inject uncorrelated random noise into the <code>.flux</code> array.</p> <p>This injects independent noise to each data point, drawn from either a Gaussian or Poisson distribution. If the inputs can be scalar, or they can be arrays that we will try to broadcast into the shape of the <code>.flux</code> array.</p> <p>Inject some random outliers.</p> <p>To approximately simulate cosmic rays or other rare weird outliers, this randomly injects outliers into a small fraction of pixels. For this simple method, outliers will have the same amplitude, either as a ratio above the per-data-point or as a fixed number (if no uncertainties exist).</p> <p>Inject a stellar spectrum into the flux.</p> <p>This injects a constant stellar spectrum into all times in the <code>Rainbow</code>. Injection happens by multiplying the <code>.model</code> flux array, so for example a model that already has a transit in it will be scaled up to match the stellar spectrum in all wavelengths.</p> <p>Inject some (very cartoony) instrumental systematics.</p> <p>Here's the basic procedure:</p> <p>1) Generate some fake variables that vary either just with wavelength, just with time, or with both time and wavelength. Store these variables for later use. For example, these might represent an average <code>x</code> and <code>y</code> centroid of the trace on the detector (one for each time), or the background flux associated with each wavelength (one for each time and for each wavelength).</p> <p>2) Generate a flux model as some function of those variables. In reality, we probably don't know the actual relationship between these inputs and the flux, but we can imagine one!</p> <p>3) Inject the model flux into the <code>flux</code> of this Rainbow, and store the combined model in <code>systematics-model</code> and each individual component in <code>systematic-model-{...}</code>.</p> <p>Simulate a wavelength-dependent planetary transit.</p> <p>This uses one of a few methods to inject a transit signal into the <code>Rainbow</code>, allowing the transit depth to change with wavelength (for example due to a planet's effective radius changing with wavelength due to its atmospheric transmission spectrum). Other parameters can also be wavlength-dependent, but some (like period, inclination, etc...) probably shouldn't be.</p> <p>The current methods include:</p> <p><code>'trapezoid'</code> to inject a cartoon transit, using nomenclature from Winn (2010). This is the default method, to avoid package dependencies that can be finicky to compile and/or install on different operating systems.</p> <p><code>'exoplanet'</code> to inject a limb-darkened transit using exoplanet-core. This option requires <code>exoplanet-core</code> be installed, but it doesn't require complicated dependencies or compiling steps, so it's already included as a dependency.</p> <p>A quick tool to approximately remove trends.</p> <p>This function provides some simple tools for kludgily removing trends from a <code>Rainbow</code>, through a variety of filtering methods. If you just want to remove all slow trends, whether astrophysical or instrumental, options like the <code>median_filter</code> or <code>savgol_filter</code> will effectively suppress all trends on timescales longer than their filtering window. If you want a more restricted approach to removing long trends, the <code>polyfit</code> option allows you to fit out slow trends.</p> <p>Doppler shift the wavelengths of this <code>Rainbow</code>.</p> <p>This shifts the wavelengths in a <code>Rainbow</code> by applying a velocity shift. Positive velocities make wavelengths longer (redshift); negative velocities make wavelengths shorter (bluesfhit).</p> <p>Trim away bad wavelengths and/or times.</p> <p>If entire wavelengths or times are marked as not <code>ok</code>, we can probably remove them to simplify calculations and visualizations. This function will trim those away, by default only removing problem rows/columns on the ends, to maintain a contiguous block.</p>"},{"location":"api/#chromatic.rainbows.actions.align_wavelengths--parameters","title":"Parameters","text":"<p>minimum_acceptable_ok : float, optional     The numbers in the <code>.ok</code> attribute express \"how OK?\" each     data point is, ranging from 0 (not OK) to 1 (super OK).     In most cases, <code>.ok</code> will be binary, but there may be times     where it's intermediate (for example, if a bin was created     from some data that were not OK and some that were).     The <code>minimum_acceptable_ok</code> parameter allows you to specify what     level of OK-ness for a point to go into the binning.     Reasonable options may include:         minimum_acceptable_ok = 1               Only data points that are perfectly OK               will go into the binning. All other points               will effectively be interpolated over. Flux               uncertainties should be inflated appropriately,               but it's very possible to create correlated               bins next to each other if many of your ingoing               data points are not perfectly OK.         minimum_acceptable_ok = 1               All data points that aren't definitely not OK               will go into the binning. The OK-ness of points               will propagate onward for future binning.         minimum_acceptable_ok &lt; 0               All data points will be included in the bin.               The OK-ness will propagate onward. wscale : str, optional     What kind of a new wavelength axis should be created?     Options include:         'linear' = constant d[wavelength] between grid points         'log' = constant d[wavelength]/[wavelength] between grid points         'nonlinear' = the median wavelength grid for all time points supersampling : float, optional     By how many times should we increase or decrease the wavelength sampling?     In general, values &gt;1 will split each input wavelength grid point into     multiple supersampled wavelength grid points, values close to 1 will     produce approximately one output wavelength for each input wavelength,     and values &lt;1 will average multiple input wavelengths into a single output     wavelength bin.     Unless this is significantly less than 1, there's a good chance your output     array may have strong correlations between one or more adjacent wavelengths.     Be careful when trying to use the resulting uncertainties! visualize : bool     Should we make some plots showing how the shared wavelength     axis compares to the original input wavelength axes?</p>"},{"location":"api/#chromatic.rainbows.actions.align_wavelengths--returns","title":"Returns","text":"<p>rainbow : RainbowWithModel     A new <code>RainbowWithModel</code> object, with the model attached.</p> Source code in <code>chromatic/rainbows/actions/align_wavelengths.py</code> <pre><code>def align_wavelengths(\n    self,\n    minimum_acceptable_ok=1,\n    minimum_points_per_bin=0,\n    wscale=\"linear\",\n    supersampling=1,\n    visualize=False,\n):\n    \"\"\"\n    Use 2D wavelength information to align onto a single 1D wavelength array.\n\n    This relies on the existence of a `.fluxlike['wavelength_2d']` array,\n    expressing the wavelength associated with each flux element.\n    Those wavelengths will be used to (a) establish a new compromise\n    wavelength grid and (b) bin the individual timepoints onto that\n    new grid, effectively shifting the wavelengths to align.\n\n    Parameters\n    ----------\n    minimum_acceptable_ok : float, optional\n        The numbers in the `.ok` attribute express \"how OK?\" each\n        data point is, ranging from 0 (not OK) to 1 (super OK).\n        In most cases, `.ok` will be binary, but there may be times\n        where it's intermediate (for example, if a bin was created\n        from some data that were not OK and some that were).\n        The `minimum_acceptable_ok` parameter allows you to specify what\n        level of OK-ness for a point to go into the binning.\n        Reasonable options may include:\n            minimum_acceptable_ok = 1\n                  Only data points that are perfectly OK\n                  will go into the binning. All other points\n                  will effectively be interpolated over. Flux\n                  uncertainties *should* be inflated appropriately,\n                  but it's very possible to create correlated\n                  bins next to each other if many of your ingoing\n                  data points are not perfectly OK.\n            minimum_acceptable_ok = 1\n                  All data points that aren't definitely not OK\n                  will go into the binning. The OK-ness of points\n                  will propagate onward for future binning.\n            minimum_acceptable_ok &lt; 0\n                  All data points will be included in the bin.\n                  The OK-ness will propagate onward.\n    wscale : str, optional\n        What kind of a new wavelength axis should be created?\n        Options include:\n            'linear' = constant d[wavelength] between grid points\n            'log' = constant d[wavelength]/[wavelength] between grid points\n            'nonlinear' = the median wavelength grid for all time points\n    supersampling : float, optional\n        By how many times should we increase or decrease the wavelength sampling?\n        In general, values &gt;1 will split each input wavelength grid point into\n        multiple supersampled wavelength grid points, values close to 1 will\n        produce approximately one output wavelength for each input wavelength,\n        and values &lt;1 will average multiple input wavelengths into a single output\n        wavelength bin.\n        Unless this is significantly less than 1, there's a good chance your output\n        array may have strong correlations between one or more adjacent wavelengths.\n        Be careful when trying to use the resulting uncertainties!\n    visualize : bool\n        Should we make some plots showing how the shared wavelength\n        axis compares to the original input wavelength axes?\n\n    Returns\n    -------\n    rainbow : RainbowWithModel\n        A new `RainbowWithModel` object, with the model attached.\n    \"\"\"\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"align_wavelengths\", locals())\n\n    if \"wavelength_2d\" not in self.fluxlike:\n        cheerfully_suggest(\n            f\"\"\"\n        No 2D wavelength information was found, so\n        it's assumed wavelengths don't need to be aligned.\n        Wavelength alignment is being skipped!\n        \"\"\"\n        )\n        shifted = self._create_copy()\n    else:\n        # create a shared wavelength array\n        shared_wavelengths = self._create_shared_wavelength_axis(\n            wscale=wscale, supersampling=supersampling, visualize=visualize\n        )\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n\n            # bin the rainbow onto that new grid, starting from 2D wavelengths\n            shifted = self.bin_in_wavelength(\n                wavelength=shared_wavelengths,\n                minimum_acceptable_ok=minimum_acceptable_ok,\n                starting_wavelengths=\"2D\",\n                minimum_points_per_bin=minimum_points_per_bin,\n            )\n\n    # append the history entry to the new Rainbow\n    shifted._record_history_entry(h)\n\n    # return the new Rainbow\n    return shifted\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.attach_model--parameters","title":"Parameters","text":"<p>model : array, Quantity     An array of model values, with the same shape as 'flux' **kw : dict, optional     All other keywords will be interpreted as items     that can be added to a <code>Rainbow</code>. You might use this     to attach intermediate model steps or quantities.     Variable names ending with <code>_model</code> can be particularly     easily incorporated into multi-part model visualizations     (for example, <code>'planet_model'</code> or <code>'systematics_model'</code>).</p>"},{"location":"api/#chromatic.rainbows.actions.attach_model--returns","title":"Returns","text":"<p>rainbow : RainbowWithModel     A new <code>RainbowWithModel</code> object, with the model attached.</p> Source code in <code>chromatic/rainbows/actions/attach_model.py</code> <pre><code>def attach_model(self, model, **kw):\n    \"\"\"\n    Attach a `fluxlike` model, thus making a new `RainbowWithModel.`\n\n    Having a model attached makes it possible to make calculations\n    (residuals, chi^2) and visualizations comparing data to model.\n\n    The `model` array will be stored in `.fluxlike['model']`.\n    After running this to make a `RainbowWithModel` it's OK\n    (and faster) to simply update `.fluxlike['model']` or `.model`.\n\n    Parameters\n    ----------\n    model : array, Quantity\n        An array of model values, with the same shape as 'flux'\n    **kw : dict, optional\n        All other keywords will be interpreted as items\n        that can be added to a `Rainbow`. You might use this\n        to attach intermediate model steps or quantities.\n        Variable names ending with `_model` can be particularly\n        easily incorporated into multi-part model visualizations\n        (for example, `'planet_model'` or `'systematics_model'`).\n\n\n    Returns\n    -------\n    rainbow : RainbowWithModel\n        A new `RainbowWithModel` object, with the model attached.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"attach_model\", locals())\n\n    # make sure the shape is reasonable\n    assert np.shape(model) == np.shape(self.flux)\n\n    # add the model to the fluxlike array\n    inputs = self._create_copy()._get_core_dictionaries()\n    inputs[\"fluxlike\"][\"model\"] = model\n\n    # import here (rather than globally) to avoid recursion?\n    from ..withmodel import RainbowWithModel\n\n    # create new object\n    new = RainbowWithModel(**inputs)\n\n    # add other inputs to the model\n    for k, v in kw.items():\n        new.__setattr__(k, v)\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the RainboWithModel\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.bin","title":"<code>bin(self, dt=None, time=None, time_edges=None, ntimes=None, R=None, dw=None, wavelength=None, wavelength_edges=None, nwavelengths=None, minimum_acceptable_ok=1, minimum_points_per_bin=None, trim=True)</code>","text":"<p>Bin in wavelength and/or time.</p> <p>Average together some number of adjacent data points, in wavelength and/or time. For well-behaved data where data points are independent from each other, binning down by N data points should decrease the noise per bin by approximately 1/sqrt(N), making it easier to see subtle signals. To bin data points together, data are combined using inverse-variance weighting through interpolation of cumulative distributions, in an attempt to make sure that flux integrals between limits are maintained.</p> <p>Currently, the inverse-variance weighting is most reliable only for datasets that have been normalized to be close to 1. We still need to do a little work to make sure it works well on unnormalized datasets with dramatically non-uniform uncertainties.</p> <p>By default, time binning happens before wavelength binning. To control the order, use separate calls to <code>.bin()</code>.</p> <p>The time-setting order of precendence is [<code>time_edges</code>, <code>time</code>, <code>dt</code>, <code>ntimes</code>] The first will be used, and others will be ignored.</p> <p>The wavelength-setting order of precendence is [<code>wavelength_edges</code>, <code>wavelength</code>, <code>dw</code>, <code>R</code>, <code>nwavelengths</code>] The first will be used, and others will be ignored.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bin--parameters","title":"Parameters","text":"<p>dt : Quantity     The d(time) bin size for creating a grid     that is uniform in linear space. time : Quantity     An array of times, if you just want to give     it an entirely custom array.     The widths of the bins will be guessed from the centers     (well, if the spacing is uniform constant; pretty well     but not perfectly otherwise). time_edges : Quantity     An array of times for the edges of bins,     if you just want to give an entirely custom array.     The bins will span <code>time_edges[:-1]</code> to     <code>time_edges[1:]</code>, so the resulting binned     Rainbow will have <code>len(time_edges) - 1</code>     time bins associated with it. ntimes : int     A fixed number of time to bin together.     Binning will start from the 0th element of the     starting times; if you want to start from     a different index, trim before binning. R : float     The spectral resolution for creating a grid     that is uniform in logarithmic space. dw : Quantity     The d(wavelength) bin size for creating a grid     that is uniform in linear space. wavelength : Quantity     An array of wavelengths for the centers of bins,     if you just want to give an entirely custom array.     The widths of the bins will be guessed from the centers     (well, if the spacing is uniform constant; pretty well     but not perfectly otherwise). wavelength_edges : Quantity     An array of wavelengths for the edges of bins,     if you just want to give an entirely custom array.     The bins will span <code>wavelength_edges[:-1]</code> to     <code>wavelength_edges[1:]</code>, so the resulting binned     Rainbow will have <code>len(wavelength_edges) - 1</code>     wavelength bins associated with it. nwavelengths : int     A fixed number of wavelengths to bin together.     Binning will start from the 0th element of the     starting wavelengths; if you want to start from     a different index, trim before binning. minimum_acceptable_ok : float     The numbers in the <code>.ok</code> attribute express \"how OK?\" each     data point is, ranging from 0 (not OK) to 1 (super OK).     In most cases, <code>.ok</code> will be binary, but there may be times     where it's intermediate (for example, if a bin was created     from some data that were not OK and some that were).     The <code>minimum_acceptable_ok</code> parameter allows you to specify what     level of OK-ness for a point to go into the binning.     Reasonable options may include:         minimum_acceptable_ok = 1               Only data points that are perfectly OK               will go into the binning.         minimum_acceptable_ok = 1e-10               All data points that aren't definitely not OK               will go into the binning.         minimum_acceptable_ok = 0               All data points will be included in the bin. minimum_points_per_bin : float     If you're creating bins that are smaller than those in     the original dataset, it's possible to end up with bins     that effectively contain fewer than one original datapoint     (in the sense that the contribution of one original datapoint     might be split across multiple new bins). By default,     we allow this behavior with <code>minimum_points_per_bin=0</code>, but you can     limit your result to only bins that contain one or more     original datapoints with <code>minimum_points_per_bin=1</code>. trim : bool     Should any wavelengths or columns that end up     as entirely nan be trimmed out of the result?     (default = True)</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bin--returns","title":"Returns","text":"<p>binned : Rainbow     The binned <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/actions/binning.py</code> <pre><code>def bin(\n    self,\n    dt=None,\n    time=None,\n    time_edges=None,\n    ntimes=None,\n    R=None,\n    dw=None,\n    wavelength=None,\n    wavelength_edges=None,\n    nwavelengths=None,\n    minimum_acceptable_ok=1,\n    minimum_points_per_bin=None,\n    trim=True,\n):\n    \"\"\"\n    Bin in wavelength and/or time.\n\n    Average together some number of adjacent data points,\n    in wavelength and/or time. For well-behaved data where\n    data points are independent from each other, binning down\n    by N data points should decrease the noise per bin by\n    approximately 1/sqrt(N), making it easier to see subtle\n    signals. To bin data points together, data are combined\n    using inverse-variance weighting through interpolation\n    of cumulative distributions, in an attempt to make sure\n    that flux integrals between limits are maintained.\n\n    Currently, the inverse-variance weighting is most reliable\n    only for datasets that have been normalized to be close\n    to 1. We still need to do a little work to make sure\n    it works well on unnormalized datasets with dramatically\n    non-uniform uncertainties.\n\n    By default, time binning happens before wavelength binning.\n    To control the order, use separate calls to `.bin()`.\n\n    The time-setting order of precendence is\n    [`time_edges`, `time`, `dt`, `ntimes`]\n    The first will be used, and others will be ignored.\n\n    The wavelength-setting order of precendence is\n    [`wavelength_edges`, `wavelength`, `dw`, `R`, `nwavelengths`]\n    The first will be used, and others will be ignored.\n\n\n    Parameters\n    ----------\n    dt : Quantity\n        The d(time) bin size for creating a grid\n        that is uniform in linear space.\n    time : Quantity\n        An array of times, if you just want to give\n        it an entirely custom array.\n        The widths of the bins will be guessed from the centers\n        (well, if the spacing is uniform constant; pretty well\n        but not perfectly otherwise).\n    time_edges : Quantity\n        An array of times for the edges of bins,\n        if you just want to give an entirely custom array.\n        The bins will span `time_edges[:-1]` to\n        `time_edges[1:]`, so the resulting binned\n        Rainbow will have `len(time_edges) - 1`\n        time bins associated with it.\n    ntimes : int\n        A fixed number of time to bin together.\n        Binning will start from the 0th element of the\n        starting times; if you want to start from\n        a different index, trim before binning.\n    R : float\n        The spectral resolution for creating a grid\n        that is uniform in logarithmic space.\n    dw : Quantity\n        The d(wavelength) bin size for creating a grid\n        that is uniform in linear space.\n    wavelength : Quantity\n        An array of wavelengths for the centers of bins,\n        if you just want to give an entirely custom array.\n        The widths of the bins will be guessed from the centers\n        (well, if the spacing is uniform constant; pretty well\n        but not perfectly otherwise).\n    wavelength_edges : Quantity\n        An array of wavelengths for the edges of bins,\n        if you just want to give an entirely custom array.\n        The bins will span `wavelength_edges[:-1]` to\n        `wavelength_edges[1:]`, so the resulting binned\n        Rainbow will have `len(wavelength_edges) - 1`\n        wavelength bins associated with it.\n    nwavelengths : int\n        A fixed number of wavelengths to bin together.\n        Binning will start from the 0th element of the\n        starting wavelengths; if you want to start from\n        a different index, trim before binning.\n    minimum_acceptable_ok : float\n        The numbers in the `.ok` attribute express \"how OK?\" each\n        data point is, ranging from 0 (not OK) to 1 (super OK).\n        In most cases, `.ok` will be binary, but there may be times\n        where it's intermediate (for example, if a bin was created\n        from some data that were not OK and some that were).\n        The `minimum_acceptable_ok` parameter allows you to specify what\n        level of OK-ness for a point to go into the binning.\n        Reasonable options may include:\n            minimum_acceptable_ok = 1\n                  Only data points that are perfectly OK\n                  will go into the binning.\n            minimum_acceptable_ok = 1e-10\n                  All data points that aren't definitely not OK\n                  will go into the binning.\n            minimum_acceptable_ok = 0\n                  All data points will be included in the bin.\n    minimum_points_per_bin : float\n        If you're creating bins that are smaller than those in\n        the original dataset, it's possible to end up with bins\n        that effectively contain fewer than one original datapoint\n        (in the sense that the contribution of one original datapoint\n        might be split across multiple new bins). By default,\n        we allow this behavior with `minimum_points_per_bin=0`, but you can\n        limit your result to only bins that contain one or more\n        original datapoints with `minimum_points_per_bin=1`.\n    trim : bool\n        Should any wavelengths or columns that end up\n        as entirely nan be trimmed out of the result?\n        (default = True)\n\n    Returns\n    -------\n    binned : Rainbow\n        The binned `Rainbow`.\n    \"\"\"\n\n    # bin first in time\n    binned_in_time = self.bin_in_time(\n        dt=dt,\n        time=time,\n        time_edges=time_edges,\n        ntimes=ntimes,\n        minimum_acceptable_ok=minimum_acceptable_ok,\n        minimum_points_per_bin=minimum_points_per_bin,\n        trim=trim,\n    )\n\n    # then bin in wavelength\n    binned = binned_in_time.bin_in_wavelength(\n        R=R,\n        dw=dw,\n        wavelength=wavelength,\n        wavelength_edges=wavelength_edges,\n        nwavelengths=nwavelengths,\n        minimum_acceptable_ok=minimum_acceptable_ok,\n        minimum_points_per_bin=minimum_points_per_bin,\n        trim=trim,\n    )\n\n    # return the binned object\n    return binned\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.bin_in_time","title":"<code>bin_in_time(self, dt=None, time=None, time_edges=None, ntimes=None, minimum_acceptable_ok=1, minimum_points_per_bin=None, trim=True)</code>","text":"<p>Bin in time.</p> <p>The time-setting order of precendence is [<code>time_edges</code>, <code>time</code>, <code>dt</code>, <code>ntimes</code>] The first will be used, and others will be ignored.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bin_in_time--parameters","title":"Parameters","text":"<p>dt : Quantity     The d(time) bin size for creating a grid     that is uniform in linear space. time : Quantity     An array of times, if you just want to give     it an entirely custom array.     The widths of the bins will be guessed from the centers     (well, if the spacing is uniform constant; pretty well     but not perfectly otherwise). time_edges : Quantity     An array of times for the edges of bins,     if you just want to give an entirely custom array.     The bins will span <code>time_edges[:-1]</code> to     <code>time_edges[1:]</code>, so the resulting binned     <code>Rainbow</code> will have <code>len(time_edges) - 1</code>     time bins associated with it. ntimes : int     A fixed number of time to bin together.     Binning will start from the 0th element of the     starting times; if you want to start from     a different index, trim before binning. minimum_acceptable_ok : float     The numbers in the <code>.ok</code> attribute express \"how OK?\" each     data point is, ranging from 0 (not OK) to 1 (super OK).     In most cases, <code>.ok</code> will be binary, but there may be times     where it's intermediate (for example, if a bin was created     from some data that were not OK and some that were).     The <code>minimum_acceptable_ok</code> parameter allows you to specify what     level of OK-ness for a point to go into the binning.     Reasonable options may include:         minimum_acceptable_ok = 1               Only data points that are perfectly OK               will go into the binning.         minimum_acceptable_ok = 1e-10               All data points that aren't definitely not OK               will go into the binning.         minimum_acceptable_ok = 0               All data points will be included in the bin. minimum_points_per_bin : float     If you're creating bins that are smaller than those in     the original dataset, it's possible to end up with bins     that effectively contain fewer than one original datapoint     (in the sense that the contribution of one original datapoint     might be split across multiple new bins). By default,     we allow this behavior with <code>minimum_points_per_bin=0</code>, but you can     limit your result to only bins that contain one or more     original datapoints with <code>minimum_points_per_bin=1</code>. trim : bool     Should any wavelengths or columns that end up     as entirely nan be trimmed out of the result?     (default = True)</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bin_in_time--returns","title":"Returns","text":"<p>binned : Rainbow     The binned <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/actions/binning.py</code> <pre><code>def bin_in_time(\n    self,\n    dt=None,\n    time=None,\n    time_edges=None,\n    ntimes=None,\n    minimum_acceptable_ok=1,\n    minimum_points_per_bin=None,\n    trim=True,\n):\n    \"\"\"\n    Bin in time.\n\n    The time-setting order of precendence is\n    [`time_edges`, `time`, `dt`, `ntimes`]\n    The first will be used, and others will be ignored.\n\n\n    Parameters\n    ----------\n    dt : Quantity\n        The d(time) bin size for creating a grid\n        that is uniform in linear space.\n    time : Quantity\n        An array of times, if you just want to give\n        it an entirely custom array.\n        The widths of the bins will be guessed from the centers\n        (well, if the spacing is uniform constant; pretty well\n        but not perfectly otherwise).\n    time_edges : Quantity\n        An array of times for the edges of bins,\n        if you just want to give an entirely custom array.\n        The bins will span `time_edges[:-1]` to\n        `time_edges[1:]`, so the resulting binned\n        `Rainbow` will have `len(time_edges) - 1`\n        time bins associated with it.\n    ntimes : int\n        A fixed number of time to bin together.\n        Binning will start from the 0th element of the\n        starting times; if you want to start from\n        a different index, trim before binning.\n    minimum_acceptable_ok : float\n        The numbers in the `.ok` attribute express \"how OK?\" each\n        data point is, ranging from 0 (not OK) to 1 (super OK).\n        In most cases, `.ok` will be binary, but there may be times\n        where it's intermediate (for example, if a bin was created\n        from some data that were not OK and some that were).\n        The `minimum_acceptable_ok` parameter allows you to specify what\n        level of OK-ness for a point to go into the binning.\n        Reasonable options may include:\n            minimum_acceptable_ok = 1\n                  Only data points that are perfectly OK\n                  will go into the binning.\n            minimum_acceptable_ok = 1e-10\n                  All data points that aren't definitely not OK\n                  will go into the binning.\n            minimum_acceptable_ok = 0\n                  All data points will be included in the bin.\n    minimum_points_per_bin : float\n        If you're creating bins that are smaller than those in\n        the original dataset, it's possible to end up with bins\n        that effectively contain fewer than one original datapoint\n        (in the sense that the contribution of one original datapoint\n        might be split across multiple new bins). By default,\n        we allow this behavior with `minimum_points_per_bin=0`, but you can\n        limit your result to only bins that contain one or more\n        original datapoints with `minimum_points_per_bin=1`.\n    trim : bool\n        Should any wavelengths or columns that end up\n        as entirely nan be trimmed out of the result?\n        (default = True)\n\n    Returns\n    -------\n    binned : Rainbow\n        The binned `Rainbow`.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"bin_in_time\", locals())\n\n    # if no bin information is provided, don't bin\n    if np.all([x is None for x in [dt, time, time_edges, ntimes]]):\n        return self\n\n    # set up binning parameters\n    binkw = dict(weighting=\"inversevariance\", drop_nans=False)\n\n    # [`time_edges`, `time`, `dt`, `ntimes`]\n    if time_edges is not None:\n        binkw[\"newx_edges\"] = time_edges\n    elif time is not None:\n        binkw[\"newx\"] = time\n    elif dt is not None:\n        binkw[\"dx\"] = dt\n    elif ntimes is not None:\n        binkw[\"nx\"] = ntimes\n\n    # create a new, empty Rainbow\n    new = self._create_copy()\n\n    # populate the wavelength information\n    new.wavelike = {**self.wavelike}\n    new.metadata[\"wscale\"] = self.wscale\n\n    # bin the time-like variables\n    # Technically, we should include uncertainties here too,\n    # so that times/wavelengths are weighted more toward\n    # inputs with higher flux weights (e.g. smaller variance),\n    # but that will make non-uniform grids that will be\n    # really hard to deal with.\n    new.timelike = {}\n    for k in self.timelike:\n        binned = bintogrid(x=self.time, y=self.timelike[k], unc=None, **binkw)\n        new.timelike[k] = binned[\"y\"]\n    new.timelike[\"time\"] = binned[\"x\"]\n    new.timelike[\"time_lower\"] = binned[\"x_edge_lower\"]\n    new.timelike[\"time_upper\"] = binned[\"x_edge_upper\"]\n    new.timelike[\"unbinned_times_per_binned_time\"] = binned[\"N_unbinned/N_binned\"]\n\n    # bin the flux-like variables\n    # TODO (add more careful treatment of uncertainty + DQ)\n    # TODO (think about cleverer bintogrid for 2D arrays?)\n    new.fluxlike = {}\n    ok = self.ok\n    # loop through wavelengths\n    for w in tqdm(np.arange(new.nwave), leave=False):\n\n        '''\n        if k == \"uncertainty\":\n            cheerfully_suggest(\n                \"\"\"\n            Uncertainties and/or data quality flags might\n            not be handled absolutely perfectly yet...\n            \"\"\"\n            )'''\n\n        for k in self.fluxlike:\n            # mask out \"bad\" wavelengths\n            time_is_bad = ok[w, :] &lt; minimum_acceptable_ok\n            if (self.uncertainty is None) or np.all(self.uncertainty == 0):\n                uncertainty_for_binning = np.ones(self.ntime).astype(bool)\n            elif k in self._keys_that_get_uncertainty_weighting:\n                uncertainty_for_binning = self.uncertainty[w, :] * 1\n            else:\n                uncertainty_for_binning = np.ones(self.ntime).astype(bool)\n\n            if k != \"ok\":\n                uncertainty_for_binning[time_is_bad] = np.inf\n\n            # bin the quantities for this wavelength\n            binned = bintogrid(\n                x=self.time[:],\n                y=self.fluxlike[k][w, :],\n                unc=uncertainty_for_binning,\n                **binkw,\n            )\n\n            # if necessary, create a new fluxlike array\n            if k not in new.fluxlike:\n                new_shape = (new.nwave, new.ntime)\n                new.fluxlike[k] = np.zeros(new_shape)\n                if isinstance(self.fluxlike[k], u.Quantity):\n                    new.fluxlike[k] *= self.fluxlike[k].unit\n\n            # store the binned array in the appropriate place\n            if k == \"uncertainty\":\n                # uncertainties are usually standard error on the mean\n                new.fluxlike[k][w, :] = binned[\"uncertainty\"]\n            else:\n                # note: all quantities are weighted the same as flux (probably inversevariance)\n                new.fluxlike[k][w, :] = binned[\"y\"]\n\n    if (new.nwave == 0) or (new.ntime == 0):\n        message = f\"\"\"\n        You tried to bin {self} to {new}.\n\n        After accounting for `minimum_acceptable_ok &gt; {minimum_acceptable_ok}`,\n        all new bins would end up with no usable data points.\n        Please (a) make sure your input `Rainbow` has at least\n        one wavelength and time, (b) check `.ok` accurately expresses\n        which data you think are usable, (c) change the `minimum_acceptable_ok`\n        keyword for `.bin` to a smaller value, and/or (d) try larger bins.\n        \"\"\"\n        cheerfully_suggest(message)\n        raise RuntimeError(\"No good data to bin! (see above)\")\n\n    # make sure dictionaries are on the up and up\n    new._validate_core_dictionaries()\n\n    # figure out the scales, after binning\n    new._guess_wscale()\n    new._guess_tscale()\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # deal with bins that are smaller than original\n    N = new.timelike[\"unbinned_times_per_binned_time\"]\n    if minimum_points_per_bin is None:\n        _warn_about_weird_binning(N, \"time\")\n    else:\n        ok = new.timelike.get(\"ok\", np.ones(new.ntime, bool))\n        new.timelike[\"ok\"] = ok * (N &gt;= minimum_points_per_bin)\n\n    # return the new Rainbow (with trimming if necessary)\n    if trim:\n        return new.trim_times(minimum_acceptable_ok=minimum_acceptable_ok)\n    else:\n        return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.bin_in_wavelength","title":"<code>bin_in_wavelength(self, R=None, dw=None, wavelength=None, wavelength_edges=None, nwavelengths=None, minimum_acceptable_ok=1, minimum_points_per_bin=None, trim=True, starting_wavelengths='1D')</code>","text":"<p>Bin in wavelength.</p> <p>The wavelength-setting order of precendence is [<code>wavelength_edges</code>, <code>wavelength</code>, <code>dw</code>, <code>R</code>, <code>nwavelengths</code>] The first will be used, and others will be ignored.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bin_in_wavelength--parameters","title":"Parameters","text":"<p>R : float     The spectral resolution for creating a grid     that is uniform in logarithmic space. dw : Quantity     The d(wavelength) bin size for creating a grid     that is uniform in linear space. wavelength : Quantity     An array of wavelength centers, if you just want to give     it an entirely custom array. The widths of the bins     will be guessed from the centers. It will do a good     job if the widths are constant, but don't 100% trust     it otherwise. wavelength_edges : Quantity     An array of wavelengths for the edges of bins,     if you just want to give an entirely custom array.     The bins will span <code>wavelength_edges[:-1]</code> to     <code>wavelength_edges[1:]</code>, so the resulting binned     <code>Rainbow</code> will have <code>len(wavelength_edges) - 1</code>     wavelength bins associated with it. nwavelengths : int     A fixed number of wavelengths to bin together.     Binning will start from the 0th element of the     starting wavelengths; if you want to start from     a different index, trim before binning. minimum_acceptable_ok : float     The numbers in the <code>.ok</code> attribute express \"how OK?\" each     data point is, ranging from 0 (not OK) to 1 (super OK).     In most cases, <code>.ok</code> will be binary, but there may be times     where it's intermediate (for example, if a bin was created     from some data that were not OK and some that were).     The <code>minimum_acceptable_ok</code> parameter allows you to specify what     level of OK-ness for a point to go into the binning.     Reasonable options may include:         minimum_acceptable_ok = 1               Only data points that are perfectly OK               will go into the binning.         minimum_acceptable_ok = 1e-10               All data points that aren't definitely not OK               will go into the binning.         minimum_acceptable_ok = 0               All data points will be included in the bin. minimum_points_per_bin : float     If you're creating bins that are smaller than those in     the original dataset, it's possible to end up with bins     that effectively contain fewer than one original datapoint     (in the sense that the contribution of one original datapoint     might be split across multiple new bins). By default,     we allow this behavior with <code>minimum_points_per_bin=0</code>, but you can     limit your result to only bins that contain one or more     original datapoints with <code>minimum_points_per_bin=1</code>. trim : bool     Should any wavelengths or columns that end up     as entirely nan be trimmed out of the result?     (default = True) starting_wavelengths : str     What wavelengths should be used as the starting     value from which we will be binning? Options include:     '1D' = (default) the shared 1D wavelengths for all times            stored in <code>.wavelike['wavelength']</code>     '2D' = (used only by <code>align_wavelengths</code>) the per-time 2D array            stored in <code>.fluxlike['wavelength']</code>     [Most users probably don't need to change this from default.]</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bin_in_wavelength--returns","title":"Returns","text":"<p>binned : Rainbow     The binned <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/actions/binning.py</code> <pre><code>def bin_in_wavelength(\n    self,\n    R=None,\n    dw=None,\n    wavelength=None,\n    wavelength_edges=None,\n    nwavelengths=None,\n    minimum_acceptable_ok=1,\n    minimum_points_per_bin=None,\n    trim=True,\n    starting_wavelengths=\"1D\",\n):\n    \"\"\"\n    Bin in wavelength.\n\n    The wavelength-setting order of precendence is\n    [`wavelength_edges`, `wavelength`, `dw`, `R`, `nwavelengths`]\n    The first will be used, and others will be ignored.\n\n    Parameters\n    ----------\n    R : float\n        The spectral resolution for creating a grid\n        that is uniform in logarithmic space.\n    dw : Quantity\n        The d(wavelength) bin size for creating a grid\n        that is uniform in linear space.\n    wavelength : Quantity\n        An array of wavelength centers, if you just want to give\n        it an entirely custom array. The widths of the bins\n        will be guessed from the centers. It will do a good\n        job if the widths are constant, but don't 100% trust\n        it otherwise.\n    wavelength_edges : Quantity\n        An array of wavelengths for the edges of bins,\n        if you just want to give an entirely custom array.\n        The bins will span `wavelength_edges[:-1]` to\n        `wavelength_edges[1:]`, so the resulting binned\n        `Rainbow` will have `len(wavelength_edges) - 1`\n        wavelength bins associated with it.\n    nwavelengths : int\n        A fixed number of wavelengths to bin together.\n        Binning will start from the 0th element of the\n        starting wavelengths; if you want to start from\n        a different index, trim before binning.\n    minimum_acceptable_ok : float\n        The numbers in the `.ok` attribute express \"how OK?\" each\n        data point is, ranging from 0 (not OK) to 1 (super OK).\n        In most cases, `.ok` will be binary, but there may be times\n        where it's intermediate (for example, if a bin was created\n        from some data that were not OK and some that were).\n        The `minimum_acceptable_ok` parameter allows you to specify what\n        level of OK-ness for a point to go into the binning.\n        Reasonable options may include:\n            minimum_acceptable_ok = 1\n                  Only data points that are perfectly OK\n                  will go into the binning.\n            minimum_acceptable_ok = 1e-10\n                  All data points that aren't definitely not OK\n                  will go into the binning.\n            minimum_acceptable_ok = 0\n                  All data points will be included in the bin.\n    minimum_points_per_bin : float\n        If you're creating bins that are smaller than those in\n        the original dataset, it's possible to end up with bins\n        that effectively contain fewer than one original datapoint\n        (in the sense that the contribution of one original datapoint\n        might be split across multiple new bins). By default,\n        we allow this behavior with `minimum_points_per_bin=0`, but you can\n        limit your result to only bins that contain one or more\n        original datapoints with `minimum_points_per_bin=1`.\n    trim : bool\n        Should any wavelengths or columns that end up\n        as entirely nan be trimmed out of the result?\n        (default = True)\n    starting_wavelengths : str\n        What wavelengths should be used as the starting\n        value from which we will be binning? Options include:\n        '1D' = (default) the shared 1D wavelengths for all times\n               stored in `.wavelike['wavelength']`\n        '2D' = (used only by `align_wavelengths`) the per-time 2D array\n               stored in `.fluxlike['wavelength']`\n        [Most users probably don't need to change this from default.]\n\n    Returns\n    -------\n    binned : Rainbow\n        The binned `Rainbow`.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"bin_in_wavelength\", locals())\n\n    # if no bin information is provided, don't bin\n    if (\n        (wavelength is None)\n        and (wavelength_edges is None)\n        and (nwavelengths is None)\n        and (dw is None)\n        and (R is None)\n    ):\n        return self\n\n    if (\n        (self._is_probably_normalized() == False)\n        and (self.uncertainty is not None)\n        and np.any(self.uncertainty != 0)\n    ):\n\n        cheerfully_suggest(\n            f\"\"\"\n        It looks like you're trying to bin in wavelength for a\n        `Rainbow` object that might not be normalized. In the\n        current version of `chromatic`, binning before normalizing\n        might give inaccurate results if the typical uncertainty\n        varies strongly with wavelength.\n\n        Please consider normalizing first, for example with\n        `rainbow.normalize().bin(...)`\n        so that all uncertainties will effectively be relative,\n        and the inverse variance weighting used for binning\n        wavelengths together will give more reasonable answers.\n\n        If you really need to bin before normalizing, please submit\n        an Issue at github.com/zkbt/chromatic/, and we'll try to\n        prioritize implementing a statistically sound solution as\n        soon as possible!\n        \"\"\"\n        )\n\n    # set up binning parameters\n    binkw = dict(weighting=\"inversevariance\", drop_nans=False)\n\n    # [`wavelength_edges`, `wavelength`, `dw`, `R`, `nwavelengths`]\n    if wavelength_edges is not None:\n        binning_function = bintogrid\n        binkw[\"newx_edges\"] = wavelength_edges\n    elif wavelength is not None:\n        binning_function = bintogrid\n        binkw[\"newx\"] = wavelength\n    elif dw is not None:\n        binning_function = bintogrid\n        binkw[\"dx\"] = dw\n    elif R is not None:\n        binning_function = bintoR\n        binkw[\"R\"] = R\n    elif nwavelengths is not None:\n        binning_function = bintogrid\n        binkw[\"nx\"] = nwavelengths\n\n    # create a new, empty Rainbow\n    new = self._create_copy()\n\n    # populate the time information\n    new.timelike = {**self.timelike}\n\n    # bin the time-like variables\n    # TODO (add more careful treatment of uncertainty + DQ)\n    new.wavelike = {}\n    for k in self.wavelike:\n        binned = binning_function(\n            x=self.wavelike[\"wavelength\"], y=self.wavelike[k], unc=None, **binkw\n        )\n        new.wavelike[k] = binned[\"y\"]\n    new.wavelike[\"wavelength\"] = binned[\"x\"]\n    new.wavelike[\"wavelength_lower\"] = binned[\"x_edge_lower\"]\n    new.wavelike[\"wavelength_upper\"] = binned[\"x_edge_upper\"]\n    new.wavelike[\"unbinned_wavelengths_per_binned_wavelength\"] = binned[\n        \"N_unbinned/N_binned\"\n    ]\n    uncertainty_per_wavelength = self.get_expected_uncertainty()\n    uncertainty_weighted_binned = binning_function(\n        x=self.wavelike[\"wavelength\"],\n        y=self.wavelike[\"wavelength\"],\n        unc=uncertainty_per_wavelength,\n        **binkw,\n    )\n    new.wavelike[\"effective_wavelength\"] = uncertainty_weighted_binned[\"y\"]\n\n    # bin the flux-like variables\n    # TODO (add more careful treatment of uncertainty + DQ)\n    # TODO (think about cleverer bintogrid for 2D arrays)\n    new.fluxlike = {}\n\n    # get a fluxlike array of what's OK to include in the bins\n    ok = self.ok\n    for t in tqdm(np.arange(new.ntime), leave=False):\n\n        for k in self.fluxlike:\n\n            # mask out \"bad\" wavelengths\n            wavelength_is_bad = ok[:, t] &lt; minimum_acceptable_ok\n\n            if (self.uncertainty is None) or np.all(self.uncertainty == 0):\n                uncertainty_for_binning = np.ones(self.nwave).astype(bool)\n            elif k in self._keys_that_get_uncertainty_weighting:\n                uncertainty_for_binning = self.uncertainty[:, t] * 1\n            else:\n                uncertainty_for_binning = np.ones(self.nwave).astype(bool)\n            if k != \"ok\":\n                uncertainty_for_binning[wavelength_is_bad] = np.inf\n\n            if starting_wavelengths.upper() == \"1D\":\n                w = self.wavelike[\"wavelength\"][:]\n            elif starting_wavelengths.upper() == \"2D\":\n                w = self.fluxlike[\"wavelength_2d\"][:, t]\n            # bin the quantities for this time\n            binned = binning_function(\n                x=w,\n                y=self.fluxlike[k][:, t] * 1,\n                unc=uncertainty_for_binning,\n                **binkw,\n            )\n\n            # if necessary, create a new fluxlike array\n            if k not in new.fluxlike:\n                new_shape = (new.nwave, new.ntime)\n                new.fluxlike[k] = np.zeros(new_shape)\n                if isinstance(self.fluxlike[k], u.Quantity):\n                    new.fluxlike[k] *= self.fluxlike[k].unit\n\n            # store the binned array in the appropriate place\n            if k == \"uncertainty\":\n                # uncertainties are usually standard error on the mean\n                new.fluxlike[k][:, t] = binned[\"uncertainty\"]\n            else:\n                # note: all quantities are weighted the same as flux (probably inversevariance)\n                new.fluxlike[k][:, t] = binned[\"y\"]\n\n    if (new.nwave == 0) or (new.ntime == 0):\n        message = f\"\"\"\n        You tried to bin {self} to {new}.\n\n        After accounting for `minimum_acceptable_ok &gt; {minimum_acceptable_ok}`,\n        all new bins would end up with no usable data points.\n        Please (a) make sure your input `Rainbow` has at least\n        one wavelength and time, (b) check `.ok` accurately expresses\n        which data you think are usable, (c) change the `minimum_acceptable_ok`\n        keyword for `.bin` to a smaller value, and/or (d) try larger bins.\n        \"\"\"\n        cheerfully_suggest(message)\n        raise RuntimeError(\"No good data to bin! (see above)\")\n\n    # make sure dictionaries are on the up and up\n    new._validate_core_dictionaries()\n\n    # figure out the scales, after binning\n    new._guess_wscale()\n    new._guess_tscale()\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # deal with bins that are smaller than original\n    N = new.wavelike[\"unbinned_wavelengths_per_binned_wavelength\"]\n    if minimum_points_per_bin is None:\n        _warn_about_weird_binning(N, \"wavelength\")\n    else:\n        ok = new.wavelike.get(\"ok\", np.ones(new.nwave, bool))\n        new.wavelike[\"ok\"] = ok * (N &gt;= minimum_points_per_bin)\n\n    # return the new Rainbow (with trimming if necessary)\n    if trim:\n        return new.trim_wavelengths(minimum_acceptable_ok=minimum_acceptable_ok)\n    else:\n        return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.actions.binning.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.actions.binning.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.get_average_lightcurve_as_rainbow","title":"<code>get_average_lightcurve_as_rainbow(self)</code>","text":"<p>Produce a wavelength-integrated light curve.</p> <p>The average across wavelengths is uncertainty-weighted.</p> <p>This uses <code>bin</code>, which is a horribly slow way of doing what is fundamentally a very simply array calculation, because we don't need to deal with partial pixels.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.get_average_lightcurve_as_rainbow--returns","title":"Returns","text":"<p>lc : Rainbow     A <code>Rainbow</code> object with just one wavelength.</p> Source code in <code>chromatic/rainbows/actions/binning.py</code> <pre><code>def get_average_lightcurve_as_rainbow(self):\n    \"\"\"\n    Produce a wavelength-integrated light curve.\n\n    The average across wavelengths is uncertainty-weighted.\n\n    This uses `bin`, which is a horribly slow way of doing what is\n    fundamentally a very simply array calculation, because we\n    don't need to deal with partial pixels.\n\n    Returns\n    -------\n    lc : Rainbow\n        A `Rainbow` object with just one wavelength.\n    \"\"\"\n    h = self._create_history_entry(\"get_average_spectrum_as_rainbow\", locals())\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        new = self.bin(nwavelengths=self.nwave, trim=False)\n\n    new._record_history_entry(h)\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.get_average_spectrum_as_rainbow","title":"<code>get_average_spectrum_as_rainbow(self)</code>","text":"<p>Produce a time-integrated spectrum.</p> <p>The average across times is uncertainty-weighted.</p> <p>This uses <code>bin</code>, which is a horribly slow way of doing what is fundamentally a very simply array calculation, because we don't need to deal with partial pixels.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.get_average_spectrum_as_rainbow--returns","title":"Returns","text":"<p>lc : Rainbow     A <code>Rainbow</code> object with just one time.</p> Source code in <code>chromatic/rainbows/actions/binning.py</code> <pre><code>def get_average_spectrum_as_rainbow(self):\n    \"\"\"\n    Produce a time-integrated spectrum.\n\n    The average across times is uncertainty-weighted.\n\n    This uses `bin`, which is a horribly slow way of doing what is\n    fundamentally a very simply array calculation, because we\n    don't need to deal with partial pixels.\n\n    Returns\n    -------\n    lc : Rainbow\n        A `Rainbow` object with just one time.\n    \"\"\"\n    h = self._create_history_entry(\"get_average_spectrum_as_rainbow\", locals())\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        new = self.bin(ntimes=self.ntime, trim=False)\n\n    new._record_history_entry(h)\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.actions.binning.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.actions.binning.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.actions.binning.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.binning.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.actions.binning.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.actions.binning.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.compare--parameters","title":"Parameters","text":"<p>rainbows : list     A list containing one or more other <code>Rainbow</code> objects.     If you only want to compare with one other <code>Rainbow</code>,     supply it in a 1-element list like <code>.compare([other])</code></p>"},{"location":"api/#chromatic.rainbows.actions.compare--returns","title":"Returns","text":"<p>rainbow : MultiRainbow     A <code>MultiRainbow</code> comparison object including all input <code>Rainbow</code>s</p> Source code in <code>chromatic/rainbows/actions/compare.py</code> <pre><code>def compare(self, rainbows):\n    \"\"\"\n    Compare this `Rainbow` to others.\n\n    (still in development) This connects the current `Rainbow`\n    to a collection of other `Rainbow` objects, which can then\n    be visualized side-by-side in a uniform way.\n\n    Parameters\n    ----------\n    rainbows : list\n        A list containing one or more other `Rainbow` objects.\n        If you only want to compare with one other `Rainbow`,\n        supply it in a 1-element list like `.compare([other])`\n\n    Returns\n    -------\n    rainbow : MultiRainbow\n        A `MultiRainbow` comparison object including all input `Rainbow`s\n    \"\"\"\n    try:\n        rainbows.remove(self)\n    except (ValueError, IndexError):\n        pass\n    return compare_rainbows([self] + rainbows)\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.flag_outliers--parameters","title":"Parameters","text":"<p>how_many_sigma : float, optional     Standard deviations (sigmas) allowed for individual data     points before they are flagged as outliers. remove_trends : bool, optional     Should we remove trends from the flux data before     trying to look for outliers? inflate_uncertainty : bool, optional     Should uncertainties per wavelength be inflated to     match the (MAD-based) standard deviation of the data?</p>"},{"location":"api/#chromatic.rainbows.actions.flag_outliers--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new Rainbow object with the outliers flagged as 0 in <code>.ok</code></p> Source code in <code>chromatic/rainbows/actions/flag_outliers.py</code> <pre><code>def flag_outliers(self, how_many_sigma=5, remove_trends=True, inflate_uncertainty=True):\n    \"\"\"\n    Flag outliers as not `ok`.\n\n    This examines the flux array, identifies significant outliers,\n    and marks them 0 in the `ok` array. The default procedure is to use\n    a median filter to remove temporal trends (`remove_trends`),\n    inflate the uncertainties based on the median-absolute-deviation\n    scatter (`inflate_uncertainty`), and call points outliers if they\n    deviate by more than a certain number of sigma (`how_many_sigma`)\n    from the median-filtered level.\n\n    The returned `Rainbow` object should be identical to the input\n    one, except for the possibility that some elements in `ok` array\n    will have been marked as zero. (The filtering or inflation are\n    not applied to the returned object.)\n\n    Parameters\n    ----------\n    how_many_sigma : float, optional\n        Standard deviations (sigmas) allowed for individual data\n        points before they are flagged as outliers.\n    remove_trends : bool, optional\n        Should we remove trends from the flux data before\n        trying to look for outliers?\n    inflate_uncertainty : bool, optional\n        Should uncertainties per wavelength be inflated to\n        match the (MAD-based) standard deviation of the data?\n\n    Returns\n    -------\n    rainbow : Rainbow\n        A new Rainbow object with the outliers flagged as 0 in `.ok`\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"flag_outliers\", locals())\n\n    # create a copy of the existing rainbow\n    new = self._create_copy()\n\n    # how many outliers are expected from noise alone\n    outliers_expected_from_normal_distribution = erfc(how_many_sigma) * self.nflux * 2\n    if outliers_expected_from_normal_distribution &gt;= 1:\n        cheerfully_suggest(\n            f\"\"\"\n        When drawing from a normal distribution, an expected {outliers_expected_from_normal_distribution:.1f} out of\n        the total {self.nflux} datapoints in {self} would be marked\n        as a &gt;{how_many_sigma} sigma outlier.\n\n        If you don't want to accidentally clip legitimate data points that\n        might have arisen merely by chance, please consider setting the\n        outlier flagging threshold (`sigma=`) to a larger value.\n        \"\"\"\n        )\n\n    # create a trend-filtered object\n    if remove_trends:\n        filtered = new.remove_trends(method=\"median_filter\", size=(3, 5))\n    else:\n        filtered = new._create_copy()\n\n    # update the uncertainties, if need be\n    if np.all(filtered.uncertainty == 0):\n        filtered.uncertainty = (\n            np.ones(filtered.shape)\n            * filtered.get_measured_scatter(method=\"MAD\")[:, np.newaxis]\n        )\n        inflate_uncertainty = False\n\n    # inflate the per-wavelength uncertainties, as needed\n    if inflate_uncertainty:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            inflated = filtered.inflate_uncertainty(method=\"MAD\", remove_trends=True)\n    else:\n        inflated = filtered\n\n    # decide which points are outliers\n    is_outlier = np.abs(inflated.flux - 1) &gt; how_many_sigma * inflated.uncertainty\n\n    # update the output object\n    new.fluxlike[\"flagged_as_outlier\"] = is_outlier\n    new.ok = new.ok * (is_outlier == False)\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.fold--parameters","title":"Parameters","text":"<p>period : Quantity     The orbital period of the planet (with astropy units of time). t0 : Quantity     Any mid-transit epoch (with astropy units of time). event : str     A description of the event that happens periodically.     For example, you might want to switch this to     'Mid-Eclipse' (as well as offsetting the <code>t0</code> by the     appropriate amount relative to transit). This description     may be used in plot labels.</p>"},{"location":"api/#chromatic.rainbows.actions.fold--returns","title":"Returns","text":"<p>folded : Rainbow     The folded <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/actions/fold.py</code> <pre><code>def fold(self, period=None, t0=None, event=\"Mid-Transit\"):\n    \"\"\"\n    Fold this `Rainbow` to a period and reference epoch.\n\n    This changes the times from some original time into\n    a phased time, for example the time within an orbital\n    period, relative to the time of mid-transit. This\n    is mostly a convenience function for plotting data\n    relative to mid-transit and/or trimming data based\n    on orbital phase.\n\n    Parameters\n    ----------\n    period : Quantity\n        The orbital period of the planet (with astropy units of time).\n    t0 : Quantity\n        Any mid-transit epoch (with astropy units of time).\n    event : str\n        A description of the event that happens periodically.\n        For example, you might want to switch this to\n        'Mid-Eclipse' (as well as offsetting the `t0` by the\n        appropriate amount relative to transit). This description\n        may be used in plot labels.\n\n    Returns\n    -------\n    folded : Rainbow\n        The folded `Rainbow`.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"fold\", locals())\n\n    # warn\n    if (period is None) or (t0 is None):\n        message = \"\"\"\n        Folding to a transit period requires both\n        `period` and `t0` be specified. Please try again.\n        \"\"\"\n        cheerfully_suggest(message)\n        return self\n\n    # create a copy of the existing rainbow\n    new = self._create_copy()\n\n    # calculate predicted time from transit\n    new.time = (((self.time - t0) + 0.5 * period) % period) - 0.5 * period\n    # (the nudge by 0.5 period is to center on -period/2 to period/2)\n\n    # change the default time label\n    new.metadata[\"time_label\"] = f\"Time from {event}\"\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.inflate_uncertainty--parameters","title":"Parameters","text":"<p>method : string     What method to use to obtain measured scatter.     Current options are 'MAD', 'standard-deviation'. remove_trends : bool     Should we remove trends before estimating by how     much we need to inflate the uncertainties? remove_trends_method : str     What method should be used to remove trends?     See <code>.remove_trends</code> for options. remove_trends_kw : dict     What keyword arguments should be passed to <code>remove_trends</code>? minimum_inflate_ratio : float, optional     the minimum inflate_ratio that can be. We don't want people     to deflate uncertainty unless a very specific case of unstable     pipeline output.</p>"},{"location":"api/#chromatic.rainbows.actions.inflate_uncertainty--returns","title":"Returns","text":"<p>removed : Rainbow     The Rainbow with estimated signals removed.</p> Source code in <code>chromatic/rainbows/actions/inflate_uncertainty.py</code> <pre><code>def inflate_uncertainty(\n    self,\n    method=\"MAD\",\n    remove_trends=True,\n    remove_trends_method=\"median_filter\",\n    remove_trends_kw={},\n    minimum_inflate_ratio=1.0,\n):\n    \"\"\"\n    Inflate uncertainties to match observed scatter.\n\n    This is a quick and approximate tool for inflating\n    the flux uncertainties in a `Rainbow` to match the\n    observed scatter. With defaults, this will estimate\n    the scatter using a robust median-absolute-deviation\n    estimate of the standard deviation (`method='MAD'`),\n    applied to time-series from which temporal trends\n    have been removed (`remove_trends=True`), and inflate\n    the uncertainties on a per-wavelength basis. The trend\n    removal, by default by subtracting off local medians\n    (`remove_trends_method='median_filter'`), will squash\n    many types of both astrophysical and systematic trends,\n    so this function should be used with caution in\n    applicants where precise and reliable uncertainties\n    are needed.\n\n    Parameters\n    ----------\n    method : string\n        What method to use to obtain measured scatter.\n        Current options are 'MAD', 'standard-deviation'.\n    remove_trends : bool\n        Should we remove trends before estimating by how\n        much we need to inflate the uncertainties?\n    remove_trends_method : str\n        What method should be used to remove trends?\n        See `.remove_trends` for options.\n    remove_trends_kw : dict\n        What keyword arguments should be passed to `remove_trends`?\n    minimum_inflate_ratio : float, optional\n        the minimum inflate_ratio that can be. We don't want people\n        to deflate uncertainty unless a very specific case of unstable\n        pipeline output.\n\n    Returns\n    -------\n    removed : Rainbow\n        The Rainbow with estimated signals removed.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"inflate_uncertainty\", locals())\n\n    # create a new copy\n    new = self._create_copy()\n\n    # if desired, remove trends before estimating inflation factor\n    if remove_trends:\n        trend_removed = new.remove_trends(**remove_trends_kw)\n    else:\n        trend_removed = new\n\n    # estimate the scatter\n    measured_scatter = trend_removed.get_measured_scatter(\n        method=method, minimum_acceptable_ok=1e-10\n    )\n\n    # get the expected uncertainty\n    expected_uncertainty = trend_removed.get_expected_uncertainty()\n\n    # calculate the necessary inflation ratio\n    inflate_ratio = measured_scatter / expected_uncertainty\n\n    # warn if there are some inflation ratios below minimum (usually = 1)\n    if np.min(inflate_ratio) &lt; minimum_inflate_ratio:\n        cheerfully_suggest(\n            f\"\"\"\n        {np.sum(inflate_ratio &lt; minimum_inflate_ratio)} uncertainty inflation ratios would be below\n        the `minimum_inflate_ratio` of {minimum_inflate_ratio}, so they have not been changed.\n        \"\"\"\n        )\n        inflate_ratio = np.maximum(inflate_ratio, minimum_inflate_ratio)\n\n    # store the inflation ratio\n    new.wavelike[\"inflate_ratio\"] = inflate_ratio\n\n    # inflate the uncertainties\n    new.uncertainty = new.uncertainty * inflate_ratio[:, np.newaxis]\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the new Rainbow\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.inject_noise--parameters","title":"Parameters","text":"float, array, optional <p>The signal-to-noise per wavelength per time. For example, S/N=100 would mean that the uncertainty on the flux for each each wavelength-time data point will be 1%. If it is a scalar, then even point is the same. If it is an array with a fluxlike, wavelike, or timelike shape it will be broadcast appropriately.</p> <p>number_of_photons : float, array, optional     The number of photons expected to be recieved     from the light source per wavelength and time.     If it is a scalar, then even point is the same.     If it is an array with a fluxlike, wavelike,     or timelike shape it will be broadcast     appropriately.     If <code>number_of_photons</code> is set, then <code>signal_to_noise</code>     will be ignored.</p>"},{"location":"api/#chromatic.rainbows.actions.inject_noise--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new <code>Rainbow</code> object with the noise injected.</p> Source code in <code>chromatic/rainbows/actions/inject_noise.py</code> <pre><code>def inject_noise(self, signal_to_noise=100, number_of_photons=None):\n    \"\"\"\n    Inject uncorrelated random noise into the `.flux` array.\n\n    This injects independent noise to each data point,\n    drawn from either a Gaussian or Poisson distribution.\n    If the inputs can be scalar, or they can be arrays\n    that we will try to broadcast into the shape of the\n    `.flux` array.\n\n    Parameters\n    ----------\n\n    signal_to_noise : float, array, optional\n        The signal-to-noise per wavelength per time.\n        For example, S/N=100 would mean that the\n        uncertainty on the flux for each each\n        wavelength-time data point will be 1%.\n        If it is a scalar, then even point is the same.\n        If it is an array with a fluxlike, wavelike,\n        or timelike shape it will be broadcast\n        appropriately.\n    number_of_photons : float, array, optional\n        The number of photons expected to be recieved\n        from the light source per wavelength and time.\n        If it is a scalar, then even point is the same.\n        If it is an array with a fluxlike, wavelike,\n        or timelike shape it will be broadcast\n        appropriately.\n        If `number_of_photons` is set, then `signal_to_noise`\n        will be ignored.\n\n    Returns\n    -------\n    rainbow : Rainbow\n        A new `Rainbow` object with the noise injected.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"inject_noise\", locals())\n\n    # create a copy of the existing Rainbow\n    new = self._create_copy()\n\n    # get the underlying model (or create one if needed)\n    if \"model\" in new.fluxlike:\n        model = new.fluxlike[\"model\"]\n    else:\n        # kludge, do we really want to allow this?\n        model = self.flux * 1\n        new.fluxlike[\"model\"] = model\n\n    # setting up an if/else statement so that the user\n    # can choose if they want to use their own\n    # number_of_photons or the automatic signal_to_noise\n    # noise generation\n    if number_of_photons is not None:\n        if u.Quantity(model).unit != u.Unit(\"\"):\n            raise ValueError(\n                f\"\"\"\n            We haven't yet implemented `number_of_photons` noise\n            for models that have units associated with them. Sorry!\n            \"\"\"\n            )\n\n        mu = model * self._broadcast_to_fluxlike(number_of_photons)\n\n        # convert the model to photons and store it\n        new.fluxlike[\"model\"] = mu * u.photon\n\n        # inject a realization of noise using number_of_photons\n        # (yields poisson distribution)\n        new.fluxlike[\"flux\"] = np.random.poisson(mu) * u.photon  # mu is the center\n\n        # store number of photons as metadata\n        new.metadata[\"number_of_photons\"] = number_of_photons\n\n        # calculate the uncertainty\n        uncertainty = np.sqrt(mu)\n        new.fluxlike[\"uncertainty\"] = uncertainty * u.photon\n\n        # append the history entry to the new Rainbow\n        new._record_history_entry(h)\n\n    else:\n        # calculate the uncertainty with a fixed S/N\n        uncertainty = model / self._broadcast_to_fluxlike(signal_to_noise)\n        new.fluxlike[\"uncertainty\"] = uncertainty\n\n        # inject a realization of the noise\n        if isinstance(model, u.Quantity):\n            unit = model.unit\n            loc = model.to_value(unit)\n            scale = uncertainty.to_value(unit)\n        else:\n            unit = 1\n            loc = model\n            scale = uncertainty\n        new.fluxlike[\"flux\"] = np.random.normal(model, uncertainty) * unit\n\n        # store S/N as metadata\n        new.metadata[\"signal_to_noise\"] = signal_to_noise\n\n        # append the history entry to the new Rainbow\n        new._record_history_entry(h)\n\n    # return the new object\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.inject_outliers--parameters","title":"Parameters","text":"<p>fraction : float, optional     The fraction of pixels that should get outliers.     (default = 0.01) amplitude : float, optional     If uncertainty &gt; 0, how many sigma should outliers be?     If uncertainty = 0, what number should be injected?     (default = 10)</p>"},{"location":"api/#chromatic.rainbows.actions.inject_outliers--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new <code>Rainbow</code> object with outliers injected.</p> Source code in <code>chromatic/rainbows/actions/inject_outliers.py</code> <pre><code>def inject_outliers(self, fraction=0.01, amplitude=10):\n    \"\"\"\n    Inject some random outliers.\n\n    To approximately simulate cosmic rays or other\n    rare weird outliers, this randomly injects\n    outliers into a small fraction of pixels. For\n    this simple method, outliers will have the same\n    amplitude, either as a ratio above the per-data-point\n    or as a fixed number (if no uncertainties exist).\n\n    Parameters\n    ----------\n    fraction : float, optional\n        The fraction of pixels that should get outliers.\n        (default = 0.01)\n    amplitude : float, optional\n        If uncertainty &gt; 0, how many sigma should outliers be?\n        If uncertainty = 0, what number should be injected?\n        (default = 10)\n\n    Returns\n    -------\n    rainbow : Rainbow\n        A new `Rainbow` object with outliers injected.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"inject_outliers\", locals())\n\n    # create a copy of the existing Rainbow\n    new = self._create_copy()\n\n    # pick some random pixels to inject outliers\n    outliers = np.random.uniform(0, 1, self.shape) &lt; fraction\n\n    # inject outliers based on uncertainty if possible\n    if np.any(self.uncertainty &gt; 0):\n        new.fluxlike[\"injected_outliers\"] = outliers * amplitude * self.uncertainty\n    else:\n        new.fluxlike[\"injected_outliers\"] = outliers * amplitude\n\n    # modify the flux\n    new.fluxlike[\"flux\"] += new.fluxlike[\"injected_outliers\"]\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the new object\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.inject_spectrum--parameters","title":"Parameters","text":"<p>temperature : Quantity, optional     Temperature, in K (with no astropy units attached). logg : float, optional     Surface gravity log10[g/(cm/s**2)] (with no astropy units attached). metallicity : float, optional     Metallicity log10[metals/solar] (with no astropy units attached). radius : Quantity, optional     The radius of the star. distance : Quantity, optional     The distance to the star. phoenix : bool, optional     If <code>True</code>, use PHOENIX surface flux.     If <code>False</code>, use Planck surface flux.</p>"},{"location":"api/#chromatic.rainbows.actions.inject_spectrum--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new <code>Rainbow</code> object with the spectrum injected.</p> Source code in <code>chromatic/rainbows/actions/inject_spectrum.py</code> <pre><code>def inject_spectrum(\n    self,\n    temperature=5800 * u.K,\n    logg=4.43,\n    metallicity=0.0,\n    radius=1 * u.Rsun,\n    distance=10 * u.pc,\n    phoenix=True,\n):\n    \"\"\"\n    Inject a stellar spectrum into the flux.\n\n    This injects a constant stellar spectrum into\n    all times in the `Rainbow`. Injection happens\n    by multiplying the `.model` flux array, so for\n    example a model that already has a transit in\n    it will be scaled up to match the stellar spectrum\n    in all wavelengths.\n\n    Parameters\n    ----------\n    temperature : Quantity, optional\n        Temperature, in K (with no astropy units attached).\n    logg : float, optional\n        Surface gravity log10[g/(cm/s**2)] (with no astropy units attached).\n    metallicity : float, optional\n        Metallicity log10[metals/solar] (with no astropy units attached).\n    radius : Quantity, optional\n        The radius of the star.\n    distance : Quantity, optional\n        The distance to the star.\n    phoenix : bool, optional\n        If `True`, use PHOENIX surface flux.\n        If `False`, use Planck surface flux.\n\n    Returns\n    -------\n    rainbow : Rainbow\n        A new `Rainbow` object with the spectrum injected.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"inject_spectrum\", locals())\n\n    # create a copy of the existing Rainbow\n    new = self._create_copy()\n\n    # warn if maybe we shouldn't inject anything\n    if np.all(u.Quantity(self.flux).value != 1):\n        cheerfully_suggest(\n            f\"\"\"\n        None of the pre-existing flux values were 1,\n        which hints at the possibility that there\n        might already be a spectrum in them. Please\n        watch out for weird units or values!\n        \"\"\"\n        )\n\n    if phoenix:\n        f = get_phoenix_photons\n    else:\n        f = get_planck_photons\n\n    # get the spectrum from the surface\n    _, surface_flux = f(\n        temperature=u.Quantity(temperature).value,\n        logg=logg,\n        metallicity=metallicity,\n        wavelength=self.wavelength,\n    )\n\n    # get the received flux at Earth\n    received_flux = surface_flux * (radius / distance).decompose() ** 2\n\n    # do math with spectrum\n    for k in [\"flux\", \"model\", \"uncertainty\"]:\n        try:\n            new.fluxlike[k] = self.get(k) * self._broadcast_to_fluxlike(received_flux)\n        except KeyError:\n            pass\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the new object\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.inject_systematics--parameters","title":"Parameters","text":"<p>amplitude : float, optional     The (standard deviation-ish) amplitude of the systematics     in units normalized to 1. For example, an amplitude of 0.003     will produce systematic trends that tend to range (at 1 sigma)     from 0.997 to 1.003. wavelike : list of strings, optional     A list of wave-like cotrending quantities to serve as ingredients     to a linear combination systematics model. Existing quantities     will be pulled from the appropriate core dictionary; fake     data will be created for quantities that don't already exist,     from a cartoony Gaussian process model. timelike : list of strings, optional     A list of time-like cotrending quantities to serve as ingredients     to a linear combination systematics model. Existing quantities     will be pulled from the appropriate core dictionary; fake     data will be created for quantities that don't already exist,     from a cartoony Gaussian process model. fluxlike : list of strings, optional     A list of flux-like cotrending quantities to serve as ingredients     to a linear combination systematics model. Existing quantities     will be pulled from the appropriate core dictionary; fake     data will be created for quantities that don't already exist,     from a cartoony Gaussian process model.</p>"},{"location":"api/#chromatic.rainbows.actions.inject_systematics--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new Rainbow object with the systematics injected.</p> Source code in <code>chromatic/rainbows/actions/inject_systematics.py</code> <pre><code>def inject_systematics(\n    self,\n    amplitude=0.003,\n    wavelike=[],\n    timelike=[\"x\", \"y\", \"time\"],\n    fluxlike=[\"background\"],\n):\n    \"\"\"\n    Inject some (very cartoony) instrumental systematics.\n\n    Here's the basic procedure:\n\n    1) Generate some fake variables that vary either just with\n    wavelength, just with time, or with both time and wavelength.\n    Store these variables for later use. For example, these might\n    represent an average `x` and `y` centroid of the trace on the\n    detector (one for each time), or the background flux associated\n    with each wavelength (one for each time and for each wavelength).\n\n    2) Generate a flux model as some function of those variables.\n    In reality, we probably don't know the actual relationship\n    between these inputs and the flux, but we can imagine one!\n\n    3) Inject the model flux into the `flux` of this Rainbow,\n    and store the combined model in `systematics-model` and\n    each individual component in `systematic-model-{...}`.\n\n    Parameters\n    ----------\n    amplitude : float, optional\n        The (standard deviation-ish) amplitude of the systematics\n        in units normalized to 1. For example, an amplitude of 0.003\n        will produce systematic trends that tend to range (at 1 sigma)\n        from 0.997 to 1.003.\n    wavelike : list of strings, optional\n        A list of wave-like cotrending quantities to serve as ingredients\n        to a linear combination systematics model. Existing quantities\n        will be pulled from the appropriate core dictionary; fake\n        data will be created for quantities that don't already exist,\n        from a cartoony Gaussian process model.\n    timelike : list of strings, optional\n        A list of time-like cotrending quantities to serve as ingredients\n        to a linear combination systematics model. Existing quantities\n        will be pulled from the appropriate core dictionary; fake\n        data will be created for quantities that don't already exist,\n        from a cartoony Gaussian process model.\n    fluxlike : list of strings, optional\n        A list of flux-like cotrending quantities to serve as ingredients\n        to a linear combination systematics model. Existing quantities\n        will be pulled from the appropriate core dictionary; fake\n        data will be created for quantities that don't already exist,\n        from a cartoony Gaussian process model.\n\n    Returns\n    -------\n    rainbow : Rainbow\n        A new Rainbow object with the systematics injected.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"inject_systematics\", locals())\n\n    # create a copy of the existing Rainbow\n    new = self._create_copy()\n    new.fluxlike[\"systematics_model\"] = np.ones(self.shape)\n\n    def standardize(q):\n        \"\"\"\n        A quick helper to normalize all inputs to zero mean\n        and unit standard deviation. It\n        \"\"\"\n        offset = np.nanmean(q)\n        sigma = np.nanstd(q)\n        return u.Quantity((q - offset) / sigma).value, offset, sigma\n\n    components = {}\n    for k in wavelike:\n        if k in self.wavelike:\n            x, offset, sigma = standardize(self.wavelike[k])\n        else:\n            x = new._create_fake_wavelike_quantity()\n            offset, sigma = 0, 1\n            new.wavelike[k] = x\n        c = np.random.normal(0, amplitude)\n        df = c * x[:, np.newaxis] * np.ones(self.shape)\n        new.fluxlike[f\"systematics_model_from_{k}\"] = df\n        new.fluxlike[\"systematics_model\"] += df\n        components.update(\n            **{\n                f\"linear_{k}\": f\"c_{k}*({k} - offset_{k})/sigma_{k}\",\n                f\"c_{k}\": c,\n                f\"offset_{k}\": offset,\n                f\"sigma_{k}\": sigma,\n            }\n        )\n\n    for k in timelike:\n        if k in self.timelike:\n            x, offset, sigma = standardize(self.timelike[k])\n        else:\n            x = new._create_fake_timelike_quantity()\n            offset, sigma = 0, 1\n            new.timelike[k] = x\n        c = np.random.normal(0, amplitude)\n        df = c * x[np.newaxis, :] * np.ones(self.shape)\n        new.fluxlike[f\"systematics_model_from_{k}\"] = df\n        new.fluxlike[\"systematics_model\"] += df\n        components.update(\n            **{\n                f\"linear_{k}\": f\"c_{k}*({k} - offset_{k})/sigma_{k}\",\n                f\"c_{k}\": c,\n                f\"offset_{k}\": offset,\n                f\"sigma_{k}\": sigma,\n            }\n        )\n\n    for k in fluxlike:\n        if k in self.fluxlike:\n            x, offset, sigma = standardize(self.fluxlike[k])\n        else:\n            x = new._create_fake_fluxlike_quantity()\n            offset, sigma = 0, 1\n            new.fluxlike[k] = x\n        c = np.random.normal(0, amplitude)\n        df = c * x * np.ones(self.shape)\n        new.fluxlike[f\"systematics_model_from_{k}\"] = df\n        new.fluxlike[\"systematics_model\"] += df\n        components.update(\n            **{\n                f\"linear_{k}\": f\"c_{k}*({k} - offset_{k})/sigma_{k}\",\n                f\"c_{k}\": c,\n                f\"offset_{k}\": offset,\n                f\"sigma_{k}\": sigma,\n            }\n        )\n\n    new.metadata[\"systematics_components\"] = components\n    new.metadata[\"systematics_equation\"] = \"f = 1\\n  + \" + \"\\n  + \".join(\n        [v for k, v in components.items() if k[:7] == \"linear_\"]\n    )\n\n    # modify both the model and flux arrays\n    new.flux *= new.systematics_model\n    new.model = new.fluxlike.get(\"model\", 1) * new.systematics_model\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the new object\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.inject_transit--parameters","title":"Parameters","text":"<p>planet_radius : float, array, None     The planet-to-star radius ratio = [transit depth]0.5,     which can be either a single value for all wavelengths,     or an array with one value for each wavelength. method : str     What method should be used to inject the transits? Different     methods will produce different results and have different options.     The currently implement options are <code>'trapezoid'</code> and <code>'exoplanet'</code>. transit_parameters : dict     All additional keywords will be passed to the transit model.     The accepted keywords for the different methods are as follows.         <code>'trapezoid'</code> accepts the following keyword arguments:             <code>delta</code> = The depth of the transit, as a fraction of the out-of-transit flux (default 0.01)             (If not provided, it will be set by <code>planet_radius</code>.)             <code>P</code> = The orbital period of the planet, in days (default 3.0)             <code>t0</code> = Mid-transit time of the transit, in days (default 0.0)             <code>T</code> = The duration of the transit (from mid-ingress to mid-egress), in days (default 0.1)             <code>tau</code> = The duration of ingress/egress, in days (default 0.01)             <code>baseline</code> = The baseline, out-of-transit flux level (default 1.0)         <code>'exoplanet'</code> accepts the following keyword arguments:             <code>rp</code> = (planet radius)/(star radius), unitless (default 0.1)             (If not provided, it will be set by <code>planet_radius</code>.)             <code>t0</code> = Mid-transit time of the transit, in days (default 0.0)             <code>per</code> = The orbital period of the planet, in days (default 3.0)             <code>a</code> = (semi-major axis)/(star radius), unitless (default 10)             <code>inc</code> = The orbital inclination, in degrees (default 90)             <code>ecc</code> = The orbital eccentricity, unitless (default 0.0)             <code>w</code> = The longitude of periastron, in degrees (default 0.0)             <code>u</code> = The quadratic limb-darkening coefficients (default [0.2, 0.2])                 These coefficients can only be a 2D array of the form (n_wavelengths, n_coefficients) where                 each row is the set of limb-darkening coefficients corresponding                 to a single wavelength             Note that this currently does not calculate the appropriate             coefficient vs wavelength variations itself; there exist codes             (such as hpparvi/PyLDTk and nespinoza/limb-darkening) which             can be used for this.</p> Source code in <code>chromatic/rainbows/actions/inject_transit.py</code> <pre><code>def inject_transit(\n    self,\n    planet_radius=0.1,\n    method=\"exoplanet\",\n    **transit_parameters,\n):\n    \"\"\"\n    Simulate a wavelength-dependent planetary transit.\n\n    This uses one of a few methods to inject a transit\n    signal into the `Rainbow`, allowing the transit\n    depth to change with wavelength (for example due to a\n    planet's effective radius changing with wavelength due\n    to its atmospheric transmission spectrum). Other\n    parameters can also be wavlength-dependent, but\n    some (like period, inclination, etc...) probably\n    shouldn't be.\n\n    The current methods include:\n\n    `'trapezoid'` to inject a cartoon transit, using nomenclature\n    from [Winn (2010)](https://arxiv.org/abs/1001.2010).\n    This is the default method, to avoid package dependencies\n    that can be finicky to compile and/or install on different\n    operating systems.\n\n    `'exoplanet'` to inject a limb-darkened transit using [exoplanet-core](https://github.com/exoplanet-dev/exoplanet-core).\n    This option requires `exoplanet-core` be installed,\n    but it doesn't require complicated dependencies or\n    compiling steps, so it's already included as a dependency.\n\n    Parameters\n    ----------\n    planet_radius : float, array, None\n        The planet-to-star radius ratio = [transit depth]**0.5,\n        which can be either a single value for all wavelengths,\n        or an array with one value for each wavelength.\n    method : str\n        What method should be used to inject the transits? Different\n        methods will produce different results and have different options.\n        The currently implement options are `'trapezoid'` and `'exoplanet'`.\n    **transit_parameters : dict\n        All additional keywords will be passed to the transit model.\n        The accepted keywords for the different methods are as follows.\n            `'trapezoid'` accepts the following keyword arguments:\n                `delta` = The depth of the transit, as a fraction of the out-of-transit flux (default 0.01)\n                (If not provided, it will be set by `planet_radius`.)\n                `P` = The orbital period of the planet, in days (default 3.0)\n                `t0` = Mid-transit time of the transit, in days (default 0.0)\n                `T` = The duration of the transit (from mid-ingress to mid-egress), in days (default 0.1)\n                `tau` = The duration of ingress/egress, in days (default 0.01)\n                `baseline` = The baseline, out-of-transit flux level (default 1.0)\n            `'exoplanet'` accepts the following keyword arguments:\n                `rp` = (planet radius)/(star radius), unitless (default 0.1)\n                (If not provided, it will be set by `planet_radius`.)\n                `t0` = Mid-transit time of the transit, in days (default 0.0)\n                `per` = The orbital period of the planet, in days (default 3.0)\n                `a` = (semi-major axis)/(star radius), unitless (default 10)\n                `inc` = The orbital inclination, in degrees (default 90)\n                `ecc` = The orbital eccentricity, unitless (default 0.0)\n                `w` = The longitude of periastron, in degrees (default 0.0)\n                `u` = The quadratic limb-darkening coefficients (default [0.2, 0.2])\n                    These coefficients can only be a 2D array of the form (n_wavelengths, n_coefficients) where\n                    each row is the set of limb-darkening coefficients corresponding\n                    to a single wavelength\n                Note that this currently does not calculate the appropriate\n                coefficient vs wavelength variations itself; there exist codes\n                (such as hpparvi/PyLDTk and nespinoza/limb-darkening) which\n                can be used for this.\n\n\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"inject_transit\", locals())\n    h = h.replace(\"transit_parameters={\", \"**{\")\n\n    # create a copy of the existing Rainbow\n    new = self._create_copy()\n\n    # make sure the depth is set, with some flexibility\n    # to allow for different names. parameter names that\n    # belong directly to the transit model [delta, rp]\n    # will take precendence first, then [depth], then\n    # [planet_radius = the default]\n\n    # set defaults for planet simulation\n    if method == \"trapezoid\":\n        parameters_to_use = {\n            \"delta\": planet_radius**2 * np.sign(planet_radius),\n            \"P\": 1.0,\n            \"t0\": 0.0,\n            \"T\": 0.1,\n            \"tau\": 0.01,\n            \"baseline\": 1.0,\n        }\n    elif method == \"exoplanet\":\n        parameters_to_use = {\n            \"rp\": planet_radius,\n            \"t0\": 0.0,\n            \"per\": 3.0,\n            \"a\": 10.0,\n            \"inc\": 90.0,\n            \"ecc\": 0.0,\n            \"w\": 0.0,\n            \"u\": [[0.2, 0.2]],\n        }\n    else:\n        raise ValueError(\n            f\"\"\"\n        'method' must be one of ['exoplanet', 'trapezoid']\n        \"\"\"\n        )\n\n    # update based on explicit keyword arguments\n    parameters_to_use.update(**transit_parameters)\n\n    # check the parameter shapes are legitimate\n    for k, v in parameters_to_use.items():\n        s = np.shape(v)\n        if (s != ()) and (s[0] not in [1, new.nwave]):\n            raise ValueError(\n                f\"\"\"\n            The parameter {k}={v}\n            has a shape of {np.shape(v)}, which we don't know\n            how to interpret. It should be a single value,\n            or have a first dimension of either 1 or nwave={new.nwave}.\n            \"\"\"\n            )\n\n    # call the model for each wavelength\n    t = new.time.to_value(\"day\")\n    planet_flux = np.ones(new.shape)\n    for i in range(self.nwave):\n        parameters_for_this_wavelength = {\n            k: get_for_wavelength(parameters_to_use[k], i) for k in parameters_to_use\n        }\n        f = transit_model_functions[method]\n        monochromatic_flux = f(t, **parameters_for_this_wavelength)\n        planet_flux[i, :] = monochromatic_flux\n\n    # store the model in the new Rainbow object\n    new.planet_model = planet_flux\n    new.flux *= new.planet_model\n    new.model = new.fluxlike.get(\"model\", 1) * new.planet_model\n\n    # store the injected parameters as metadata or wavelike\n    new.metadata[\"injected_transit_method\"] = method\n    new.metadata[\"injected_transit_parameters\"] = parameters_to_use\n    for k, v in parameters_to_use.items():\n        label = f\"injected_transit_{k}\"\n        s = np.shape(v)\n        if s == ():\n            continue\n        elif s[0] == new.nwave:\n            if len(s) == 1:\n                new.wavelike[label] = v\n            elif len(s) &gt; 1:\n                for i in range(s[1]):\n                    new.wavelike[f\"{label}{i+1}\"] = v[:, i]\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the new object\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.normalize","title":"<code>normalize(self, axis='wavelength', percentile=50)</code>","text":"<p>Normalize by dividing through by the median spectrum and/or lightcurve.</p> <p>This normalizes a <code>Rainbow</code> by estimating dividing through by a wavelength-dependent normalization. With default inputs, this would normalize each wavelength to have flux values near 1, to make it easier to see differences across time (such as a transit or eclipse). This function could also be used to divide through by a median light curve, to make it easier to see variations across wavelength.</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.normalize--parameters","title":"Parameters","text":"<p>axis : str     The axis that should be normalized out.     <code>w</code> or <code>wave</code> or <code>wavelength</code> will divide out the typical spectrum.     <code>t</code> or <code>time</code> will divide out the typical light curve</p> float <p>A number between 0 and 100, specifying the percentile of the data along an axis to use as the reference. The default of <code>percentile=50</code> corresponds to the median. If you want to normalize to out-of-transit, maybe you want a higher percentile. If you want to normalize to the baseline below a flare, maybe you want a lower percentage.</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.normalize--returns","title":"Returns","text":"<p>normalized : Rainbow     The normalized Rainbow.</p> Source code in <code>chromatic/rainbows/actions/normalization.py</code> <pre><code>def normalize(self, axis=\"wavelength\", percentile=50):\n    \"\"\"\n    Normalize by dividing through by the median spectrum and/or lightcurve.\n\n    This normalizes a `Rainbow` by estimating dividing\n    through by a wavelength-dependent normalization. With\n    default inputs, this would normalize each wavelength\n    to have flux values near 1, to make it easier to see\n    differences across time (such as a transit or eclipse).\n    This function could also be used to divide through by\n    a median light curve, to make it easier to see variations\n    across wavelength.\n\n    Parameters\n    ----------\n    axis : str\n        The axis that should be normalized out.\n        `w` or `wave` or `wavelength` will divide out the typical spectrum.\n        `t` or `time` will divide out the typical light curve\n\n    percentile : float\n        A number between 0 and 100, specifying the percentile\n        of the data along an axis to use as the reference.\n        The default of `percentile=50` corresponds to the median.\n        If you want to normalize to out-of-transit, maybe you\n        want a higher percentile. If you want to normalize to\n        the baseline below a flare, maybe you want a lower\n        percentage.\n\n    Returns\n    -------\n    normalized : Rainbow\n        The normalized Rainbow.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"normalize\", locals())\n\n    # create an empty copy\n    new = self._create_copy()\n\n    # shortcut for the first letter of the axis\n    a = axis.lower()[0]\n\n    # (ignore nan warnings)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        # get fluxes, with not-OK replaced with nans\n        flux_for_normalizing = new.get_ok_data()\n        negative_normalization_message = \"\"\n        if a == \"w\":\n            normalization = np.nanpercentile(\n                flux_for_normalizing, percentile, axis=self.timeaxis\n            )\n            for k in self._keys_that_respond_to_math:\n                new.fluxlike[k] = new.get(k) / normalization[:, np.newaxis]\n            try:\n                new.fluxlike[\"uncertainty\"] = (\n                    self.uncertainty / normalization[:, np.newaxis]\n                )\n            except ValueError:\n                pass\n\n        elif a == \"t\":\n            normalization = np.nanpercentile(\n                flux_for_normalizing, percentile, axis=self.waveaxis\n            )\n            for k in self._keys_that_respond_to_math:\n                new.fluxlike[k] = new.get(k) / normalization[np.newaxis, :]\n            try:\n                new.fluxlike[\"uncertainty\"] = (\n                    self.uncertainty / normalization[np.newaxis, :]\n                )\n            except ValueError:\n                pass\n\n    if a in \"wt\":\n        thing = {\"w\": \"wavelengths\", \"t\": \"times\"}[a]\n        fix = {\n            \"w\": \"\"\"\n                ok = rainbow.get_median_spectrum() &gt; 0\n                rainbow[ok, :].normalize()\n        \"\"\",\n            \"t\": \"\"\"\n                ok = rainbow.get_median_lightcurve() &gt; 0\n                rainbow[:, ok].normalize()\n        \"\"\",\n        }[a]\n        if np.any(normalization &lt; 0):\n            cheerfully_suggest(\n                f\"\"\"\n            There are {np.sum(normalization &lt; 0)} negative {thing} that\n            are going into the normalization of this Rainbow. If you're\n            not expecting negative fluxes, it may be useful to trim them\n            away with something like:\n\n            {fix}\n\n            Otherwise, watch out that your fluxes and uncertainties may\n            potentially have flipped sign!\n            \"\"\"\n            )\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the new Rainbow\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.normalization.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.actions.normalization.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.__add__","title":"<code>__add__(self, other)</code>","text":"<p>Add the flux of a rainbow and an input array (or another rainbow) and output in a new rainbow other.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__add__--parameters","title":"Parameters","text":"<p>other : Array or float.     Multiple options:     1) float     2) 1D array with same length as wavelength axis     3) 1D array with same length as time axis     4) 2D array with same shape as rainbow flux     5) Rainbow other with same dimensions as self.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__add__--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new <code>Rainbow</code> with the mathematical operation applied.</p> Source code in <code>chromatic/rainbows/actions/operations.py</code> <pre><code>def __add__(self, other):\n    \"\"\"\n    Add the flux of a rainbow and an input array (or another rainbow)\n    and output in a new rainbow other.\n\n    Parameters\n    ----------\n    other : Array or float.\n        Multiple options:\n        1) float\n        2) 1D array with same length as wavelength axis\n        3) 1D array with same length as time axis\n        4) 2D array with same shape as rainbow flux\n        5) Rainbow other with same dimensions as self.\n\n    Returns\n    ----------\n    rainbow : Rainbow\n        A new `Rainbow` with the mathematical operation applied.\n    \"\"\"\n\n    # create the history entry\n    h = self._create_history_entry(\"+\", locals())\n\n    # calculate a new Rainbow using the operation and error propagation\n    result = self._apply_operation(other, operation=np.add, dzdx=\"1\", dzdy=\"1\")\n\n    # append the history entry to the new Rainbow\n    result._record_history_entry(h)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.__eq__","title":"<code>__eq__(self, other)</code>","text":"<p>Test whether <code>self == other</code>.</p> <p>This compares the wavelike, timelike, and fluxlike arrays for exact matches. It skips entirely over the metadata.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__eq__--parameters","title":"Parameters","text":"<p>other : Rainbow     Another <code>Rainbow</code> to compare to.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__eq__--returns","title":"Returns","text":"<p>equal : bool     Are they (effectively) equivalent?</p> Source code in <code>chromatic/rainbows/actions/operations.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"\n    Test whether `self == other`.\n\n    This compares the wavelike, timelike, and fluxlike arrays\n    for exact matches. It skips entirely over the metadata.\n\n    Parameters\n    ----------\n    other : Rainbow\n        Another `Rainbow` to compare to.\n\n    Returns\n    -------\n    equal : bool\n        Are they (effectively) equivalent?\n    \"\"\"\n    # start by assuming the Rainbows are identical\n    same = True\n\n    for a, b in zip([self, other], [other, self]):\n\n        # loop through the core dictionaries\n        for d in a._core_dictionaries:\n            if d == \"metadata\":\n                continue\n            # pull out each core dictionary from both\n            d1, d2 = vars(self)[d], vars(b)[d]\n            same *= set(d1.keys()) == set(d2.keys())\n\n            # loop through elements of each dictionary\n            for k in d1:\n\n                # ignore different histories (e.g. new vs loaded)\n                if k != \"history\":\n\n                    # test that all elements match for both\n                    if d == \"fluxlike\":\n                        same *= np.all(\n                            np.isclose(\n                                a.get(k)[a.ok.astype(bool)], b.get(k)[a.ok.astype(bool)]\n                            )\n                        )\n                    else:\n                        same *= np.all(np.isclose(a.get(k), b.get(k)))\n\n    return bool(same)\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.__mul__","title":"<code>__mul__(self, other)</code>","text":"<p>Multiply the flux of a rainbow and an input array (or another rainbow) and output in a new rainbow other.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__mul__--parameters","title":"Parameters","text":"<p>other : Array or float.     Multiple options:     1) float     2) 1D array with same length as wavelength axis     3) 1D array with same length as time axis     4) 2D array with same shape as rainbow flux     5) Rainbow other with same dimensions as self.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__mul__--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new <code>Rainbow</code> with the mathematical operation applied.</p> Source code in <code>chromatic/rainbows/actions/operations.py</code> <pre><code>def __mul__(self, other):\n    \"\"\"\n    Multiply the flux of a rainbow and an input array (or another rainbow)\n    and output in a new rainbow other.\n\n    Parameters\n    ----------\n    other : Array or float.\n        Multiple options:\n        1) float\n        2) 1D array with same length as wavelength axis\n        3) 1D array with same length as time axis\n        4) 2D array with same shape as rainbow flux\n        5) Rainbow other with same dimensions as self.\n\n    Returns\n    ----------\n    rainbow : Rainbow\n        A new `Rainbow` with the mathematical operation applied.\n    \"\"\"\n\n    # create the history entry\n    h = self._create_history_entry(\"*\", locals())\n\n    # calculate a new Rainbow using the operation and error propagation\n    result = self._apply_operation(other, operation=np.multiply, dzdx=\"y\", dzdy=\"x\")\n\n    # append the history entry to the new Rainbow\n    result._record_history_entry(h)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.__sub__","title":"<code>__sub__(self, other)</code>","text":"<p>Subtract the flux of a rainbow from an input array (or another rainbow) and output in a new rainbow other.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__sub__--parameters","title":"Parameters","text":"<p>other : Array or float.     Multiple options:     1) float     2) 1D array with same length as wavelength axis     3) 1D array with same length as time axis     4) 2D array with same shape as rainbow flux     5) Rainbow other with same dimensions as self.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__sub__--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new <code>Rainbow</code> with the mathematical operation applied.</p> Source code in <code>chromatic/rainbows/actions/operations.py</code> <pre><code>def __sub__(self, other):\n    \"\"\"\n    Subtract the flux of a rainbow from an input array (or another rainbow)\n    and output in a new rainbow other.\n\n    Parameters\n    ----------\n    other : Array or float.\n        Multiple options:\n        1) float\n        2) 1D array with same length as wavelength axis\n        3) 1D array with same length as time axis\n        4) 2D array with same shape as rainbow flux\n        5) Rainbow other with same dimensions as self.\n\n    Returns\n    ----------\n    rainbow : Rainbow\n        A new `Rainbow` with the mathematical operation applied.\n    \"\"\"\n    # create the history entry\n    h = self._create_history_entry(\"-\", locals())\n\n    # calculate a new Rainbow using the operation and error propagation\n    result = self._apply_operation(other, operation=np.subtract, dzdx=\"1\", dzdy=\"1\")\n\n    # append the history entry to the new Rainbow\n    result._record_history_entry(h)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.__truediv__","title":"<code>__truediv__(self, other)</code>","text":"<p>Divide the flux of a rainbow and an input array (or another rainbow) and output in a new rainbow other.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__truediv__--parameters","title":"Parameters","text":"<p>other : Array or float.     Multiple options:     1) float     2) 1D array with same length as wavelength axis     3) 1D array with same length as time axis     4) 2D array with same shape as rainbow flux     5) Rainbow other with same dimensions as self.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.__truediv__--returns","title":"Returns","text":"<p>rainbow : Rainbow     A new <code>Rainbow</code> with the mathematical operation applied.</p> Source code in <code>chromatic/rainbows/actions/operations.py</code> <pre><code>def __truediv__(self, other):\n    \"\"\"\n    Divide the flux of a rainbow and an input array (or another rainbow)\n    and output in a new rainbow other.\n\n    Parameters\n    ----------\n    other : Array or float.\n        Multiple options:\n        1) float\n        2) 1D array with same length as wavelength axis\n        3) 1D array with same length as time axis\n        4) 2D array with same shape as rainbow flux\n        5) Rainbow other with same dimensions as self.\n\n    Returns\n    ----------\n    rainbow : Rainbow\n        A new `Rainbow` with the mathematical operation applied.\n    \"\"\"\n    # create the history entry\n    h = self._create_history_entry(\"/\", locals())\n\n    # calculate a new Rainbow using the operation and error propagation\n    result = self._apply_operation(\n        other, operation=np.true_divide, dzdx=\"1/y\", dzdy=\"-x/y**2\"\n    )\n\n    # append the history entry to the new Rainbow\n    result._record_history_entry(h)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.actions.operations.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.actions.operations.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.diff","title":"<code>diff(self, other)</code>","text":"<p>Test whether <code>self == other</code>, and print the differences.</p> <p>This compares the wavelike, timelike, and fluxlike arrays for exact matches. It skips entirely over the metadata. The <code>diff</code> function is the same as <code>__eq__</code>, but a little more verbose, just to serve as a helpful debugging tool.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.diff--parameters","title":"Parameters","text":"<p>other : Rainbow     Another <code>Rainbow</code> to compare to.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.diff--returns","title":"Returns","text":"<p>equal : bool     Are they (effectively) equivalent?</p> Source code in <code>chromatic/rainbows/actions/operations.py</code> <pre><code>def diff(self, other):\n    \"\"\"\n    Test whether `self == other`, and print the differences.\n\n    This compares the wavelike, timelike, and fluxlike arrays\n    for exact matches. It skips entirely over the metadata.\n    The `diff` function is the same as `__eq__`, but a little\n    more verbose, just to serve as a helpful debugging tool.\n\n    Parameters\n    ----------\n    other : Rainbow\n        Another `Rainbow` to compare to.\n\n    Returns\n    -------\n    equal : bool\n        Are they (effectively) equivalent?\n    \"\"\"\n    # start by assuming the Rainbows are identical\n    same = True\n\n    for a, b in zip([self, other], [other, self]):\n\n        # loop through the core dictionaries\n        for d in a._core_dictionaries:\n            if d == \"metadata\":\n                continue\n            # pull out each core dictionary from both\n            d1, d2 = vars(self)[d], vars(b)[d]\n            if set(d1.keys()) != set(d2.keys()):\n                differences = list(set(d1.keys()) - set(d2.keys()))\n                print(f\"{a}.{d} has {differences} and {b} does not\")\n\n            # loop through elements of each dictionary\n            for k in d1:\n                # ignore different histories (e.g. new vs loaded)\n                if k != \"history\":\n                    continue\n                # test that all elements match for both\n                if np.all(a.get(k) == b.get(k)) == False:\n                    print(f\"{a}.{d}[{k}] != {b}.{d}[{k}]\")\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.actions.operations.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.actions.operations.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.actions.operations.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.operations.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.actions.operations.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.actions.operations.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.remove_trends--parameters","title":"Parameters","text":"<p>method : str, optional     What method should be used to make an approximate model     for smooth trends that will then be subtracted off?     <code>differences</code> will do an extremely rough filtering     of replacing the fluxes with their first differences.     Trends that are smooth relative to the noise will     be removed this way, but sharp features will remain.     Required keywords:         None.     <code>median_filter</code> is a wrapper for scipy.signal.median_filter.     It smoothes each data point to the median of its surrounding     points in time and/or wavelength. Required keywords:         <code>size</code> = centered on each point, what shape rectangle         should be used to select surrounding points for median?         The dimensions are (nwavelengths, ntimes), so <code>size=(3,7)</code>         means we'll take the median across three wavelengths and         seven times. Default is <code>(1,5)</code>.     <code>savgol_filter</code> is a wrapper for scipy.signal.savgol_filter.     It applies a Savitzky-Golay filter for polynomial smoothing.     Required keywords:         <code>window_length</code> = the length of the filter window,         which must be a positive odd integer. Default is <code>5</code>.         <code>polyorder</code> = the order of the polynomial to use.         Default is <code>2</code>.     <code>polyfit</code> is a wrapper for numpy.polyfit to use a weighted     linear least squares polynomial fit to remove smooth trends     in time. Required keywods:         <code>deg</code> = the polynomial degree, which must be a positive         integer. Default is <code>1</code>, meaning a line.     <code>custom</code> allow users to pass any fluxlike array of model     values for an astrophysical signal to remove it. Required     keywords:         <code>model</code> = the (nwavelengths, ntimes) model array **kw : dict, optional     Any additional keywords will be passed to the function     that does the filtering. See <code>method</code> keyword for options.</p>"},{"location":"api/#chromatic.rainbows.actions.remove_trends--returns","title":"Returns","text":"<p>removed : Rainbow     The Rainbow with estimated signals removed.</p> Source code in <code>chromatic/rainbows/actions/remove_trends.py</code> <pre><code>def remove_trends(self, method=\"median_filter\", **kw):\n    \"\"\"\n    A quick tool to approximately remove trends.\n\n    This function provides some simple tools for kludgily\n    removing trends from a `Rainbow`, through a variety of\n    filtering methods. If you just want to remove all\n    slow trends, whether astrophysical or instrumental,\n    options like the `median_filter` or `savgol_filter`\n    will effectively suppress all trends on timescales\n    longer than their filtering window. If you want a\n    more restricted approach to removing long trends,\n    the `polyfit` option allows you to fit out slow trends.\n\n    Parameters\n    ----------\n    method : str, optional\n        What method should be used to make an approximate model\n        for smooth trends that will then be subtracted off?\n        `differences` will do an extremely rough filtering\n        of replacing the fluxes with their first differences.\n        Trends that are smooth relative to the noise will\n        be removed this way, but sharp features will remain.\n        Required keywords:\n            None.\n        `median_filter` is a wrapper for scipy.signal.median_filter.\n        It smoothes each data point to the median of its surrounding\n        points in time and/or wavelength. Required keywords:\n            `size` = centered on each point, what shape rectangle\n            should be used to select surrounding points for median?\n            The dimensions are (nwavelengths, ntimes), so `size=(3,7)`\n            means we'll take the median across three wavelengths and\n            seven times. Default is `(1,5)`.\n        `savgol_filter` is a wrapper for scipy.signal.savgol_filter.\n        It applies a Savitzky-Golay filter for polynomial smoothing.\n        Required keywords:\n            `window_length` = the length of the filter window,\n            which must be a positive odd integer. Default is `5`.\n            `polyorder` = the order of the polynomial to use.\n            Default is `2`.\n        `polyfit` is a wrapper for numpy.polyfit to use a weighted\n        linear least squares polynomial fit to remove smooth trends\n        in time. Required keywods:\n            `deg` = the polynomial degree, which must be a positive\n            integer. Default is `1`, meaning a line.\n        `custom` allow users to pass any fluxlike array of model\n        values for an astrophysical signal to remove it. Required\n        keywords:\n            `model` = the (nwavelengths, ntimes) model array\n    **kw : dict, optional\n        Any additional keywords will be passed to the function\n        that does the filtering. See `method` keyword for options.\n\n    Returns\n    -------\n    removed : Rainbow\n        The Rainbow with estimated signals removed.\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"remove_trends\", locals())\n\n    # TODO, think about more careful treatment of uncertainties + good/bad data\n    new = self._create_copy()\n\n    if method == \"differences\":\n        new.flux = np.sqrt(2) * np.gradient(new.flux, axis=0) + 1\n\n    #    if method == \"butter_highpass\":\n    #        for i in range (0,new.nwave):\n    #            nyq = 0.5 * butter_fs\n    #            normal_cutoff = butter_cutoff/nyq\n    #            b, a = butter(butter_order, normal_cutoff, btype = \"high\", analog = False)\n    #            butter_filt = filtfilt(b, a, new.flux[i,:])\n    #            new.flux[i,:] = new.flux[i,:]/butter_filt\n    #\n    #    if method == \"convolve\":\n    #        for i in range (0,new.nwave):\n    #            box = np.ones(win_length)/win_length\n    #            grad = np.convolve(new.flux[i,:], box, mode = \"same\")\n    #            new.flux[i,:] = new.flux[i,:]/grad\n\n    if method == \"median_filter\":\n        kw_to_use = dict(size=(1, 11))\n        kw_to_use.update(**kw)\n        if \"size\" not in kw:\n            cheerfully_suggest(\n                f\"\"\"\n            You didn't supply all expected keywords for '{method}'.\n            Relying on defaults, the values will be:\n            {kw_to_use}\n            \"\"\"\n            )\n        medfilt = median_filter(self.flux, **kw_to_use)\n        new.flux = self.flux / medfilt\n        new.uncertainty = self.uncertainty / medfilt\n\n    if method == \"savgol_filter\":\n        kw_to_use = dict(window_length=11, polyorder=1)\n        kw_to_use.update(**kw)\n        if (\"window_length\" not in kw) or (\"polyorder\" not in kw):\n            cheerfully_suggest(\n                f\"\"\"\n            You didn't supply all expected keywords for '{method}'.\n            Relying on defaults, the values will be:\n            {kw_to_use}\n            \"\"\"\n            )\n        for i in range(new.nwave):\n            savgolfilter = savgol_filter(self.flux[i, :], **kw_to_use)\n            new.flux[i, :] = self.flux[i, :] / savgolfilter\n            new.uncertainty[i, :] = self.uncertainty[i, :] / savgolfilter\n\n    if method == \"polyfit\":\n        kw_to_use = dict(deg=1)\n        kw_to_use.update(**kw)\n        if \"deg\" not in kw:\n            cheerfully_suggest(\n                f\"\"\"\n            You didn't supply all expected keywords for '{method}'.\n            Relying on defaults, the values will be:\n            {kw_to_use}\n            \"\"\"\n            )\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            for i in range(new.nwave):\n                x, y, sigma = self.get_ok_data_for_wavelength(\n                    i, express_badness_with_uncertainty=True\n                )\n                ok = np.isfinite(y)\n                if np.sum(ok) &gt;= 2:\n                    try:\n                        coefs = np.polyfit(\n                            x=remove_unit(x)[ok],\n                            y=remove_unit(y)[ok],\n                            w=1 / remove_unit(sigma)[ok],\n                            **kw_to_use,\n                        )\n                        poly = np.polyval(coefs, remove_unit(x))\n                        new.flux[i, :] = self.flux[i, :] / poly\n                        new.uncertainty[i, :] = self.uncertainty[i, :] / poly\n                    except:\n                        pass\n\n    if method == \"custom\":\n        if \"model\" not in kw:\n            raise ValueError(\"You need a fluxlike `model` for this `custom` method\")\n        elif kw[\"model\"].shape != new.flux.shape:\n            raise ValueError(\"Your model doesn't match flux shape\")\n        else:\n            new.flux = new.flux / kw[\"model\"]\n            new.uncertainty = new.uncertainty / kw[\"model\"]\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the new Rainbow\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.shift--parameters","title":"Parameters","text":"<p>velocity : Quantity     The systemic velocity by which we should shift,     with units of velocity (for example, u.km/u.s)</p> Source code in <code>chromatic/rainbows/actions/shift.py</code> <pre><code>def shift(self, velocity=0 * u.km / u.s):\n    \"\"\"\n    Doppler shift the wavelengths of this `Rainbow`.\n\n    This shifts the wavelengths in a `Rainbow` by\n    applying a velocity shift. Positive velocities make\n    wavelengths longer (redshift); negative velocities make\n    wavelengths shorter (bluesfhit).\n\n    Parameters\n    ----------\n    velocity : Quantity\n        The systemic velocity by which we should shift,\n        with units of velocity (for example, u.km/u.s)\n    \"\"\"\n\n    # create a history entry for this action (before other variables are defined)\n    h = self._create_history_entry(\"shift\", locals())\n\n    # create a new copy of this rainbow\n    new = self._create_copy()\n\n    # get the speed of light from astropy constants\n    lightspeed = con.c.to(\"km/s\")  # speed of light in km/s\n\n    # calculate beta and make sure the units cancel\n    beta = (velocity / lightspeed).decompose()\n\n    # apply wavelength shift\n    new_wavelength = new.wavelength * np.sqrt((1 + beta) / (1 - beta))\n    new.wavelike[\"wavelength\"] = new_wavelength\n\n    # append the history entry to the new Rainbow\n    new._record_history_entry(h)\n\n    # return the new object\n    return new\n</code></pre>"},{"location":"api/#chromatic.rainbows.actions.trim--parameters","title":"Parameters","text":"<p>t_min : u.Quantity     The minimum time to keep. t_max : u.Quantity     The maximum time to keep. w_min : u.Quantity     The minimum wavelength to keep. w_max : u.Quantity     The maximum wavelength to keep. just_edges : bool, optional     Should we only trim the outermost bad wavelength bins?         <code>True</code> = Just trim off the bad edges and keep         interior bad values. Keeping interior data, even if         they're all bad, often helps to make for more         intuititive imshow plots.         <code>False</code> = Trim off every bad wavelength, whether it's on         the edge or somewhere in the middle of the dataset.         The resulting Rainbow will be smaller, but it might         be a little tricky to visualize with imshow. when_to_give_up : float, optional     The fraction of times that must be nan or not OK     for the entire wavelength to be considered bad (default = 1).         <code>1.0</code> = trim only if all times are bad         <code>0.5</code> = trim if more than 50% of times are bad         <code>0.0</code> = trim if any times are bad minimum_acceptable_ok : float, optional     The numbers in the <code>.ok</code> attribute express \"how OK?\" each     data point is, ranging from 0 (not OK) to 1 (super OK).     In most cases, <code>.ok</code> will be binary, but there may be times     where it's intermediate (for example, if a bin was created     from some data that were not OK and some that were).     The <code>minimum_acceptable_ok</code> parameter allows you to specify what     level of OK-ness for a point to not get trimmed.</p>"},{"location":"api/#chromatic.rainbows.actions.trim--returns","title":"Returns","text":"<p>trimmed : Rainbow     The trimmed <code>Rainbow</code>.</p> Source code in <code>chromatic/rainbows/actions/trim.py</code> <pre><code>def trim(\n    self,\n    t_min=None,\n    t_max=None,\n    w_min=None,\n    w_max=None,\n    just_edges=True,\n    when_to_give_up=1,\n    minimum_acceptable_ok=1,\n):\n    \"\"\"\n    Trim away bad wavelengths and/or times.\n\n    If entire wavelengths or times are marked as not `ok`,\n    we can probably remove them to simplify calculations\n    and visualizations. This function will trim those away,\n    by default only removing problem rows/columns on the ends,\n    to maintain a contiguous block.\n\n    Parameters\n    ----------\n    t_min : u.Quantity\n        The minimum time to keep.\n    t_max : u.Quantity\n        The maximum time to keep.\n    w_min : u.Quantity\n        The minimum wavelength to keep.\n    w_max : u.Quantity\n        The maximum wavelength to keep.\n    just_edges : bool, optional\n        Should we only trim the outermost bad wavelength bins?\n            `True` = Just trim off the bad edges and keep\n            interior bad values. Keeping interior data, even if\n            they're all bad, often helps to make for more\n            intuititive imshow plots.\n            `False` = Trim off every bad wavelength, whether it's on\n            the edge or somewhere in the middle of the dataset.\n            The resulting Rainbow will be smaller, but it might\n            be a little tricky to visualize with imshow.\n    when_to_give_up : float, optional\n        The fraction of times that must be nan or not OK\n        for the entire wavelength to be considered bad (default = 1).\n            `1.0` = trim only if all times are bad\n            `0.5` = trim if more than 50% of times are bad\n            `0.0` = trim if any times are bad\n    minimum_acceptable_ok : float, optional\n        The numbers in the `.ok` attribute express \"how OK?\" each\n        data point is, ranging from 0 (not OK) to 1 (super OK).\n        In most cases, `.ok` will be binary, but there may be times\n        where it's intermediate (for example, if a bin was created\n        from some data that were not OK and some that were).\n        The `minimum_acceptable_ok` parameter allows you to specify what\n        level of OK-ness for a point to not get trimmed.\n\n    Returns\n    -------\n    trimmed : Rainbow\n        The trimmed `Rainbow`.\n    \"\"\"\n\n    trimmed = self.trim_times(\n        t_min=t_min,\n        t_max=t_max,\n        when_to_give_up=when_to_give_up,\n        just_edges=just_edges,\n        minimum_acceptable_ok=minimum_acceptable_ok,\n    )\n    trimmed = trimmed.trim_wavelengths(\n        w_min=w_min,\n        w_max=w_max,\n        when_to_give_up=when_to_give_up,\n        just_edges=just_edges,\n        minimum_acceptable_ok=minimum_acceptable_ok,\n    )\n\n    return trimmed\n</code></pre>"},{"location":"api/#gettimelike","title":"\ud83c\udf08 Get/Timelike","text":""},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.get_average_lightcurve","title":"<code>get_average_lightcurve(self)</code>","text":"<p>Return a lightcurve of the star, averaged over all wavelengths.</p> <p>This uses <code>bin</code>, which is a horribly slow way of doing what is fundamentally a very simply array calculation, because we don't need to deal with partial pixels.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.get_average_lightcurve--returns","title":"Returns","text":"<p>lightcurve : array     Timelike array of fluxes.</p> Source code in <code>chromatic/rainbows/get/timelike/average_lightcurve.py</code> <pre><code>def get_average_lightcurve(self):\n    \"\"\"\n    Return a lightcurve of the star, averaged over all wavelengths.\n\n    This uses `bin`, which is a horribly slow way of doing what is\n    fundamentally a very simply array calculation, because we\n    don't need to deal with partial pixels.\n\n    Returns\n    -------\n    lightcurve : array\n        Timelike array of fluxes.\n    \"\"\"\n    return self.get_average_lightcurve_as_rainbow().flux[0, :]\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.timelike.average_lightcurve.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.get_median_lightcurve","title":"<code>get_median_lightcurve(self)</code>","text":"<p>Return a lightcurve of the star, medianed over all wavelengths.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.get_median_lightcurve--returns","title":"Returns","text":"<p>median_lightcurve : array     Timelike array of fluxes.</p> Source code in <code>chromatic/rainbows/get/timelike/median_lightcurve.py</code> <pre><code>def get_median_lightcurve(self):\n    \"\"\"\n    Return a lightcurve of the star, medianed over all wavelengths.\n\n    Returns\n    -------\n    median_lightcurve : array\n        Timelike array of fluxes.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        return np.nanmedian(self.get_ok_data(), axis=0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.timelike.median_lightcurve.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.get_for_time","title":"<code>get_for_time(self, i, quantity='flux')</code>","text":"<p>Get <code>'quantity'</code> associated with time <code>'i'</code>.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.get_for_time--parameters","title":"Parameters","text":"<p>i : int     The time index to retrieve. quantity : string     The quantity to retrieve. If it is flux-like,     column 'i' will be returned. If it is wave-like,     the array itself will be returned.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.get_for_time--returns","title":"Returns","text":"<p>quantity : array, Quantity     The 1D array of 'quantity' corresponding to time 'i'.</p> Source code in <code>chromatic/rainbows/get/timelike/subset.py</code> <pre><code>def get_for_time(self, i, quantity=\"flux\"):\n    \"\"\"\n    Get `'quantity'` associated with time `'i'`.\n\n    Parameters\n    ----------\n    i : int\n        The time index to retrieve.\n    quantity : string\n        The quantity to retrieve. If it is flux-like,\n        column 'i' will be returned. If it is wave-like,\n        the array itself will be returned.\n\n    Returns\n    -------\n    quantity : array, Quantity\n        The 1D array of 'quantity' corresponding to time 'i'.\n    \"\"\"\n    z = self.get(quantity)\n    if np.shape(z) == self.shape:\n        return z[:, i]\n    elif len(z) == self.nwave:\n        return z\n    else:\n        raise RuntimeError(\n            f\"\"\"\n        You tried to retrieve time {i} from '{quantity}',\n        but this quantity is neither flux-like nor wave-like.\n        It's not possible to return a wave-like array. Sorry!\n        \"\"\"\n        )\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.get_ok_data_for_time","title":"<code>get_ok_data_for_time(self, i, x='wavelength', y='flux', sigma='uncertainty', minimum_acceptable_ok=1, express_badness_with_uncertainty=False)</code>","text":"<p>A small wrapper to get the good data from a time.</p> <p>Extract a slice of data, marking data that are not <code>ok</code> either by trimming them out entirely or by inflating their uncertainties to infinity.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.get_ok_data_for_time--parameters","title":"Parameters","text":"<p>i : int     The time index to retrieve. x : string, optional     What quantity should be retrieved as 'x'? (default = 'time') y : string, optional     What quantity should be retrieved as 'y'? (default = 'flux') sigma : string, optional     What quantity should be retrieved as 'sigma'? (default = 'uncertainty') minimum_acceptable_ok : float, optional     The smallest value of <code>ok</code> that will still be included.     (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data) express_badness_with_uncertainty : bool, optional     If False, data that don't pass the <code>ok</code> cut will be removed.     If True, data that don't pass the <code>ok</code> cut will have their     uncertainties inflated to infinity (np.inf).</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.get_ok_data_for_time--returns","title":"Returns","text":"<p>x : array     The time. y : array     The desired quantity (default is <code>flux</code>) sigma : array     The uncertainty on the desired quantity</p> Source code in <code>chromatic/rainbows/get/timelike/subset.py</code> <pre><code>def get_ok_data_for_time(\n    self,\n    i,\n    x=\"wavelength\",\n    y=\"flux\",\n    sigma=\"uncertainty\",\n    minimum_acceptable_ok=1,\n    express_badness_with_uncertainty=False,\n):\n    \"\"\"\n    A small wrapper to get the good data from a time.\n\n    Extract a slice of data, marking data that are not `ok` either\n    by trimming them out entirely or by inflating their\n    uncertainties to infinity.\n\n    Parameters\n    ----------\n    i : int\n        The time index to retrieve.\n    x : string, optional\n        What quantity should be retrieved as 'x'? (default = 'time')\n    y : string, optional\n        What quantity should be retrieved as 'y'? (default = 'flux')\n    sigma : string, optional\n        What quantity should be retrieved as 'sigma'? (default = 'uncertainty')\n    minimum_acceptable_ok : float, optional\n        The smallest value of `ok` that will still be included.\n        (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data)\n    express_badness_with_uncertainty : bool, optional\n        If False, data that don't pass the `ok` cut will be removed.\n        If True, data that don't pass the `ok` cut will have their\n        uncertainties inflated to infinity (np.inf).\n\n    Returns\n    -------\n    x : array\n        The time.\n    y : array\n        The desired quantity (default is `flux`)\n    sigma : array\n        The uncertainty on the desired quantity\n    \"\"\"\n\n    # get 1D independent variable\n    x_values = self.get_for_time(i, x) * 1\n\n    # get 1D array of what to keep\n    ok = self.ok[:, i] &gt;= minimum_acceptable_ok\n\n    # get 1D array of the quantity\n    y_values = self.get_for_time(i, y) * 1\n\n    # get 1D array of uncertainty\n    sigma_values = self.get_for_time(i, sigma) * 1\n\n    if express_badness_with_uncertainty:\n        sigma_values[ok == False] = np.inf\n        return x_values, y_values, sigma_values\n    else:\n        return x_values[ok], y_values[ok], sigma_values[ok]\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.timelike.subset.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.get_times_as_astropy","title":"<code>get_times_as_astropy(self, time=None, format=None, scale=None, is_barycentric=None)</code>","text":"<p>Convert times from a <code>Rainbow</code> into an astropy <code>Time</code> object.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.get_times_as_astropy--parameters","title":"Parameters","text":"<p>time : Quantity, optional     The time-like Quantity to be converted.     If None (default), convert the time values in <code>self.time</code>     If another time-like Quantity, convert those values. format : str, optional     The time format to supply to astropy.time.Time.     If None (default), format will be pulled from     <code>self.metadata['time_details']['format']</code> scale : str, optional     The time scale to supply to astropy.time.Time.     If None (default), scale will be pulled from     <code>self.metadata['time_details']['scale']</code> is_barycentric : bool, optional     Are the times already measured relative to the     Solar System barycenter? This is mostly for warning     the user that it's not.     If <code>None</code> (default), <code>is_barycentric</code> will be pulled from     <code>self.metadata['time_details']['is_barycentric']</code></p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.get_times_as_astropy--returns","title":"Returns","text":"<p>astropy_time : Time     The times as an astropy <code>Time</code> object.</p> Source code in <code>chromatic/rainbows/get/timelike/time.py</code> <pre><code>def get_times_as_astropy(self, time=None, format=None, scale=None, is_barycentric=None):\n    \"\"\"\n    Convert times from a `Rainbow` into an astropy `Time` object.\n\n    Parameters\n    ----------\n    time : Quantity, optional\n        The time-like Quantity to be converted.\n        If None (default), convert the time values in `self.time`\n        If another time-like Quantity, convert those values.\n    format : str, optional\n        The time format to supply to astropy.time.Time.\n        If None (default), format will be pulled from\n        `self.metadata['time_details']['format']`\n    scale : str, optional\n        The time scale to supply to astropy.time.Time.\n        If None (default), scale will be pulled from\n        `self.metadata['time_details']['scale']`\n    is_barycentric : bool, optional\n        Are the times already measured relative to the\n        Solar System barycenter? This is mostly for warning\n        the user that it's not.\n        If `None` (default), `is_barycentric` will be pulled from\n        `self.metadata['time_details']['is_barycentric']`\n\n    Returns\n    -------\n    astropy_time : Time\n        The times as an astropy `Time` object.\n    \"\"\"\n\n    # take times from self or from the keyword\n    if time is None:\n        time = self.time\n\n    # give a format warning\n    format = format or self.get(\"time_format\")\n    if format is None:\n        cheerfully_suggest(\n            f\"\"\"\n        `.metadata['time_details']['format']` is not set,\n        nor was a `format=` keyword argument provided.\n\n        Since `.time` is already an astropy Quantity,\n        this is likely a question of whether the format is\n        'jd' or 'mjd' (= 'jd' - 2400000.5) or something else.\n        If you get this wrong, you might be lost in time!\n\n        For more about astropy.Time formats, please see:\n        https://docs.astropy.org/en/stable/time/index.html#time-format\n        \"\"\"\n        )\n\n    # give a scale warning\n    scale = scale or self.get(\"time_scale\")\n    if scale is None:\n        now = Time.now()\n        differences_string = \"\"\n        for s in now.SCALES:\n            dt = ((getattr(now, s).jd - now.tdb.jd) * u.day).to(u.second)\n            differences_string += f\"{s:&gt;15} - tdb = {dt:10.6f}\\n\"\n        cheerfully_suggest(\n            f\"\"\"\n        .metadata['time_details']['scale'] is not set,\n        nor was a `scale=` keyword argument provided.\n\n        The main question is whether the time scale is 'tdb'\n        (Barycentric Dynamical Time) or something close to it,\n        or 'utc' or something close to it. The differences\n        between these options, at {now.utc.iso} (UTC), are:\n        \\n{differences_string}\n        If you get this wrong, you might be lost in time!\n\n        For more about astropy.Time scales, please see:\n        https://docs.astropy.org/en/stable/time/index.html#time-scale\n        \"\"\"\n        )\n\n    # give some barycenter warnings\n    is_barycentric = is_barycentric or self.get(\"time_is_barycentric\")\n    if is_barycentric == True and \"ut\" in scale.lower():\n        cheerfully_suggest(\n            f\"\"\"\n        barycentic={is_barycentric} and scale={scale}\n        It's a deeply weird combination to have a barycentric\n        time measured at the Solar System barycentric but in\n        Earth's leap-second-based UTC system. Please consider\n        checking your time details.\n        \"\"\"\n        )\n    if is_barycentric != True:\n        cheerfully_suggest(\n            f\"\"\"\n        The returned time is not known to be measured relative\n        to the Solar System barycenter. It's probably therefore\n        measured from Earth or the position of your telescope,\n        but please be warned that the timing of very distant event\n        (like exoplanet transits) might be off by up to about\n        8 minutes (= the light travel time between Earth + Sun).\n        \"\"\"\n        )\n\n    # generate astropy Time array\n    astropy_time = Time(self.time, format=format, scale=scale)\n\n    # do a check that the values aren't really weird\n    if (astropy_time.min().decimalyear &lt; 1000) or (\n        astropy_time.max().decimalyear &gt; 3000\n    ):\n        cheerfully_suggest(\n            f\"\"\"\n        The times, which span\n        jd={astropy_time.min().jd} to jd={astropy_time.max().jd}\n        don't seem likely to be within the range of modern astronomical\n        observations. Please consider double checking your time values\n        and/or (format='{format}', scale='{scale}').\n        \"\"\"\n        )\n\n    return astropy_time\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.timelike.time.set_times_from_astropy","title":"<code>set_times_from_astropy(self, astropy_time, is_barycentric=None)</code>","text":"<p>Set the times for this <code>Rainbow</code> from an astropy <code>Time</code> object.</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.set_times_from_astropy--parameters","title":"Parameters","text":"<p>astropy_time : Time     The times as an astropy <code>Time</code> object. is_barycentric : bool, optional     Are the times already measured relative to the     Solar System barycenter? This is mostly for warning     the user that it's not. Options are True, False,     None (= don't know).</p>"},{"location":"api/#chromatic.rainbows.get.timelike.time.set_times_from_astropy--returns","title":"Returns","text":"<p>time : Quantity     An astropy Quantity with units of time,     expressing the Time as julian day.     In addition to this returned variable,     the function sets the following internal     variables:     <code>self.time # (= the astropy Quantity of times)     self.metadata['time_format'] # (= the format to convert back to Time)     self.metadata['time_scale'] # (= the scale to convert back to Time)     self.metadata['time_is_barycentric'] # (= is it barycentric?)</code></p> Source code in <code>chromatic/rainbows/get/timelike/time.py</code> <pre><code>def set_times_from_astropy(self, astropy_time, is_barycentric=None):\n    \"\"\"\n    Set the times for this `Rainbow` from an astropy `Time` object.\n\n    Parameters\n    ----------\n    astropy_time : Time\n        The times as an astropy `Time` object.\n    is_barycentric : bool, optional\n        Are the times already measured relative to the\n        Solar System barycenter? This is mostly for warning\n        the user that it's not. Options are True, False,\n        None (= don't know).\n\n    Returns\n    -------\n    time : Quantity\n        An astropy Quantity with units of time,\n        expressing the Time as julian day.\n        In addition to this returned variable,\n        the function sets the following internal\n        variables:\n        ```\n        self.time # (= the astropy Quantity of times)\n        self.metadata['time_format'] # (= the format to convert back to Time)\n        self.metadata['time_scale'] # (= the scale to convert back to Time)\n        self.metadata['time_is_barycentric'] # (= is it barycentric?)\n        ```\n    \"\"\"\n\n    # set the formats\n    format = \"jd\"\n    unit = u.day\n    scale = \"tdb\"\n\n    # store the necessary values\n    self.timelike[\"time\"] = getattr(getattr(astropy_time, scale), format) * unit\n    self.metadata[\"time_format\"] = format\n    self.metadata[\"time_scale\"] = scale\n    self.metadata[\"time_is_barycentric\"] = is_barycentric\n\n    # do some accounting to sync everything together\n    self._guess_tscale()\n    self._make_sure_time_edges_are_defined()\n    return self.time\n</code></pre>"},{"location":"api/#getwavelike","title":"\ud83c\udf08 Get/Wavelike","text":""},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.get_average_spectrum","title":"<code>get_average_spectrum(self)</code>","text":"<p>Return a average_spectrum of the star, averaged over all times.</p> <p>This uses <code>bin</code>, which is a horribly slow way of doing what is fundamentally a very simply array calculation, because we don't need to deal with partial pixels.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.get_average_spectrum--returns","title":"Returns","text":"<p>average_spectrum : array     Wavelike array of average spectrum.</p> Source code in <code>chromatic/rainbows/get/wavelike/average_spectrum.py</code> <pre><code>def get_average_spectrum(self):\n    \"\"\"\n    Return a average_spectrum of the star, averaged over all times.\n\n    This uses `bin`, which is a horribly slow way of doing what is\n    fundamentally a very simply array calculation, because we\n    don't need to deal with partial pixels.\n\n    Returns\n    -------\n    average_spectrum : array\n        Wavelike array of average spectrum.\n    \"\"\"\n    return self.get_average_spectrum_as_rainbow().flux[:, 0]\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.average_spectrum.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.get_expected_uncertainty","title":"<code>get_expected_uncertainty(self, function=np.nanmedian, *args, **kw)</code>","text":"<p>Get the typical per-wavelength uncertainty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.get_expected_uncertainty--parameters","title":"Parameters","text":"<p>function : function, optional     What function should be used to choose the \"typical\"     value for each wavelength? Good options are probably     things like <code>np.nanmedian</code>, <code>np.median</code>, <code>np.nanmean</code> <code>np.mean</code>, <code>np.percentile</code> args : list, optional     Addition arguments will be passed to <code>function</code> *kw : dict, optional     Additional keyword arguments will be passed to <code>function</code></p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.get_expected_uncertainty--returns","title":"Returns","text":"<p>uncertainty_per_wavelength : array     The uncertainty associated with each wavelength.</p> Source code in <code>chromatic/rainbows/get/wavelike/expected_uncertainty.py</code> <pre><code>def get_expected_uncertainty(self, function=np.nanmedian, *args, **kw):\n    \"\"\"\n    Get the typical per-wavelength uncertainty.\n\n    Parameters\n    ----------\n    function : function, optional\n        What function should be used to choose the \"typical\"\n        value for each wavelength? Good options are probably\n        things like `np.nanmedian`, `np.median`, `np.nanmean`\n        `np.mean`, `np.percentile`\n    *args : list, optional\n        Addition arguments will be passed to `function`\n    **kw : dict, optional\n        Additional keyword arguments will be passed to `function`\n\n    Returns\n    -------\n    uncertainty_per_wavelength : array\n        The uncertainty associated with each wavelength.\n    \"\"\"\n    uncertainty_per_wavelength = function(\n        self.uncertainty, *args, axis=self.timeaxis, **kw\n    )\n    return uncertainty_per_wavelength\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.expected_uncertainty.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.get_measured_scatter_in_bins","title":"<code>get_measured_scatter_in_bins(self, ntimes=2, nbins=4, method='standard-deviation', minimum_acceptable_ok=1e-10)</code>","text":"<p>Get measured scatter in time bins of increasing sizes.</p> <p>For uncorrelated Gaussian noise, the scatter should decrease as 1/sqrt(N), where N is the number points in a bin. This function calculates the scatter for a range of N, thus providing a quick test for correlated noise.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.get_measured_scatter_in_bins--parameters","title":"Parameters","text":"<p>ntimes : int     How many times should be binned together? Binning will     continue recursively until fewer that nbins would be left. nbins : int     What's the smallest number of bins that should be used to     calculate a scatter? The absolute minimum is 2. method : string     What method to use to obtain measured scatter. Current options are 'MAD', 'standard-deviation'. minimum_acceptable_ok : float     The smallest value of <code>ok</code> that will still be included.     (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.get_measured_scatter_in_bins--returns","title":"Returns","text":"<p>scatter_dictionary : dict     Dictionary with lots of information about scatter in bins per wavelength.</p> Source code in <code>chromatic/rainbows/get/wavelike/measured_scatter_in_bins.py</code> <pre><code>def get_measured_scatter_in_bins(\n    self, ntimes=2, nbins=4, method=\"standard-deviation\", minimum_acceptable_ok=1e-10\n):\n    \"\"\"\n    Get measured scatter in time bins of increasing sizes.\n\n    For uncorrelated Gaussian noise, the scatter should\n    decrease as 1/sqrt(N), where N is the number points\n    in a bin. This function calculates the scatter for\n    a range of N, thus providing a quick test for\n    correlated noise.\n\n    Parameters\n    ----------\n    ntimes : int\n        How many times should be binned together? Binning will\n        continue recursively until fewer that nbins would be left.\n    nbins : int\n        What's the smallest number of bins that should be used to\n        calculate a scatter? The absolute minimum is 2.\n    method : string\n        What method to use to obtain measured scatter. Current options are 'MAD', 'standard-deviation'.\n    minimum_acceptable_ok : float\n        The smallest value of `ok` that will still be included.\n        (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data)\n\n    Returns\n    -------\n    scatter_dictionary : dict\n        Dictionary with lots of information about scatter in bins per wavelength.\n    \"\"\"\n\n    from ...rainbow import Rainbow\n\n    if \"remove_trends\" in self.history():\n        cheerfully_suggest(\n            f\"\"\"\n        The `remove_trends` function was applied to this `Rainbow`,\n        making it very plausible that some long-timescale signals\n        and/or noise have been suppressed. Be suspicious of binned\n        scatters on long timescales.\n        \"\"\"\n        )\n\n    # create a simplified rainbow so we don't waste time binning\n    simple = Rainbow(\n        time=self.time,\n        wavelength=self.wavelength,\n        flux=self.flux,\n        uncertainty=self.uncertainty,\n        ok=self.ok,\n    )\n\n    # loop through binning until done\n    binnings = [simple]\n    N = [1]\n    while binnings[-1].ntime &gt; ntimes * nbins:\n        binnings.append(\n            binnings[-1].bin(ntimes=ntimes, minimum_acceptable_ok=minimum_acceptable_ok)\n        )\n        N.append(N[-1] * ntimes)\n\n    scatters = [b.get_measured_scatter(method=method) for b in binnings]\n    expectation = [b.get_expected_uncertainty() for b in binnings]\n    uncertainty_on_scatters = (\n        scatters\n        / np.sqrt(2 * (np.array([b.ntime for b in binnings]) - 1))[:, np.newaxis]\n    )\n    dt = [np.median(np.diff(b.time)) for b in binnings]\n\n    return dict(\n        N=np.array(N),\n        dt=u.Quantity(dt),\n        scatters=np.transpose(scatters),\n        expectation=np.transpose(expectation),\n        uncertainty=np.transpose(uncertainty_on_scatters),\n    )\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter_in_bins.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.get_measured_scatter","title":"<code>get_measured_scatter(self, quantity='flux', method='standard-deviation', minimum_acceptable_ok=1e-10)</code>","text":"<p>Get measured scatter for each wavelength.</p> <p>Calculate the standard deviation (or outlier-robust equivalent) for each wavelength, which can be compared to the expected per-wavelength uncertainty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.get_measured_scatter--parameters","title":"Parameters","text":"<p>quantity : string, optional     The <code>fluxlike</code> quantity for which we should calculate the scatter. method : string, optional     What method to use to obtain measured scatter.     Current options are 'MAD', 'standard-deviation'. minimum_acceptable_ok : float, optional     The smallest value of <code>ok</code> that will still be included.     (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.get_measured_scatter--returns","title":"Returns","text":"<p>scatter : array     Wavelike array of measured scatters.</p> Source code in <code>chromatic/rainbows/get/wavelike/measured_scatter.py</code> <pre><code>def get_measured_scatter(\n    self, quantity=\"flux\", method=\"standard-deviation\", minimum_acceptable_ok=1e-10\n):\n    \"\"\"\n    Get measured scatter for each wavelength.\n\n    Calculate the standard deviation (or outlier-robust\n    equivalent) for each wavelength, which can be compared\n    to the expected per-wavelength uncertainty.\n\n    Parameters\n    ----------\n    quantity : string, optional\n        The `fluxlike` quantity for which we should calculate the scatter.\n    method : string, optional\n        What method to use to obtain measured scatter.\n        Current options are 'MAD', 'standard-deviation'.\n    minimum_acceptable_ok : float, optional\n        The smallest value of `ok` that will still be included.\n        (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data)\n\n    Returns\n    -------\n    scatter : array\n        Wavelike array of measured scatters.\n    \"\"\"\n\n    if method not in [\"standard-deviation\", \"MAD\"]:\n        cheerfully_suggest(\n            f\"\"\"\n        '{method}' is not an available method.\n        Please choose from ['MAD', 'standard-deviation'].\n        \"\"\"\n        )\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        scatters = np.zeros(self.nwave)\n        for i in range(self.nwave):\n            x, y, sigma = self.get_ok_data_for_wavelength(\n                i, y=quantity, minimum_acceptable_ok=minimum_acceptable_ok\n            )\n            if u.Quantity(y).unit == u.Unit(\"\"):\n                y_value, y_unit = y, 1\n            else:\n                y_value, y_unit = y.value, y.unit\n            if method == \"standard-deviation\":\n                scatters[i] = np.nanstd(y_value)\n            elif method == \"MAD\":\n                scatters[i] = mad_std(y_value, ignore_nan=True)\n        return scatters * y_unit\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.measured_scatter.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.get_median_spectrum","title":"<code>get_median_spectrum(self)</code>","text":"<p>Return a spectrum of the star, medianed over all times.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.get_median_spectrum--returns","title":"Returns","text":"<p>median_spectrum : array     Wavelike array of fluxes.</p> Source code in <code>chromatic/rainbows/get/wavelike/median_spectrum.py</code> <pre><code>def get_median_spectrum(self):\n    \"\"\"\n    Return a spectrum of the star, medianed over all times.\n\n    Returns\n    -------\n    median_spectrum : array\n        Wavelike array of fluxes.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        return np.nanmedian(self.get_ok_data(), axis=1)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.median_spectrum.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.get_spectral_resolution","title":"<code>get_spectral_resolution(self, pixels_per_resolution_element=1)</code>","text":"<p>Estimate the R=w/dw spectral resolution.</p> <p>Higher spectral resolutions correspond to more wavelength points within a particular interval. By default, it's estimated for the interval between adjacent wavelength bins. In unbinned data coming directly from a telescope, there's a good chance that adjacent pixels both sample the same resolution element as blurred by the telescope optics, so the <code>pixels_per_resolution_element</code> keyword should likely be larger than 1.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.get_spectral_resolution--parameters","title":"Parameters","text":"<p>pixels_per_resolution_element : float, optional     How many pixels do we consider as a resolution element?</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.get_spectral_resolution--returns","title":"Returns","text":"<p>R : array     The spectral resolution at each wavelength.</p> Source code in <code>chromatic/rainbows/get/wavelike/spectral_resolution.py</code> <pre><code>def get_spectral_resolution(self, pixels_per_resolution_element=1):\n    \"\"\"\n    Estimate the R=w/dw spectral resolution.\n\n    Higher spectral resolutions correspond to more wavelength\n    points within a particular interval. By default, it's\n    estimated for the interval between adjacent wavelength\n    bins. In unbinned data coming directly from a telescope,\n    there's a good chance that adjacent pixels both sample\n    the same resolution element as blurred by the telescope\n    optics, so the `pixels_per_resolution_element` keyword\n    should likely be larger than 1.\n\n    Parameters\n    ----------\n    pixels_per_resolution_element : float, optional\n        How many pixels do we consider as a resolution element?\n\n    Returns\n    -------\n    R : array\n        The spectral resolution at each wavelength.\n    \"\"\"\n\n    # calculate spectral resolution, for this pixels/element\n    w = self.wavelength\n    dw = np.gradient(self.wavelength)\n    R = np.abs(w / dw / pixels_per_resolution_element)\n\n    return R\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.spectral_resolution.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.get_for_wavelength","title":"<code>get_for_wavelength(self, i, quantity='flux')</code>","text":"<p>Get <code>'quantity'</code> associated with wavelength <code>'i'</code>.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.get_for_wavelength--parameters","title":"Parameters","text":"<p>i : int     The wavelength index to retrieve. quantity : string     The quantity to retrieve. If it is flux-like,     row 'i' will be returned. If it is time-like,     the array itself will be returned.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.get_for_wavelength--returns","title":"Returns","text":"<p>quantity : array, Quantity     The 1D array of 'quantity' corresponding to wavelength 'i'.</p> Source code in <code>chromatic/rainbows/get/wavelike/subset.py</code> <pre><code>def get_for_wavelength(self, i, quantity=\"flux\"):\n    \"\"\"\n    Get `'quantity'` associated with wavelength `'i'`.\n\n    Parameters\n    ----------\n    i : int\n        The wavelength index to retrieve.\n    quantity : string\n        The quantity to retrieve. If it is flux-like,\n        row 'i' will be returned. If it is time-like,\n        the array itself will be returned.\n\n    Returns\n    -------\n    quantity : array, Quantity\n        The 1D array of 'quantity' corresponding to wavelength 'i'.\n    \"\"\"\n    z = self.get(quantity)\n    if np.shape(z) == self.shape:\n        return z[i, :]\n    elif len(z) == self.ntime:\n        return z\n    else:\n        raise RuntimeError(\n            f\"\"\"\n        You tried to retrieve wavelength {i} from '{quantity}',\n        but this quantity is neither flux-like nor time-like.\n        It's not possible to return a time-like array. Sorry!\n        \"\"\"\n        )\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.get_ok_data_for_wavelength","title":"<code>get_ok_data_for_wavelength(self, i, x='time', y='flux', sigma='uncertainty', minimum_acceptable_ok=1, express_badness_with_uncertainty=False)</code>","text":"<p>A small wrapper to get the good data from a wavelength.</p> <p>Extract a slice of data, marking data that are not <code>ok</code> either by trimming them out entirely or by inflating their uncertainties to infinity.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.get_ok_data_for_wavelength--parameters","title":"Parameters","text":"<p>i : int     The wavelength index to retrieve. x : string, optional     What quantity should be retrieved as 'x'? (default = 'time') y : string, optional     What quantity should be retrieved as 'y'? (default = 'flux') sigma : string, optional     What quantity should be retrieved as 'sigma'? (default = 'uncertainty') minimum_acceptable_ok : float, optional     The smallest value of <code>ok</code> that will still be included.     (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data) express_badness_with_uncertainty : bool, optional     If False, data that don't pass the <code>ok</code> cut will be removed.     If True, data that don't pass the <code>ok</code> cut will have their     uncertainties inflated to infinity (np.inf).</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.get_ok_data_for_wavelength--returns","title":"Returns","text":"<p>x : array     The time. y : array     The desired quantity (default is <code>flux</code>) sigma : array     The uncertainty on the desired quantity</p> Source code in <code>chromatic/rainbows/get/wavelike/subset.py</code> <pre><code>def get_ok_data_for_wavelength(\n    self,\n    i,\n    x=\"time\",\n    y=\"flux\",\n    sigma=\"uncertainty\",\n    minimum_acceptable_ok=1,\n    express_badness_with_uncertainty=False,\n):\n    \"\"\"\n    A small wrapper to get the good data from a wavelength.\n\n    Extract a slice of data, marking data that are not `ok` either\n    by trimming them out entirely or by inflating their\n    uncertainties to infinity.\n\n    Parameters\n    ----------\n    i : int\n        The wavelength index to retrieve.\n    x : string, optional\n        What quantity should be retrieved as 'x'? (default = 'time')\n    y : string, optional\n        What quantity should be retrieved as 'y'? (default = 'flux')\n    sigma : string, optional\n        What quantity should be retrieved as 'sigma'? (default = 'uncertainty')\n    minimum_acceptable_ok : float, optional\n        The smallest value of `ok` that will still be included.\n        (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data)\n    express_badness_with_uncertainty : bool, optional\n        If False, data that don't pass the `ok` cut will be removed.\n        If True, data that don't pass the `ok` cut will have their\n        uncertainties inflated to infinity (np.inf).\n\n    Returns\n    -------\n    x : array\n        The time.\n    y : array\n        The desired quantity (default is `flux`)\n    sigma : array\n        The uncertainty on the desired quantity\n    \"\"\"\n\n    # get 1D independent variable\n    x_values = self.get_for_wavelength(i, x) * 1\n\n    # get 1D array of what to keep\n    ok = self.ok[i, :] &gt;= minimum_acceptable_ok\n\n    # get 1D array of the quantity\n    y_values = self.get_for_wavelength(i, y) * 1\n\n    # get 1D array of uncertainty\n    sigma_values = self.get_for_wavelength(i, sigma) * 1\n\n    if express_badness_with_uncertainty:\n        sigma_values[ok == False] = np.inf\n        return x_values, y_values, sigma_values\n    else:\n        return x_values[ok], y_values[ok], sigma_values[ok]\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.get.wavelike.subset.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#visualizations","title":"\ud83c\udf08 Visualizations","text":"<p>Paint a 2D image of flux as a function of time and wavelength, using <code>plt.imshow</code> where pixels will have constant size.</p> <p>By using <code>.imshow()</code>, pixels must have constant size, so non-uniform axes will be displayed and labeled by index, rather than actual value. This is faster than <code>.pcolormesh()</code> but less flexible.  <code>.paint()</code> will try to choose the best between <code>.imshow()</code> and <code>.pcolormesh()</code>.</p> <p>Paint a 2D image of flux as a function of time and wavelength.</p> <p>By using <code>.pcolormesh()</code>, pixels can transform based on their edges, so non-uniform axes are allowed. This is a tiny bit slower than <code>.imshow()</code>, but otherwise very similar. <code>.paint()</code> will try to choose the best between <code>.imshow()</code> and <code>.pcolormesh()</code>.</p> <p>Plot flux as sequence of offset light curves.</p> <p>Plot flux as sequence of offset spectrum.</p> <p>Plot flux either as a sequence of offset lightcurves (default) or as a sequence of offset spectra.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.animate_lightcurves","title":"<code>animate_lightcurves(self, filename='animated-lightcurves.gif', fps=5, dpi=None, bitrate=None, **kwargs)</code>","text":"<p>Create an animation to show how the lightcurve changes as we flip through every wavelength.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.animate_lightcurves--parameters","title":"Parameters","text":"<p>filename : str     Name of file you'd like to save results in.     Currently supports only .gif or .html files. fps : float     frames/second of animation ax : Axes     The axes into which this animated plot should go. xlim : tuple     Custom xlimits for the plot ylim : tuple     Custom ylimits for the plot cmap : str,     The color map to use for expressing wavelength vmin : Quantity     The minimum value to use for the wavelength colormap vmax : Quantity     The maximum value to use for the wavelength colormap scatterkw : dict     A dictionary of keywords to be passed to <code>plt.scatter</code>     so you can have more detailed control over the plot     appearance. Common keyword arguments might include:     <code>[s, c, marker, alpha, linewidths, edgecolors, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html textkw : dict     A dictionary of keywords passed to <code>plt.text</code>     so you can have more detailed control over the text     appearance. Common keyword arguments might include:     <code>[alpha, backgroundcolor, color, fontfamily, fontsize,       fontstyle, fontweight, rotation, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html</p> Source code in <code>chromatic/rainbows/visualizations/animate.py</code> <pre><code>def animate_lightcurves(\n    self,\n    filename=\"animated-lightcurves.gif\",\n    fps=5,\n    dpi=None,\n    bitrate=None,\n    **kwargs,\n):\n    \"\"\"\n    Create an animation to show how the lightcurve changes\n    as we flip through every wavelength.\n\n    Parameters\n    ----------\n    filename : str\n        Name of file you'd like to save results in.\n        Currently supports only .gif or .html files.\n    fps : float\n        frames/second of animation\n    ax : Axes\n        The axes into which this animated plot should go.\n    xlim : tuple\n        Custom xlimits for the plot\n    ylim : tuple\n        Custom ylimits for the plot\n    cmap : str,\n        The color map to use for expressing wavelength\n    vmin : Quantity\n        The minimum value to use for the wavelength colormap\n    vmax : Quantity\n        The maximum value to use for the wavelength colormap\n    scatterkw : dict\n        A dictionary of keywords to be passed to `plt.scatter`\n        so you can have more detailed control over the plot\n        appearance. Common keyword arguments might include:\n        `[s, c, marker, alpha, linewidths, edgecolors, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n    textkw : dict\n        A dictionary of keywords passed to `plt.text`\n        so you can have more detailed control over the text\n        appearance. Common keyword arguments might include:\n        `[alpha, backgroundcolor, color, fontfamily, fontsize,\n          fontstyle, fontweight, rotation, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html\n    \"\"\"\n    self._setup_animate_lightcurves(**kwargs)\n\n    filename = self._label_plot_file(filename)\n\n    # initialize the animator\n    writer, displayer = get_animation_writer_and_displayer(\n        filename=filename, fps=fps, bitrate=bitrate\n    )\n\n    # set up to save frames directly into the animation\n    figure = self._animate_lightcurves_components[\"fi\"]\n    with writer.saving(figure, filename, dpi or figure.get_dpi()):\n        for i in tqdm(range(self.nwave), leave=False):\n            self._animate_lightcurves_components[\"update\"](i)\n            writer.grab_frame()\n\n    # close the figure that was created\n    plt.close(figure)\n\n    # display the animation\n    from IPython.display import display\n\n    try:\n        display(displayer(filename, embed=True))\n    except TypeError:\n        display(displayer(filename))\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.animate_spectra","title":"<code>animate_spectra(self, filename='animated-spectra.gif', fps=5, dpi=None, bitrate=None, **kw)</code>","text":"<p>Create an animation to show how the spectrum changes as we flip through every timepoint.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.animate_spectra--parameters","title":"Parameters","text":"<p>filename : str     Name of file you'd like to save results in.     Currently supports only .gif files. ax : Axes     The axes into which this animated plot should go. fps : float     frames/second of animation xlim : tuple     Custom xlimits for the plot ylim : tuple     Custom ylimits for the plot cmap : str,     The color map to use for expressing wavelength vmin : Quantity     The minimum value to use for the wavelength colormap vmax : Quantity     The maximum value to use for the wavelength colormap scatterkw : dict     A dictionary of keywords to be passed to <code>plt.scatter</code>     so you can have more detailed control over the plot     appearance. Common keyword arguments might include:     <code>[s, c, marker, alpha, linewidths, edgecolors, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html textkw : dict     A dictionary of keywords passed to <code>plt.text</code>     so you can have more detailed control over the text     appearance. Common keyword arguments might include:     <code>[alpha, backgroundcolor, color, fontfamily, fontsize,       fontstyle, fontweight, rotation, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html</p> Source code in <code>chromatic/rainbows/visualizations/animate.py</code> <pre><code>def animate_spectra(\n    self, filename=\"animated-spectra.gif\", fps=5, dpi=None, bitrate=None, **kw\n):\n    \"\"\"\n    Create an animation to show how the spectrum changes\n    as we flip through every timepoint.\n\n    Parameters\n    ----------\n    filename : str\n        Name of file you'd like to save results in.\n        Currently supports only .gif files.\n    ax : Axes\n        The axes into which this animated plot should go.\n    fps : float\n        frames/second of animation\n    xlim : tuple\n        Custom xlimits for the plot\n    ylim : tuple\n        Custom ylimits for the plot\n    cmap : str,\n        The color map to use for expressing wavelength\n    vmin : Quantity\n        The minimum value to use for the wavelength colormap\n    vmax : Quantity\n        The maximum value to use for the wavelength colormap\n    scatterkw : dict\n        A dictionary of keywords to be passed to `plt.scatter`\n        so you can have more detailed control over the plot\n        appearance. Common keyword arguments might include:\n        `[s, c, marker, alpha, linewidths, edgecolors, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n    textkw : dict\n        A dictionary of keywords passed to `plt.text`\n        so you can have more detailed control over the text\n        appearance. Common keyword arguments might include:\n        `[alpha, backgroundcolor, color, fontfamily, fontsize,\n          fontstyle, fontweight, rotation, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html\n    \"\"\"\n\n    self._setup_animate_spectra(**kw)\n\n    filename = self._label_plot_file(filename)\n    # initialize the animator\n    writer, displayer = get_animation_writer_and_displayer(\n        filename=filename, fps=fps, bitrate=bitrate\n    )\n\n    # set up to save frames directly into the animation\n    figure = self._animate_spectra_components[\"fi\"]\n    with writer.saving(figure, filename, dpi or figure.get_dpi()):\n        for i in tqdm(range(self.ntime), leave=False):\n            self._animate_spectra_components[\"update\"](i)\n            writer.grab_frame()\n\n    # close the figure that was created\n    plt.close(figure)\n\n    # display the animation\n    from IPython.display import display\n\n    display(displayer(filename))\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.get_animation_writer_and_displayer","title":"<code>get_animation_writer_and_displayer(filename='animation.html', **kw)</code>","text":"<p>Create the right animation writer based on filename.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.get_animation_writer_and_displayer--parameters","title":"Parameters","text":"<p>filename : str     The filename of the movie to create.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.get_animation_writer_and_displayer--returns","title":"Returns","text":"<p>writer : MovieWriter     The matplotlib writer object. displayer : ?     The</p> Source code in <code>chromatic/rainbows/visualizations/utilities.py</code> <pre><code>def get_animation_writer_and_displayer(filename=\"animation.html\", **kw):\n    \"\"\"\n    Create the right animation writer based on filename.\n\n    Parameters\n    ----------\n    filename : str\n        The filename of the movie to create.\n\n    Returns\n    -------\n    writer : MovieWriter\n        The matplotlib writer object.\n    displayer : ?\n        The\n    \"\"\"\n\n    # define the options\n    writers = {\"html\": ani.HTMLWriter, \"mp4\": ani.FFMpegWriter, \"gif\": ani.PillowWriter}\n    warnings = {\n        \"html\": \"Please try `pip insall matplotlib --upgrade` and rerunning?\",\n        \"mp4\": \"Please try `conda install ffmpeg` and rerunning?\",\n        \"gif\": \"Please try `pip insall matplotlib --upgrade` and rerunning?\",\n    }\n    from IPython.display import HTML, Video, Image\n\n    displayers = {\"html\": HTML, \"mp4\": Video, \"gif\": Image}\n\n    # get the writer object\n    suffix = filename.split(\".\")[-1]\n    writer = writers[suffix](**kw)\n    displayer = displayers[suffix]\n\n    if writer.isAvailable():\n        return writer, displayer\n    else:\n        raise ValueError(\n            f\"\"\"\n        The writer {writer} needed for your `.{suffix}` file is not available.\n        {warnings[suffix]}\n        \"\"\"\n        )\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.animate.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.visualizations.animate.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.get_wavelength_color","title":"<code>get_wavelength_color(self, wavelength)</code>","text":"<p>Determine the color corresponding to one or more wavelengths.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.get_wavelength_color--parameters","title":"Parameters","text":"<p>wavelength : Quantity     The wavelength value(s), either an individual     wavelength or an array of N wavelengths.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.get_wavelength_color--returns","title":"Returns","text":"<p>colors : array     An array of RGBA colors [or an (N,4) array].</p> Source code in <code>chromatic/rainbows/visualizations/colors.py</code> <pre><code>def get_wavelength_color(self, wavelength):\n    \"\"\"\n    Determine the color corresponding to one or more wavelengths.\n\n    Parameters\n    ----------\n    wavelength : Quantity\n        The wavelength value(s), either an individual\n        wavelength or an array of N wavelengths.\n\n    Returns\n    -------\n    colors : array\n        An array of RGBA colors [or an (N,4) array].\n    \"\"\"\n    w_unitless = wavelength.to(\"micron\").value\n    normalized_w = self.norm(w_unitless)\n    return self.cmap(normalized_w)\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.colors.setup_wavelength_colors","title":"<code>setup_wavelength_colors(self, cmap=None, vmin=None, vmax=None, log=None)</code>","text":"<p>Set up a color map and normalization function for colors datapoints by their wavelengths.</p>"},{"location":"api/#chromatic.rainbows.visualizations.colors.setup_wavelength_colors--parameters","title":"Parameters","text":"<p>cmap : str, Colormap     The color map to use. vmin : Quantity     The wavelength at the bottom of the cmap. vmax : Quantity     The wavelength at the top of the cmap. log : bool     If True, colors will scale with log(wavelength).     If False, colors will scale with wavelength.     If None, the scale will be guessed from the internal wscale.</p> Source code in <code>chromatic/rainbows/visualizations/colors.py</code> <pre><code>def setup_wavelength_colors(self, cmap=None, vmin=None, vmax=None, log=None):\n    \"\"\"\n    Set up a color map and normalization function for\n    colors datapoints by their wavelengths.\n\n    Parameters\n    ----------\n    cmap : str, Colormap\n        The color map to use.\n    vmin : Quantity\n        The wavelength at the bottom of the cmap.\n    vmax : Quantity\n        The wavelength at the top of the cmap.\n    log : bool\n        If True, colors will scale with log(wavelength).\n        If False, colors will scale with wavelength.\n        If None, the scale will be guessed from the internal wscale.\n    \"\"\"\n\n    # populate the cmap object\n    self.cmap = plt.colormaps.get_cmap(cmap)\n\n    vmin = vmin\n    if vmin is None:\n        vmin = np.nanmin(self.wavelength)\n    vmax = vmax\n    if vmax is None:\n        vmax = np.nanmax(self.wavelength)\n\n    if (self.wscale in [\"log\"]) or (log == True):\n        self.norm = col.LogNorm(\n            vmin=vmin.to(\"micron\").value, vmax=vmax.to(\"micron\").value\n        )\n    if (self.wscale in [\"?\", \"linear\"]) or (log == False):\n        self.norm = col.Normalize(\n            vmin=vmin.to(\"micron\").value, vmax=vmax.to(\"micron\").value\n        )\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.imshow--parameters","title":"Parameters","text":"<p>ax : Axes, optional     The axes into which to make this plot. quantity : str, optional     The fluxlike quantity to imshow.     (Must be a key of <code>rainbow.fluxlike</code>). xaxis : str     What to use as the horizontal axis, 'time' or 'wavelength'. w_unit : str, Unit, optional     The unit for plotting wavelengths. t_unit : str, Unit, optional     The unit for plotting times. colorbar : bool, optional     Should we include a colorbar? mask_ok : bool, optional     Should we mark which data are not OK? color_ok : str, optional     The color to be used for masking data points that are not OK. alpha_ok : float, optional     The transparency to be used for masking data points that are not OK. **kw : dict, optional     All other keywords will be passed on to <code>plt.imshow</code>,     so you can have more detailed control over the plot     appearance. Common keyword arguments might include:     <code>[cmap, norm, interpolation, alpha, vmin, vmax]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html</p> Source code in <code>chromatic/rainbows/visualizations/imshow.py</code> <pre><code>def imshow(\n    self,\n    ax=None,\n    quantity=\"flux\",\n    xaxis=\"time\",\n    w_unit=\"micron\",\n    t_unit=\"day\",\n    colorbar=True,\n    mask_ok=True,\n    color_ok=\"tomato\",\n    alpha_ok=0.8,\n    vmin=None,\n    vmax=None,\n    filename=None,\n    **kw,\n):\n    \"\"\"\n    Paint a 2D image of flux as a function of time and wavelength,\n    using `plt.imshow` where pixels will have constant size.\n\n    By using `.imshow()`, pixels must have constant size, so non-uniform\n    axes will be displayed and labeled by index, rather than actual value.\n    This is faster than `.pcolormesh()` but less flexible.  `.paint()` will\n    try to choose the best between `.imshow()` and `.pcolormesh()`.\n\n    Parameters\n    ----------\n    ax : Axes, optional\n        The axes into which to make this plot.\n    quantity : str, optional\n        The fluxlike quantity to imshow.\n        (Must be a key of `rainbow.fluxlike`).\n    xaxis : str\n        What to use as the horizontal axis, 'time' or 'wavelength'.\n    w_unit : str, Unit, optional\n        The unit for plotting wavelengths.\n    t_unit : str, Unit, optional\n        The unit for plotting times.\n    colorbar : bool, optional\n        Should we include a colorbar?\n    mask_ok : bool, optional\n        Should we mark which data are not OK?\n    color_ok : str, optional\n        The color to be used for masking data points that are not OK.\n    alpha_ok : float, optional\n        The transparency to be used for masking data points that are not OK.\n    **kw : dict, optional\n        All other keywords will be passed on to `plt.imshow`,\n        so you can have more detailed control over the plot\n        appearance. Common keyword arguments might include:\n        `[cmap, norm, interpolation, alpha, vmin, vmax]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n    \"\"\"\n\n    # self.speak(f'imshowing')\n    if ax is None:\n        ax = plt.subplot()\n\n    # get units\n    w_unit, t_unit = u.Unit(w_unit), u.Unit(t_unit)\n\n    # make sure some wavelength and time edges are defined\n    self._make_sure_wavelength_edges_are_defined()\n    self._make_sure_time_edges_are_defined()\n\n    # set up the wavelength extent\n    try:\n        wmin = self.wavelength_lower[0].to_value(w_unit)\n        wmax = self.wavelength_upper[-1].to_value(w_unit)\n    except AttributeError:\n        wmin, wmax = None, None\n\n    if (self.wscale == \"linear\") and (wmin is not None) and (wmax is not None):\n        wlower, wupper = wmin, wmax\n        wlabel = f\"{self._wave_label} ({w_unit.to_string('latex_inline')})\"\n    elif self.wscale == \"log\" and (wmin is not None) and (wmax is not None):\n        wlower, wupper = np.log10(wmin), np.log10(wmax)\n        wlabel = (\n            r\"log$_{10}$\" + f\"[{self._wave_label}/({w_unit.to_string('latex_inline')})]\"\n        )\n    else:\n        message = f\"\"\"\n        The wavelength scale for this rainbow is '{self.wscale}',\n        and there are {self.nwave} wavelength centers and\n        {len(self.wavelike.get('wavelength_lower', []))} wavelength edges defined.\n\n        It's hard to imshow something with a wavelength axis\n        that isn't linearly or logarithmically uniform, or doesn't\n        at least have its wavelength edges defined. We're giving up\n        and just using the wavelength index as the wavelength axis.\n\n        If you want a real wavelength axis, one solution would be\n        to use `rainbow.pcolormesh()` instead of `rainbow.imshow()`.\n        It takes basically the same inputs but can handle non-uniform\n        grids.\n\n        Or, you could bin your wavelengths to a more uniform grid with\n        `binned = rainbow.bin(R=...)` (for logarithmic wavelengths)\n        or `binned = rainbow.bin(dw=...)` (for linear wavelengths)\n        and then `binned.imshow()` will give more informative axes.\n        \"\"\"\n        cheerfully_suggest(message)\n        wlower, wupper = -0.5, self.nwave - 0.5\n        wlabel = \"Wavelength Index\"\n\n    # set up the time extent\n    try:\n        tmin = self.time_lower[0].to_value(t_unit)\n        tmax = self.time_upper[-1].to_value(t_unit)\n    except AttributeError:\n        tmin, tmax = None, None\n    if (self.tscale == \"linear\") and (tmin is not None) and (tmax is not None):\n        tlower, tupper = tmin, tmax\n        tlabel = f\"{self._time_label} ({t_unit.to_string('latex_inline')})\"\n    elif self.tscale == \"log\" and (tmin is not None) and (tmax is not None):\n        tlower, tupper = np.log10(tmin), np.log10(tmax)\n        tlabel = (\n            r\"log$_{10}$\" + f\"[{self._time_label}/({t_unit.to_string('latex_inline')})]\"\n        )\n    else:\n        message = f\"\"\"\n        The time scale for this rainbow is '{self.tscale}',\n        and there are {self.ntime} time centers and\n        {len(self.timelike.get('time_lower', []))} time edges defined.\n\n        It's hard to imshow something with a time axis\n        that isn't linearly or logarithmically uniform, or doesn't\n        at least have its time edges defined. We're giving up\n        and just using the time index as the time axis.\n\n        If you want a real time axis, one solution would be\n        to use `rainbow.pcolormesh()` instead of `rainbow.imshow()`.\n        It takes basically the same inputs but can handle non-uniform\n        grids.\n\n        Or, you could bin your times to a more uniform grid with\n        `binned = rainbow.bin(dt=...)` (for linear times) and then\n        `binned.imshow()` will give more informative axes.\n        \"\"\"\n        cheerfully_suggest(message)\n        tlower, tupper = -0.5, self.ntime - 0.5\n        tlabel = \"Time Index\"\n\n    def get_2D(k):\n        \"\"\"\n        A small helper to get a 2D quantity. This is a bit of\n        a kludge to help with weird cases of duplicate keys\n        (for example where 'wavelength' might appear in both\n        `wavelike` and `fluxlike`).\n        \"\"\"\n        z = self.get(k)\n        if np.shape(z) == self.shape:\n            return z\n        else:\n            return self.fluxlike.get(k, None)\n\n    # choose between time and wavelength on the x-axis\n    if xaxis.lower()[0] == \"t\":\n        self.metadata[\"_imshow_extent\"] = [tlower, tupper, wupper, wlower]\n        xlabel, ylabel = tlabel, wlabel\n        z = get_2D(quantity)\n        ok = get_2D(\"ok\")\n    elif xaxis.lower()[0] == \"w\":\n        self.metadata[\"_imshow_extent\"] = [wlower, wupper, tupper, tlower]\n        xlabel, ylabel = wlabel, tlabel\n        z = get_2D(quantity).T\n        ok = get_2D(\"ok\").T\n    else:\n        cheerfully_suggest(\n            \"Please specify either `xaxis='time'` or `xaxis='wavelength'` for `.plot()`\"\n        )\n\n    # figure out a good shared color limits (unless already supplied)\n    vmin = vmin or np.nanpercentile(remove_unit(z).flatten() * 1.0, 1)\n    vmax = vmax or np.nanpercentile(remove_unit(z).flatten() * 1.0, 99)\n\n    # define some default keywords\n    imshow_kw = dict(interpolation=\"antialiased\", aspect=\"auto\", vmin=vmin, vmax=vmax)\n    imshow_kw.update(**kw)\n    with quantity_support():\n        plt.sca(ax)\n\n        # create an overlaying mask of which data are OK or not\n        if mask_ok:\n            okimshow_kw = dict(**imshow_kw)\n            okimshow_kw.update(\n                cmap=one2another(\n                    bottom=color_ok,\n                    top=color_ok,\n                    alpha_bottom=alpha_ok,\n                    alpha_top=0,\n                ),\n                zorder=10,\n                vmin=0,\n                vmax=1,\n            )\n            plt.imshow(\n                remove_unit(ok),\n                extent=self.metadata[\"_imshow_extent\"],\n                origin=\"upper\",\n                **okimshow_kw,\n            )\n        plt.imshow(\n            remove_unit(z),\n            extent=self.metadata[\"_imshow_extent\"],\n            origin=\"upper\",\n            **imshow_kw,\n        )\n        plt.ylabel(ylabel)\n        plt.xlabel(xlabel)\n        if colorbar:\n            plt.colorbar(\n                ax=ax,\n                label=u.Quantity(z).unit.to_string(\"latex_inline\"),\n            )\n        plt.title(self.get(\"title\"))\n\n    if filename is not None:\n        self.savefig(filename)\n    return ax\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.imshow_interact","title":"<code>imshow_interact(self, quantity='Flux', t_unit='d', w_unit='micron', cmap='viridis', xlim=[], ylim=[], ylog=None, xbuffer=0.01, ybuffer=0.01, filename=None)</code>","text":"<p>Display interactive spectrum plot for chromatic Rainbow with a wavelength-averaged 2D quantity defined by the user. The user can interact with the 3D spectrum to choose the wavelength range over which the average is calculated.</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.imshow_interact--parameters","title":"Parameters","text":"<p>self : Rainbow object     chromatic Rainbow object to plot quantity : str     (optional, default='flux')     The quantity to imshow, currently either <code>flux</code> or <code>uncertainty</code> ylog : boolean     (optional, default=None)     Boolean for whether to take log10 of the y-axis data.     If None, will be guessed from the data. t_unit : str     (optional, default='d')     The time unit to use (seconds, minutes, hours, days etc.) w_unit : str     (optional, default='micron')     The wavelength unit to use cmap : str     (optional, default='viridis')     The color scheme to use from Vega documentation ylim : list     (optional, default=[])     If the user wants to define their own ylimits on the lightcurve plot xlim : list     (optional, default=[])     If the user wants to define their own xlimits on the lightcurve plot xbuffer : float     (optional, default=0.01)     X-axis (time) buffer for plotting, xlims=[min(x)-xbuffer, max(x)+xbuffer] ybuffer : float     (optional, default=0.01)     Y-axis (quantity) buffer for plotting, ylims=[min(y)-ybuffer, max(y)+ybuffer]</p> Source code in <code>chromatic/rainbows/visualizations/interactive.py</code> <pre><code>def imshow_interact(\n    self,\n    quantity=\"Flux\",\n    t_unit=\"d\",\n    w_unit=\"micron\",\n    cmap=\"viridis\",\n    xlim=[],\n    ylim=[],\n    ylog=None,\n    xbuffer=0.01,\n    ybuffer=0.01,\n    filename=None,\n):\n    \"\"\"\n    Display interactive spectrum plot for chromatic Rainbow with a\n    wavelength-averaged 2D quantity defined by the user. The user\n    can interact with the 3D spectrum to choose the wavelength range\n    over which the average is calculated.\n\n    Parameters\n    ----------\n    self : Rainbow object\n        chromatic Rainbow object to plot\n    quantity : str\n        (optional, default='flux')\n        The quantity to imshow, currently either `flux` or `uncertainty`\n    ylog : boolean\n        (optional, default=None)\n        Boolean for whether to take log10 of the y-axis data.\n        If None, will be guessed from the data.\n    t_unit : str\n        (optional, default='d')\n        The time unit to use (seconds, minutes, hours, days etc.)\n    w_unit : str\n        (optional, default='micron')\n        The wavelength unit to use\n    cmap : str\n        (optional, default='viridis')\n        The color scheme to use from Vega documentation\n    ylim : list\n        (optional, default=[])\n        If the user wants to define their own ylimits on the lightcurve plot\n    xlim : list\n        (optional, default=[])\n        If the user wants to define their own xlimits on the lightcurve plot\n    xbuffer : float\n        (optional, default=0.01)\n        X-axis (time) buffer for plotting, xlims=[min(x)-xbuffer, max(x)+xbuffer]\n    ybuffer : float\n        (optional, default=0.01)\n        Y-axis (quantity) buffer for plotting, ylims=[min(y)-ybuffer, max(y)+ybuffer]\n\n    \"\"\"\n\n    # preset the x and y axes as Time (in units defined by the user) and Wavelength\n    xlabel = f\"Time ({t_unit})\"\n    ylabel = f\"Wavelength ({w_unit})\"\n\n    # allow the user to plot flux or uncertainty\n    if quantity.lower() == \"flux\":\n        z = \"Flux\"\n    elif quantity.lower() == \"uncertainty\":\n        z = \"Flux Uncertainty\"\n    elif quantity.lower() == \"error\":\n        z = \"Flux Uncertainty\"\n    elif quantity.lower() == \"flux_error\":\n        z = \"Flux Uncertainty\"\n    elif quantity.lower() == \"flux_uncertainty\":\n        z = \"Flux Uncertainty\"\n    else:\n        # if the quantity is not one of the predefined values:\n        cheerfully_suggest(\"Unrecognised Quantity!\")\n        return\n\n    # convert rainbow object to pandas dataframe\n    source = self.to_df(t_unit=t_unit, w_unit=w_unit)[[xlabel, ylabel, z]]\n    # source[xlabel] = source[xlabel] - source[xlabel][0]\n\n    # if there are &gt;10,000 data points Altair will be very laggy/slow. This is probably unbinned, therefore\n    # encourage the user to bin the Rainbow before calling this function in future/\n    N_warning = 100000\n    if len(source) &gt; N_warning:\n        cheerfully_suggest(\n            f\"\"\"\n        The dataset {self} has &gt;{N_warning} data points.\n        The interactive plot may lag. Try binning first!\n        \"\"\"\n        )\n\n    if (self._is_probably_normalized() == False) and \"model\" not in self.fluxlike:\n        cheerfully_suggest(\n            \"\"\"\n        It looks like you might be trying to use `imshow_interact` with an\n        unnormalized Rainbow object. You might consider normalizing first\n        with `rainbow.normalize().imshow_interact()`.\n        \"\"\"\n        )\n\n    # The unbinned Rainbow is sometimes in log scale, therefore plotting will be ugly with uniform axis spacing\n    # ylog tells the function to take the log10 of the y-axis data\n    try:\n        ylog = ylog or (self.wscale == \"log\")\n    except AttributeError:\n        ylog = ylog or False\n\n    # print([np.min(source[xlabel]), np.max(source[xlabel])])\n\n    if ylog:\n        source[ylabel] = np.log10(source[ylabel])\n        source = source.rename(columns={ylabel: f\"log10({ylabel})\"})\n        ylabel = f\"log10({ylabel})\"\n\n    wave = self.wavelike[\"wavelength\"]\n    time = self.timelike[\"time\"]\n\n    if len(ylim) &gt; 0:\n        domainy = ylim\n        ywidth = 230 / len(wave[(wave.value &gt;= ylim[0]) &amp; (wave.value &lt;= ylim[1])])\n    else:\n        domainy = [\n            np.percentile(source[z], 2) - ybuffer,\n            np.percentile(source[z], 98) + ybuffer,\n        ]\n        ywidth = 230 / len(wave)\n\n    if len(xlim) &gt; 0:\n        domainx = xlim\n        xwidth = 230 / len(time[(time.value &gt;= xlim[0]) &amp; (time.value &lt;= xlim[1])])\n    else:\n        domainx = [np.min(source[xlabel]) - xbuffer, np.max(source[xlabel]) + xbuffer]\n        xwidth = 280 / len(time)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        # Add interactive part\n        brush = alt.selection(type=\"interval\", encodings=[\"y\"])\n\n        # Define the 3D spectrum plot\n        spectrum = (\n            alt.Chart(source, width=280, height=230)\n            .mark_rect(\n                clip=True,\n                width=xwidth,\n                height=ywidth,\n            )\n            .encode(\n                x=alt.X(\n                    f\"{xlabel}:Q\",\n                    scale=alt.Scale(zero=False, nice=False, domain=domainx),\n                ),\n                y=alt.Y(\n                    f\"{ylabel}:Q\",\n                    scale=alt.Scale(\n                        zero=False,\n                        nice=False,\n                        domain=[np.max(source[ylabel]), np.min(source[ylabel])],\n                    ),\n                ),\n                fill=alt.Color(\n                    f\"{z}:Q\",\n                    scale=alt.Scale(\n                        scheme=cmap,\n                        zero=False,\n                        domain=domainy,\n                    ),\n                ),\n                tooltip=[f\"{xlabel}\", f\"{ylabel}\", f\"{z}\"],\n            )\n        )\n\n        # gray out the background with selection\n        background = spectrum.encode(color=alt.value(\"#ddd\")).add_selection(brush)\n\n        # highlights on the transformed data\n        highlight = spectrum.transform_filter(brush)\n\n        # Layer the various plotting parts\n        spectrum_int = alt.layer(background, highlight, data=source)\n\n        # axis=alt.Axis(title=f\"{xlabel} - First Obs\"),\n        # Add the 2D averaged lightcurve (or uncertainty)\n        lightcurve = (\n            alt.Chart(\n                source, width=280, height=230, title=f\"Mean {z} for Wavelength Range\"\n            )\n            .mark_point(filled=True, size=20, clip=True, color=\"black\")\n            .encode(\n                x=alt.X(\n                    f\"{xlabel}:Q\",\n                    scale=alt.Scale(\n                        zero=False,\n                        nice=False,\n                        domain=domainx,\n                    ),\n                ),\n                y=alt.Y(\n                    f\"mean({z}):Q\",\n                    scale=alt.Scale(zero=False, domain=domainy),\n                    title=\"Mean \" + z,\n                ),\n            )\n            .transform_filter(brush)\n        )\n\n        # display the interactive Altair plot\n        (spectrum_int | lightcurve).display()\n        if filename is not None:\n            (spectrum_int | lightcurve).save(self._label_plot_file(filename))\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.rainbows.visualizations.interactive.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.pcolormesh--parameters","title":"Parameters","text":"<p>ax : Axes, optional     The axes into which to make this plot. quantity : str, optional     The fluxlike quantity to imshow.     (Must be a key of <code>rainbow.fluxlike</code>). xaxis : str     What to use as the horizontal axis, 'time' or 'wavelength'. w_unit : str, Unit, optional     The unit for plotting wavelengths. t_unit : str, Unit, optional     The unit for plotting times. colorbar : bool, optional     Should we include a colorbar? mask_ok : bool, optional     Should we mark which data are not OK? color_ok : str, optional     The color to be used for masking data points that are not OK. alpha_ok : float, optional     The transparency to be used for masking data points that are not OK. **kw : dict, optional     All other keywords will be passed on to <code>plt.pcolormesh</code>,     so you can have more detailed control over the plot     appearance. Common keyword argumentsvli might include:     <code>[cmap, norm, alpha, vmin, vmax]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pcolormesh.html</p> Source code in <code>chromatic/rainbows/visualizations/pcolormesh.py</code> <pre><code>def pcolormesh(\n    self,\n    ax=None,\n    quantity=\"flux\",\n    xaxis=\"time\",\n    w_unit=\"micron\",\n    t_unit=\"day\",\n    colorbar=True,\n    mask_ok=True,\n    color_ok=\"tomato\",\n    alpha_ok=0.8,\n    vmin=None,\n    vmax=None,\n    filename=None,\n    **kw,\n):\n    \"\"\"\n    Paint a 2D image of flux as a function of time and wavelength.\n\n    By using `.pcolormesh()`, pixels can transform based on their edges,\n    so non-uniform axes are allowed. This is a tiny bit slower than\n    `.imshow()`, but otherwise very similar. `.paint()` will try to\n    choose the best between `.imshow()` and `.pcolormesh()`.\n\n    Parameters\n    ----------\n    ax : Axes, optional\n        The axes into which to make this plot.\n    quantity : str, optional\n        The fluxlike quantity to imshow.\n        (Must be a key of `rainbow.fluxlike`).\n    xaxis : str\n        What to use as the horizontal axis, 'time' or 'wavelength'.\n    w_unit : str, Unit, optional\n        The unit for plotting wavelengths.\n    t_unit : str, Unit, optional\n        The unit for plotting times.\n    colorbar : bool, optional\n        Should we include a colorbar?\n    mask_ok : bool, optional\n        Should we mark which data are not OK?\n    color_ok : str, optional\n        The color to be used for masking data points that are not OK.\n    alpha_ok : float, optional\n        The transparency to be used for masking data points that are not OK.\n    **kw : dict, optional\n        All other keywords will be passed on to `plt.pcolormesh`,\n        so you can have more detailed control over the plot\n        appearance. Common keyword argumentsvli might include:\n        `[cmap, norm, alpha, vmin, vmax]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pcolormesh.html\n    \"\"\"\n\n    # self.speak(f'imshowing')\n    if ax is None:\n        ax = plt.subplot()\n\n    # get units\n    w_unit, t_unit = u.Unit(w_unit), u.Unit(t_unit)\n\n    # make sure some wavelength and time edges are defined\n    self._make_sure_wavelength_edges_are_defined()\n    self._make_sure_time_edges_are_defined()\n\n    # set up the wavelength and time edges\n    w_edges = leftright_to_edges(\n        self.wavelength_lower.to_value(w_unit), self.wavelength_upper.to_value(w_unit)\n    )\n    t_edges = leftright_to_edges(\n        self.time_lower.to_value(t_unit), self.time_upper.to_value(t_unit)\n    )\n\n    wlabel = f\"{self._wave_label} ({w_unit.to_string('latex_inline')})\"\n    tlabel = f\"{self._time_label} ({t_unit.to_string('latex_inline')})\"\n\n    def get_2D(k):\n        \"\"\"\n        A small helper to get a 2D quantity. This is a bit of\n        a kludge to help with weird cases of duplicate keys\n        (for example where 'wavelength' might appear in both\n        `wavelike` and `fluxlike`).\n        \"\"\"\n        z = self.get(k)\n        if np.shape(z) == self.shape:\n            return z\n        else:\n            return self.fluxlike.get(k, None)\n\n    if xaxis.lower()[0] == \"t\":\n        x, y = t_edges, w_edges\n        xlabel, ylabel = tlabel, wlabel\n        z = get_2D(quantity)\n        ok = get_2D(\"ok\")\n    elif xaxis.lower()[0] == \"w\":\n        x, y = w_edges, t_edges\n        xlabel, ylabel = wlabel, tlabel\n        z = get_2D(quantity).T\n        ok = get_2D(\"ok\").T\n    else:\n        cheerfully_suggest(\n            \"Please specify either `xaxis='time'` or `xaxis='wavelength'` for `.plot()`\"\n        )\n\n    # figure out a good shared color limits (unless already supplied)\n    vmin = vmin or np.nanpercentile(remove_unit(z).flatten(), 1)\n    vmax = vmax or np.nanpercentile(remove_unit(z).flatten(), 99)\n\n    # define some default keywords\n    pcolormesh_kw = dict(shading=\"flat\", vmin=vmin, vmax=vmax)\n    # I want to include `antialiased=True` to render nicely whether the plotting\n    # pixels super- or sub-sample the data pixels, but it appears not to have\n    # an effect in `pcolormesh`, so we just warn the user instead.\n\n    pcolormesh_kw.update(**kw)\n    with quantity_support():\n        plt.sca(ax)\n        if mask_ok:\n            okpcolormesh_kw = dict(**pcolormesh_kw)\n            okpcolormesh_kw.update(\n                cmap=one2another(\n                    bottom=color_ok,\n                    top=color_ok,\n                    alpha_bottom=alpha_ok,\n                    alpha_top=0,\n                ),\n                zorder=10,\n                vmin=0,\n                vmax=1,\n            )\n            plt.pcolormesh(\n                remove_unit(x),\n                remove_unit(y),\n                remove_unit(ok),\n                **okpcolormesh_kw,\n            )\n        pcolormeshed = plt.pcolormesh(\n            remove_unit(x),\n            remove_unit(y),\n            remove_unit(z),\n            **pcolormesh_kw,\n        )\n        plt.ylabel(ylabel)\n        plt.xlabel(xlabel)\n        if colorbar:\n            plt.colorbar(\n                ax=ax,\n                label=u.Quantity(z).unit.to_string(\"latex_inline\"),\n            )\n        # emulate origin = upper for imshow (y starts at top)\n        plt.ylim(y[-1], y[0])\n        plt.title(self.get(\"title\"))\n\n        # offer warning if aliasing might be a problem\n        plt.draw()\n        bbox = pcolormeshed.get_window_extent()\n        render_pixels = np.abs([bbox.width, bbox.height])\n        data_pixels = np.array(z.shape[::-1])\n        ratios = render_pixels / data_pixels\n        aliasing_warning_threshold = 2\n        if np.min(ratios) &lt; aliasing_warning_threshold:\n            cheerfully_suggest(\n                f\"\"\"\n            In using`.pcolormesh`, [display pixels] / [data pixels] =\n            {np.round(render_pixels)} / {data_pixels} = {ratios} {(xlabel.split()[0], ylabel.split()[0])}\n            Is less than the suggested threshold of {aliasing_warning_threshold}.\n\n            This suggests that aliasing/moir\u00e9 might be a problem, where too many\n            data pixels are trying to be displayed with too few pixels, and the\n            choices `matplotlib` makes for how to do that might not be intuitive.\n\n            Here are possible solutions:\n                - Use `.bin()` to decrease the number of data pixels in wavelength\n                and/or time, effectively averaging before displaying, rather than\n                asking `matplotlib` to decide how to visually average adjacent data.\n                - Increase the `dpi` of the figure in which this appears, so there are\n                enough display pixels to represent all the data pixels being shown.\n                - Use `.imshow()` instead of `.pcolormesh`, which can do better\n                built-in handling of antialiasing for large data arrays. Since `.imshow()`\n                can only show uniform wavelength and time grids, non-uniform grids will\n                be labeled via index instead of actual value.\n            \"\"\"\n            )\n    if filename is not None:\n        self.savefig(filename)\n    return ax\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.plot_lightcurves--parameters","title":"Parameters","text":"<p>ax : Axes, optional     The axes into which to make this plot. spacing : None, float, optional     The spacing between light curves.     (Might still change how this works.)     None uses half the standard dev of entire flux data. w_unit : str, Unit, optional     The unit for plotting wavelengths. t_unit : str, Unit, optional     The unit for plotting times. cmap : str, Colormap, optional     The color map to use for expressing wavelength. vmin : Quantity, optional     The minimum value to use for the wavelength colormap. vmax : Quantity, optional     The maximum value to use for the wavelength colormap. errorbar : boolean, optional     Should we plot errorbars? text : boolean, optional     Should we label each lightcurve? minimum_acceptable_ok : float     The smallest value of <code>ok</code> that will still be included.     (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data) plotkw : dict, optional     A dictionary of keywords passed to <code>plt.plot</code>     so you can have more detailed control over the plot     appearance. Common keyword arguments might include:     <code>[alpha, clip_on, zorder, marker, markersize,       linewidth, linestyle, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html errorbarkw : dict, optional     A dictionary of keywords passed to <code>plt.errorbar</code>     so you can have more detailed control over the plot     appearance. Common keyword arguments might include:     <code>[alpha, elinewidth, color, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html textkw : dict, optional     A dictionary of keywords passed to <code>plt.text</code>     so you can have more detailed control over the text     appearance. Common keyword arguments might include:     <code>[alpha, backgroundcolor, color, fontfamily, fontsize,       fontstyle, fontweight, rotation, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html **kw : dict, optional     Any additional keywords will be stored as <code>kw</code>.     Nothing will happen with them.</p> Source code in <code>chromatic/rainbows/visualizations/plot_lightcurves.py</code> <pre><code>def plot_lightcurves(\n    self,\n    quantity=\"flux\",\n    ax=None,\n    spacing=None,\n    w_unit=\"micron\",\n    t_unit=\"day\",\n    cmap=None,\n    vmin=None,\n    vmax=None,\n    errorbar=True,\n    text=True,\n    minimum_acceptable_ok=0.8,\n    plotkw={},\n    errorbarkw={},\n    textkw={},\n    filename=None,\n    scaling=1,\n    label_scatter=False,\n    **kw,\n):\n    \"\"\"\n    Plot flux as sequence of offset light curves.\n\n    Parameters\n    ----------\n    ax : Axes, optional\n        The axes into which to make this plot.\n    spacing : None, float, optional\n        The spacing between light curves.\n        (Might still change how this works.)\n        None uses half the standard dev of entire flux data.\n    w_unit : str, Unit, optional\n        The unit for plotting wavelengths.\n    t_unit : str, Unit, optional\n        The unit for plotting times.\n    cmap : str, Colormap, optional\n        The color map to use for expressing wavelength.\n    vmin : Quantity, optional\n        The minimum value to use for the wavelength colormap.\n    vmax : Quantity, optional\n        The maximum value to use for the wavelength colormap.\n    errorbar : boolean, optional\n        Should we plot errorbars?\n    text : boolean, optional\n        Should we label each lightcurve?\n    minimum_acceptable_ok : float\n        The smallest value of `ok` that will still be included.\n        (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data)\n    plotkw : dict, optional\n        A dictionary of keywords passed to `plt.plot`\n        so you can have more detailed control over the plot\n        appearance. Common keyword arguments might include:\n        `[alpha, clip_on, zorder, marker, markersize,\n          linewidth, linestyle, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n    errorbarkw : dict, optional\n        A dictionary of keywords passed to `plt.errorbar`\n        so you can have more detailed control over the plot\n        appearance. Common keyword arguments might include:\n        `[alpha, elinewidth, color, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html\n    textkw : dict, optional\n        A dictionary of keywords passed to `plt.text`\n        so you can have more detailed control over the text\n        appearance. Common keyword arguments might include:\n        `[alpha, backgroundcolor, color, fontfamily, fontsize,\n          fontstyle, fontweight, rotation, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html\n    **kw : dict, optional\n        Any additional keywords will be stored as `kw`.\n        Nothing will happen with them.\n    \"\"\"\n    if len(kw) &gt; 0:\n        message = f\"\"\"\n        You provided the keyword argument(s)\n        {kw}\n        but this function doesn't know how to\n        use them. Sorry!\n        \"\"\"\n        cheerfully_suggest(message)\n\n    # make sure that the wavelength-based colormap is defined\n    self._make_sure_cmap_is_defined(cmap=cmap, vmin=vmin, vmax=vmax)\n\n    w_unit, t_unit = u.Unit(w_unit), u.Unit(t_unit)\n\n    min_time = np.nanmin(self.time.to_value(t_unit))\n    max_time = np.nanmax(self.time.to_value(t_unit))\n\n    # make sure ax is set up\n    if ax is None:\n        fi = plt.figure(\n            figsize=plt.matplotlib.rcParams[\"figure.figsize\"][::-1],\n            constrained_layout=True,\n        )\n        ax = plt.subplot()\n    plt.sca(ax)\n\n    # figure out the spacing to use\n    if spacing is None:\n        try:\n            spacing = ax._most_recent_chromatic_plot_spacing\n        except AttributeError:\n            spacing = 3 * np.nanstd(self.get(quantity))\n    ax._most_recent_chromatic_plot_spacing = spacing\n\n    # TO-DO: check if this Rainbow has been normalized\n    if self._is_probably_normalized() or \"model\" in self.fluxlike:\n        label_y = \"1 - (0.5 + i) * spacing\"\n        ylim = 1 - np.array([self.nwave + 1, -1]) * spacing\n    else:\n        label_y = \"np.median(plot_y) - 0.5 * spacing\"\n        cheerfully_suggest(\n            \"\"\"\n            It's not clear if/how this object has been normalized.\n            Be aware that the baseline flux levels may therefore\n            be a little bit funny in .plot().\"\"\"\n        )\n        ylim = None\n    with quantity_support():\n\n        if label_scatter:\n            measured_rms = self.get_measured_scatter(quantity=\"residuals\")\n            expected_rms = self.get_expected_uncertainty()\n\n        #  loop through wavelengths\n        for i, w in enumerate(self.wavelength):\n\n            # grab the quantity and yerr for this particular wavelength\n            t, y, sigma = self.get_ok_data_for_wavelength(\n                i, minimum_acceptable_ok=minimum_acceptable_ok, y=quantity\n            )\n\n            if np.any(np.isfinite(y)):\n\n                plot_x = t.to_value(t_unit)\n\n                # add an offset to this quantity\n                plot_y = -i * spacing + (u.Quantity(y).value - 1) * scaling + 1\n                plot_sigma = u.Quantity(sigma).value * scaling\n\n                # get the color for this quantity\n                color = self.get_wavelength_color(w)\n\n                # plot the data points (with offsets)\n                this_plotkw = dict(marker=\"o\", linestyle=\"-\", markersize=5, color=color)\n                this_plotkw.update(**plotkw)\n\n                # set default for error bar lines\n                this_errorbarkw = dict(\n                    color=color, linewidth=0, elinewidth=1, zorder=-1\n                )\n                this_errorbarkw.update(**errorbarkw)\n\n                if errorbar:\n                    plt.errorbar(\n                        plot_x,\n                        plot_y,\n                        yerr=plot_sigma,\n                        **this_errorbarkw,\n                    )\n                plt.plot(plot_x, plot_y, **this_plotkw)\n\n                # add text labels next to each quantity plot\n                this_textkw = dict(va=\"center\", color=color)\n                this_textkw.update(**textkw)\n                if text:\n                    plt.text(\n                        min_time,\n                        eval(label_y),\n                        f\"{w.to_value(w_unit):.2f} {w_unit.to_string('latex_inline')}\",\n                        **this_textkw,\n                    )\n\n                if label_scatter is not False:\n                    this_textkw.update(ha=\"right\")\n                    measured = measured_rms[i]\n                    expected = expected_rms[i]\n                    cadence = self.dt\n                    if text:\n                        plt.text(\n                            max_time,\n                            eval(label_y),\n                            eval(f'f\"{label_scatter}\"'),\n                            **this_textkw,\n                        )\n\n        # add text labels to the plot\n        plt.xlabel(f\"{self._time_label} ({t_unit.to_string('latex_inline')})\")\n        plt.ylabel(\"Relative Flux (+ offsets)\")\n        if ylim is not None:\n            if ylim[1] != ylim[0]:\n                plt.ylim(*ylim)\n        plt.title(self.get(\"title\"))\n\n    if filename is not None:\n        self.savefig(filename)\n    return ax\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.plot_spectra--parameters","title":"Parameters","text":"<p>ax : Axes     The axes into which to make this plot. spacing : None, float     The spacing between light curves.     (Might still change how this works.)     None uses half the standard dev of entire flux data. w_unit : str, Unit     The unit for plotting wavelengths. t_unit : str, Unit     The unit for plotting times. cmap : str, Colormap     The color map to use for expressing wavelength. vmin : Quantity     The minimum value to use for the wavelength colormap. vmax : Quantity     The maximum value to use for the wavelength colormap. errorbar : boolean     Should we plot errorbars? text : boolean     Should we label each spectrum? minimum_acceptable_ok : float     The smallest value of <code>ok</code> that will still be included.     (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data) scatterkw : dict     A dictionary of keywords passed to <code>plt.scatter</code>     so you can have more detailed control over the text     appearance. Common keyword arguments might include:     <code>[alpha, color, s, m, edgecolor, facecolor]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html errorbarkw : dict     A dictionary of keywords passed to <code>plt.errorbar</code>     so you can have more detailed control over the plot     appearance. Common keyword arguments might include:     <code>[alpha, elinewidth, color, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html plotkw : dict     A dictionary of keywords passed to <code>plt.plot</code>     so you can have more detailed control over the plot     appearance. Common keyword arguments might include:     <code>[alpha, clip_on, zorder, marker, markersize,       linewidth, linestyle, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html textkw : dict     A dictionary of keywords passed to <code>plt.text</code>     so you can have more detailed control over the text     appearance. Common keyword arguments might include:     <code>[alpha, backgroundcolor, color, fontfamily, fontsize,       fontstyle, fontweight, rotation, zorder]</code> (and more)     More details are available at     https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html **kw : dict     Any additional keywords will be stored as <code>kw</code>.     Nothing will happen with them.</p> Source code in <code>chromatic/rainbows/visualizations/plot_spectra.py</code> <pre><code>def plot_spectra(\n    self,\n    quantity=\"flux\",\n    ax=None,\n    spacing=0.1,\n    w_unit=\"micron\",\n    t_unit=\"day\",\n    cmap=None,\n    vmin=None,\n    vmax=None,\n    errorbar=True,\n    text=True,\n    minimum_acceptable_ok=1,\n    scatterkw={},\n    errorbarkw={},\n    plotkw={},\n    textkw={},\n    filename=None,\n    **kw,\n):\n    \"\"\"\n    Plot flux as sequence of offset spectrum.\n\n    Parameters\n    ----------\n    ax : Axes\n        The axes into which to make this plot.\n    spacing : None, float\n        The spacing between light curves.\n        (Might still change how this works.)\n        None uses half the standard dev of entire flux data.\n    w_unit : str, Unit\n        The unit for plotting wavelengths.\n    t_unit : str, Unit\n        The unit for plotting times.\n    cmap : str, Colormap\n        The color map to use for expressing wavelength.\n    vmin : Quantity\n        The minimum value to use for the wavelength colormap.\n    vmax : Quantity\n        The maximum value to use for the wavelength colormap.\n    errorbar : boolean\n        Should we plot errorbars?\n    text : boolean\n        Should we label each spectrum?\n    minimum_acceptable_ok : float\n        The smallest value of `ok` that will still be included.\n        (1 for perfect data, 1e-10 for everything but terrible data, 0 for all data)\n    scatterkw : dict\n        A dictionary of keywords passed to `plt.scatter`\n        so you can have more detailed control over the text\n        appearance. Common keyword arguments might include:\n        `[alpha, color, s, m, edgecolor, facecolor]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n    errorbarkw : dict\n        A dictionary of keywords passed to `plt.errorbar`\n        so you can have more detailed control over the plot\n        appearance. Common keyword arguments might include:\n        `[alpha, elinewidth, color, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html\n    plotkw : dict\n        A dictionary of keywords passed to `plt.plot`\n        so you can have more detailed control over the plot\n        appearance. Common keyword arguments might include:\n        `[alpha, clip_on, zorder, marker, markersize,\n          linewidth, linestyle, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n    textkw : dict\n        A dictionary of keywords passed to `plt.text`\n        so you can have more detailed control over the text\n        appearance. Common keyword arguments might include:\n        `[alpha, backgroundcolor, color, fontfamily, fontsize,\n          fontstyle, fontweight, rotation, zorder]` (and more)\n        More details are available at\n        https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.text.html\n    **kw : dict\n        Any additional keywords will be stored as `kw`.\n        Nothing will happen with them.\n    \"\"\"\n\n    if len(kw) &gt; 0:\n        message = f\"\"\"\n        You provided the keyword argument(s)\n        {kw}\n        but this function doesn't know how to\n        use them. Sorry!\n        \"\"\"\n        cheerfully_suggest(message)\n\n    # make sure that the wavelength-based colormap is defined\n    self._make_sure_cmap_is_defined(cmap=cmap, vmin=vmin, vmax=vmax)\n\n    w_unit, t_unit = u.Unit(w_unit), u.Unit(t_unit)\n\n    min_wave = np.nanmin(self.wavelength.to_value(w_unit))\n\n    # make sure ax is set up\n    if ax is None:\n        fi = plt.figure(\n            figsize=plt.matplotlib.rcParams[\"figure.figsize\"][::-1],\n            constrained_layout=True,\n        )\n        ax = plt.subplot()\n    plt.sca(ax)\n\n    # figure out the spacing to use\n    if spacing is None:\n        try:\n            spacing = ax._most_recent_chromatic_plot_spacing\n        except AttributeError:\n            spacing = 3 * np.nanstd(self.get(quantity))\n    ax._most_recent_chromatic_plot_spacing = spacing\n\n    # TO-DO: check if this Rainbow has been normalized\n    '''cheerfully_suggest(\n        \"\"\"\n    It's not clear if/how this object has been normalized.\n    Be aware that the baseline flux levels may therefore\n    be a little bit funny in .plot().\"\"\"\n    )'''\n    with quantity_support():\n\n        #  loop through times\n        for i, t in enumerate(self.time):\n            # grab the spectrum for this particular time\n            w, y, sigma = self.get_ok_data_for_time(\n                i, minimum_acceptable_ok=minimum_acceptable_ok, y=quantity\n            )\n            if np.any(np.isfinite(y)):\n\n                plot_x = w.to_value(w_unit)\n\n                # add an offset to this spectrum\n                plot_y = -i * spacing + u.Quantity(y).value\n                plot_sigma = u.Quantity(sigma).value\n\n                default_color = \"black\"\n\n                # set default for background line plot\n                this_plotkw = dict(color=default_color, zorder=-1)\n                this_plotkw.update(**plotkw)\n\n                # set default for scatter plot with points\n                this_scatterkw = dict(\n                    marker=\"o\",\n                    linestyle=\"-\",\n                    c=plot_x,\n                    cmap=self.cmap,\n                    norm=self.norm,\n                )\n                this_scatterkw.update(**scatterkw)\n\n                # set default for error bar lines\n                this_errorbarkw = dict(\n                    color=default_color, linewidth=0, elinewidth=1, zorder=-1\n                )\n                this_errorbarkw.update(**errorbarkw)\n\n                if errorbar:\n                    plt.errorbar(\n                        plot_x,\n                        plot_y,\n                        yerr=plot_sigma,\n                        **this_errorbarkw,\n                    )\n                plt.plot(plot_x, plot_y, **this_plotkw)\n                plt.scatter(plot_x, plot_y, **this_scatterkw)\n\n                # add text labels next to each spectrum\n                this_textkw = dict(va=\"center\", color=default_color)\n                this_textkw.update(**textkw)\n                if text:\n                    plt.text(\n                        min_wave,\n                        np.median(plot_y) - 0.5 * spacing,\n                        f\"{t.to_value(t_unit):.2f} {t_unit.to_string('latex_inline')}\",\n                        **this_textkw,\n                    )\n\n        # add text labels to the plot\n        plt.xlabel(f\"Wavelength ({w_unit.to_string('latex_inline')})\")\n        plt.ylabel(\"Relative Flux (+ offsets)\")\n        if self.get(\"wscale\") == \"log\":\n            plt.xscale(\"log\")\n        plt.title(self.get(\"title\"))\n    if filename is not None:\n        self.savefig(filename)\n    return ax\n</code></pre>"},{"location":"api/#chromatic.rainbows.visualizations.plot--parameters","title":"Parameters","text":"<p>xaxis : string     What should be plotted on the x-axis of the plot?     'time' will plot a different light curve for each wavelength     'wavelength' will plot a different spectrum for each timepoint **kw : dict     All other keywords will be passed along to either     <code>.plot_lightcurves</code> or <code>.plot_spectra</code> as appropriate.     Please see the docstrings for either of those functions     to figure out what keyword arguments you might want to     provide here.</p> Source code in <code>chromatic/rainbows/visualizations/plot.py</code> <pre><code>def plot(self, xaxis=\"time\", **kw):\n    \"\"\"\n    Plot flux either as a sequence of offset lightcurves (default)\n    or as a sequence of offset spectra.\n\n    Parameters\n    ----------\n    xaxis : string\n        What should be plotted on the x-axis of the plot?\n        'time' will plot a different light curve for each wavelength\n        'wavelength' will plot a different spectrum for each timepoint\n    **kw : dict\n        All other keywords will be passed along to either\n        `.plot_lightcurves` or `.plot_spectra` as appropriate.\n        Please see the docstrings for either of those functions\n        to figure out what keyword arguments you might want to\n        provide here.\n    \"\"\"\n\n    if xaxis.lower()[0] == \"t\":\n        return self.plot_lightcurves(**kw)\n    elif xaxis.lower()[0] == \"w\":\n        return self.plot_spectra(**kw)\n    else:\n        cheerfully_suggest(\"Please specify either 'time' or 'wavelength' for `.plot()`\")\n</code></pre>"},{"location":"api/#tools","title":"\ud83d\udd28 Tools","text":"<p>Calculate the surface flux from a thermally emitted surface, according to Planck function, in units of photons/(s * m**2 * nm).</p> <p>Get a PHOENIX model spectrum for an arbitrary temperature, logg, metallicity.</p> <p>Calculate the surface flux from a thermally emitted surface, according to PHOENIX model spectra, in units of photons/(s * m**2 * nm).</p> <p>Tools for resampling array from one grid of independent variables to another.</p>"},{"location":"api/#chromatic.spectra.planck.get_planck_photons--parameters","title":"Parameters","text":"<p>temperature : Quantity     The temperature of the thermal emitter,     with units of K. wavelength : Quantity, optional     The wavelengths at which to calculate,     with units of wavelength. R : float, optional     The spectroscopic resolution for creating a log-uniform     grid that spans the limits set by <code>wlim</code>, only if     <code>wavelength</code> is not defined. wlim : Quantity, optional     The two-element [lower, upper] limits of a wavelength     grid that would be populated with resolution <code>R</code>, only if     <code>wavelength</code> is not defined. **kw : dict, optional     Other keyword arguments will be ignored.</p>"},{"location":"api/#chromatic.spectra.planck.get_planck_photons--returns","title":"Returns","text":"<p>photons : Quantity     The surface flux in photon units</p> <p>This evaluates the Planck function at the exact wavelength values; it doesn't do anything fancy to integrate over binwidths, so if you're using very wide (R~a few) bins your integrated fluxes will be messed up.</p> Source code in <code>chromatic/spectra/planck.py</code> <pre><code>def get_planck_photons(\n    temperature=3000, wavelength=None, R=100, wlim=[0.04, 6] * u.micron, **kw\n):\n    \"\"\"\n    Calculate the surface flux from a thermally emitted surface,\n    according to Planck function, in units of photons/(s * m**2 * nm).\n\n    Parameters\n    ----------\n    temperature : Quantity\n        The temperature of the thermal emitter,\n        with units of K.\n    wavelength : Quantity, optional\n        The wavelengths at which to calculate,\n        with units of wavelength.\n    R : float, optional\n        The spectroscopic resolution for creating a log-uniform\n        grid that spans the limits set by `wlim`, only if\n        `wavelength` is not defined.\n    wlim : Quantity, optional\n        The two-element [lower, upper] limits of a wavelength\n        grid that would be populated with resolution `R`, only if\n        `wavelength` is not defined.\n    **kw : dict, optional\n        Other keyword arguments will be ignored.\n\n    Returns\n    -------\n    photons : Quantity\n        The surface flux in photon units\n\n    This evaluates the Planck function at the exact\n    wavelength values; it doesn't do anything fancy to integrate\n    over binwidths, so if you're using very wide (R~a few) bins\n    your integrated fluxes will be messed up.\n\n    \"\"\"\n\n    # make sure the temperature unit is good (whether or not it's supplied)\n    temperature_unit = u.Quantity(temperature).unit\n    if temperature_unit == u.K:\n        temperature_with_unit = temperature\n    elif temperature_unit == u.Unit(\"\"):\n        temperature_with_unit = temperature * u.K\n\n    # create a wavelength grid if one isn't supplied\n    if wavelength is None:\n        wavelength_unit = wlim.unit\n        wavelength = (\n            np.exp(np.arange(np.log(wlim[0].value), np.log(wlim[1].value), 1 / R))\n            * wavelength_unit\n        )\n\n    energy = calculate_planck_flux(\n        wavelength=wavelength, temperature=temperature_with_unit\n    )\n    photon_energy = con.h * con.c / wavelength / u.ph\n\n    return wavelength, (energy / photon_energy).to(u.ph / (u.s * u.m**2 * u.nm))\n</code></pre>"},{"location":"api/#chromatic.spectra.phoenix.get_phoenix_photons--parameters","title":"Parameters","text":"<p>temperature : float, optional     Temperature, in K (with no astropy units attached). logg : float, optional     Surface gravity log10[g/(cm/s**2)] (with no astropy units attached). metallicity : float, optional     Metallicity log10[metals/solar] (with no astropy units attached). R : float, optional     Spectroscopic resolution (lambda/dlambda). Currently, this must     be in one of [3,10,30,100,300,1000,3000,10000,30000,100000], but     check back soon for custom wavelength grids. There is extra     overhead associated with switching resolutions, so if you're     going to retrieve many spectra, try to group by resolution.     (If you're using the <code>wavelength</code> or <code>wavelength_edges</code> option     below, please be ensure your requested R exceeds that needed     to support your wavelengths.) wavelength : Quantity, optional     A grid of wavelengths on which you would like your spectrum.     If this is None, the complete wavelength array will be returned     at your desired resolution. Otherwise, the spectrum will be     returned exactly at those wavelengths. Grid points will be     cached for this new wavelength grid to speed up applications     that need to retreive lots of similar spectra for the same     wavelength (like many optimization or sampling problems). wavelength_edges : Quantity, optional     Same as <code>wavelength</code> (see above!) but defining the wavelength     grid by its edges instead of its centers. The returned spectrum     will have 1 fewer element than <code>wavelength_edges</code>.</p>"},{"location":"api/#chromatic.spectra.phoenix.get_phoenix_photons--returns","title":"Returns","text":"<p>wavelength : Quantity     The wavelengths, at the specified resolution. photons : Quantity     The surface flux in photon units</p> Source code in <code>chromatic/spectra/phoenix.py</code> <pre><code>def get_phoenix_photons(\n    temperature=5780,\n    logg=4.43,\n    metallicity=0.0,\n    R=100,\n    wavelength=None,\n    wavelength_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Get a PHOENIX model spectrum for an arbitrary temperature, logg, metallicity.\n\n    Calculate the surface flux from a thermally emitted surface,\n    according to PHOENIX model spectra, in units of photons/(s * m**2 * nm).\n\n    Parameters\n    ----------\n    temperature : float, optional\n        Temperature, in K (with no astropy units attached).\n    logg : float, optional\n        Surface gravity log10[g/(cm/s**2)] (with no astropy units attached).\n    metallicity : float, optional\n        Metallicity log10[metals/solar] (with no astropy units attached).\n    R : float, optional\n        Spectroscopic resolution (lambda/dlambda). Currently, this must\n        be in one of [3,10,30,100,300,1000,3000,10000,30000,100000], but\n        check back soon for custom wavelength grids. There is extra\n        overhead associated with switching resolutions, so if you're\n        going to retrieve many spectra, try to group by resolution.\n        (If you're using the `wavelength` or `wavelength_edges` option\n        below, please be ensure your requested R exceeds that needed\n        to support your wavelengths.)\n    wavelength : Quantity, optional\n        A grid of wavelengths on which you would like your spectrum.\n        If this is None, the complete wavelength array will be returned\n        at your desired resolution. Otherwise, the spectrum will be\n        returned exactly at those wavelengths. Grid points will be\n        cached for this new wavelength grid to speed up applications\n        that need to retreive lots of similar spectra for the same\n        wavelength (like many optimization or sampling problems).\n    wavelength_edges : Quantity, optional\n        Same as `wavelength` (see above!) but defining the wavelength\n        grid by its edges instead of its centers. The returned spectrum\n        will have 1 fewer element than `wavelength_edges`.\n\n    Returns\n    -------\n    wavelength : Quantity\n        The wavelengths, at the specified resolution.\n    photons : Quantity\n        The surface flux in photon units\n    \"\"\"\n    return phoenix_library.get_spectrum(\n        temperature=temperature,\n        logg=logg,\n        metallicity=metallicity,\n        R=R,\n        wavelength=wavelength,\n        wavelength_edges=wavelength_edges,\n        visualize=visualize,\n    )\n</code></pre>"},{"location":"api/#chromatic.tools.resampling.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.tools.resampling.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.tools.resampling.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.tools.resampling.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.tools.resampling.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.tools.resampling.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.tools.resampling.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.tools.resampling.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.tools.resampling.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.tools.resampling.calculate_bin_widths","title":"<code>calculate_bin_widths(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin sizes. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.tools.resampling.calculate_bin_widths--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.tools.resampling.calculate_bin_widths--returns","title":"Returns","text":"<p>s : array     The array of bin sizes (total size, from left to right).</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_widths(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin sizes.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    s : array\n        The array of bin sizes (total size, from left to right).\n    \"\"\"\n\n    # OLD VERSION\n    # binsize = np.zeros_like(x)\n    # binsize[0:-1] = x[1:] - x[0:-1]\n    # binsize[-1] = binsize[-2]\n    left, right = calculate_bin_leftright(x)\n    binsize = right - left\n    return binsize\n</code></pre>"},{"location":"api/#chromatic.tools.resampling.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.tools.resampling.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.tools.resampling.plot_as_boxes","title":"<code>plot_as_boxes(x, y, xleft=None, xright=None, **kwargs)</code>","text":"<p>Plot with boxes, to show the left and right edges of a box. This is useful, or example, to plot flux associated with pixels, in case you are trying to do a sub-pixel resample or interpolation or shift.</p>"},{"location":"api/#chromatic.tools.resampling.plot_as_boxes--parameters","title":"Parameters","text":"<p>x : array     The original independent variable. y : array     The original dependent variable (same size as x). **kwargs : dict     All additional keywords will be passed to plt.plot</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def plot_as_boxes(x, y, xleft=None, xright=None, **kwargs):\n    \"\"\"\n    Plot with boxes, to show the left and right edges of a box.\n    This is useful, or example, to plot flux associated with\n    pixels, in case you are trying to do a sub-pixel resample\n    or interpolation or shift.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n    y : array\n        The original dependent variable (same size as x).\n    **kwargs : dict\n        All additional keywords will be passed to plt.plot\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    if (xleft is None) and (xright is None):\n        xleft, xright = calculate_bin_leftright(x)\n\n    # create a array doubling up the y values and interleaving the edges\n    plot_x = np.vstack((xleft, xright)).reshape((-1,), order=\"F\")\n    plot_y = np.vstack((y, y)).reshape((-1,), order=\"F\")\n\n    # plot those constructed arrays\n    plt.plot(plot_x, plot_y, **kwargs)\n</code></pre>"},{"location":"api/#chromatic.tools.resampling.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.tools.resampling.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.tools.resampling.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"api/#chromatic.imports.bintoR","title":"<code>bintoR(x, y, unc=None, R=50, xlim=None, weighting='inversevariance', drop_nans=True)</code>","text":"<p>Bin any x and y array onto a logarithmicly uniform grid.</p>"},{"location":"api/#chromatic.imports.bintoR--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None, optional     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) R : array, optional     The spectral resolution R=x/dx for creating a new,     logarithmically uniform grid that starts at the first     value of x. xlim : list, array, optional     A two-element list indicating the min and max values of     x for the new logarithmically spaced grid. If None,     these limits will be created from the data themselves weighting : str, optional     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool, optional     Should we skip any bins turn out to be nans?     This most often happens when bins are empty.</p>"},{"location":"api/#chromatic.imports.bintoR--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintoR(\n    x, y, unc=None, R=50, xlim=None, weighting=\"inversevariance\", drop_nans=True\n):\n    \"\"\"\n    Bin any x and y array onto a logarithmicly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None, optional\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    R : array, optional\n        The spectral resolution R=x/dx for creating a new,\n        logarithmically uniform grid that starts at the first\n        value of x.\n    xlim : list, array, optional\n        A two-element list indicating the min and max values of\n        x for the new logarithmically spaced grid. If None,\n        these limits will be created from the data themselves\n    weighting : str, optional\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool, optional\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n    \"\"\"\n\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    # create a new grid of x at the given resolution\n    lnx = np.log(x_without_unit)\n    dnewlnx = 1.0 / R\n\n    # set the limits of the new xgrid (in log space)\n    if xlim is None:\n        # use the input grid to set the limits\n        lnxbottom, lnxtop = np.nanmin(lnx), np.nanmax(lnx)\n    else:\n        # use the custom xlim to set the limits\n        lnxbottom, lnxtop = xlim\n\n    # create a new, log-uniform grid of x values\n    newlnx = np.arange(lnxbottom, lnxtop + dnewlnx, dnewlnx)\n\n    # now do the binning on a uniform grid of lnx\n    result = bintogrid(\n        lnx, y, unc, newx=newlnx, weighting=weighting, drop_nans=drop_nans\n    )\n\n    # convert back from log to real values\n    for k in [\"x\", \"x_edge_lower\", \"x_edge_upper\"]:\n        result[k] = np.exp(result[k]) * x_unit\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.imports.bintogrid","title":"<code>bintogrid(x=None, y=None, unc=None, newx=None, newx_edges=None, dx=None, nx=None, weighting='inversevariance', drop_nans=True, x_edges=None, visualize=False)</code>","text":"<p>Bin any x and y array onto a linearly uniform grid.</p>"},{"location":"api/#chromatic.imports.bintogrid--parameters","title":"Parameters","text":"<p>x : array     The original independent variable.     (For a spectrum example = wavelength) y : array     The original dependent variable (same size as x).     (For a spectrum example = flux) unc : array, None     The unceratinty on the dependent variable     (For a spectrum example = the flux uncertainty) nx : array     The number of bins from the original grid to     bin together into the new one. dx : array     The fixed spacing for creating a new, linearly uniform     grid that start at the first value of x. This will     be ignored if <code>newx</code> != None. newx : array     A new custom grid onto which we should bin. newx_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>,     so the size of the output array will be     <code>len(newx_edges) - 1</code> weighting : str     How should we weight values when averaging     them together into one larger bin?     <code>weighting = 'inversevariance'</code>         weights = 1/unc**2      <code>weighting = {literally anything else}</code>         uniform weights     This will have no impact if <code>unc == None</code>, or for any     new bins that effectively overlap less than one original     unbinned point. drop_nans : bool     Should we skip any bins turn out to be nans?     This most often happens when bins are empty. x_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>x_edges[:-1]</code> and <code>x_edges[1:]</code>,     respectively, so the associated <code>y</code> should have exactly     1 fewer element than <code>x_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>x</code>(still a little experimental)</p>"},{"location":"api/#chromatic.imports.bintogrid--returns","title":"Returns","text":"<p>result : dict     A dictionary containing at least...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>x_edge_lower</code> = the lower edges of the output grid         <code>x_edge_upper</code> = the upper edges of the output grid     ...and possibly also         <code>uncertainty</code> = the calculated uncertainty per bin</p> <p>The order of precendence for setting the new grid is [<code>newx_edges</code>, <code>newx</code>, <code>dx</code>, <code>nx</code>] The first will be used, and others will be ignored.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def bintogrid(\n    x=None,\n    y=None,\n    unc=None,\n    newx=None,\n    newx_edges=None,\n    dx=None,\n    nx=None,\n    weighting=\"inversevariance\",\n    drop_nans=True,\n    x_edges=None,\n    visualize=False,\n):\n    \"\"\"\n    Bin any x and y array onto a linearly uniform grid.\n\n    Parameters\n    ----------\n    x : array\n        The original independent variable.\n        (For a spectrum example = wavelength)\n    y : array\n        The original dependent variable (same size as x).\n        (For a spectrum example = flux)\n    unc : array, None\n        The unceratinty on the dependent variable\n        (For a spectrum example = the flux uncertainty)\n    nx : array\n        The number of bins from the original grid to\n        bin together into the new one.\n    dx : array\n        The fixed spacing for creating a new, linearly uniform\n        grid that start at the first value of x. This will\n        be ignored if `newx` != None.\n    newx : array\n        A new custom grid onto which we should bin.\n    newx_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `newx_edges[:-1]` and `newx_edges[1:]`,\n        so the size of the output array will be\n        `len(newx_edges) - 1`\n    weighting : str\n        How should we weight values when averaging\n        them together into one larger bin?\n        `weighting = 'inversevariance'`\n            weights = 1/unc**2\n         `weighting = {literally anything else}`\n            uniform weights\n        This will have no impact if `unc == None`, or for any\n        new bins that effectively overlap less than one original\n        unbinned point.\n    drop_nans : bool\n        Should we skip any bins turn out to be nans?\n        This most often happens when bins are empty.\n    x_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `x_edges[:-1]` and `x_edges[1:]`,\n        respectively, so the associated `y` should have exactly\n        1 fewer element than `x_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `x`(still a little experimental)\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing at least...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `x_edge_lower` = the lower edges of the output grid\n            `x_edge_upper` = the upper edges of the output grid\n        ...and possibly also\n            `uncertainty` = the calculated uncertainty per bin\n\n\n    The order of precendence for setting the new grid is\n    [`newx_edges`, `newx`, `dx`, `nx`]\n    The first will be used, and others will be ignored.\n    \"\"\"\n\n    # check that an OK set of inputs has been supplied\n    if (x is not None) and (x_edges is not None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 Both `x` and `x_edges` were supplied to `bintogrid`. Confusing!\"\"\"\n        )\n    if (x is None) and (x_edges is None):\n        raise RuntimeError(\n            \"\"\"\ud83c\udf08 At least one of `x` or `x_edges` must be supplied to `bintogrid`.\"\"\"\n        )\n    if y is None:\n        raise RuntimeError(\"\"\"\ud83c\udf08 `y` must be supplied to `bintogrid`.\"\"\")\n\n    # make sure the edges and the centers are set\n    if x is None:\n        x_left, x_right = edges_to_leftright(x_edges)\n        x = 0.5 * (left + right)\n    else:\n        x_left, x_right = calculate_bin_leftright(x)\n        x_edges = leftright_to_edges(x_left, x_right)\n    try:\n        x_unit = x.unit\n        x_without_unit = x.value\n    except AttributeError:\n        x_unit = 1\n        x_without_unit = x\n\n    try:\n        y_unit = y.unit\n        y_without_unit = y.value\n    except AttributeError:\n        y_unit = 1\n        y_without_unit = y\n\n    # warn if multiple inputs are provided\n    number_of_grid_options = np.sum([z is not None for z in [newx_edges, newx, dx, nx]])\n    if number_of_grid_options &gt; 1:\n        cheerfully_suggest(\n            \"\"\"More than one output grid sent to `bintogrid`.\n                         The one being used is the first to appear in\n                         [`newx_edges`, `newx`, `dx`, `nx`]\n                         but you might want to choose more carefully.\"\"\"\n        )\n\n    # define inputs based on the following order\n    if newx_edges is not None:\n        # define grid by its edges (and define others from there)\n        newx_edges_without_unit = u.Quantity(newx_edges).to(x_unit).value\n        dx_without_unit = np.diff(newx_edges_without_unit)\n        newx_without_unit = newx_edges_without_unit[:-1] + 0.5 * dx_without_unit\n        newx_left_without_unit = newx_edges_without_unit[:-1]\n        newx_right_without_unit = newx_edges_without_unit[1:]\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif newx is not None:\n        # define grid by its centers (and define others from there)\n        newx_without_unit = u.Quantity(newx).to(x_unit).value\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n        dx_without_unit = np.diff(newx_edges_without_unit)\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n    elif dx is not None:\n        # define grid by a bin width (and define others from there)\n        dx_without_unit = u.Quantity(dx).to(x_unit).value\n        newx_without_unit = np.arange(\n            np.nanmin(x_without_unit),\n            np.nanmax(x_without_unit) + dx_without_unit,\n            dx_without_unit,\n        )\n        newx_left_without_unit, newx_right_without_unit = calculate_bin_leftright(\n            newx_without_unit\n        )\n        newx_edges_without_unit = np.hstack(\n            [newx_left_without_unit, newx_right_without_unit[-1]]\n        )\n\n        # make sure the final output grid is defined\n        final_newx, final_newx_left, final_newx_right = (\n            newx_without_unit * x_unit,\n            newx_left_without_unit * x_unit,\n            newx_right_without_unit * x_unit,\n        )\n\n    elif nx is not None:\n        # keep track of the original input x values\n        original_x_without_unit = x_without_unit\n\n        # redefine the input x to indices, to do interpolation in index space\n        x_without_unit = np.arange(0, len(x_without_unit))\n\n        # define a grid of edges that will enclose the right number of indices\n        x_left_i, x_right_i = calculate_bin_leftright(x_without_unit)\n        newx_edges_without_unit = leftright_to_edges(x_left_i, x_right_i)[::nx]\n        newx_without_unit = 0.5 * (\n            newx_edges_without_unit[1:] + newx_edges_without_unit[:-1]\n        )\n\n        # calculate the actual x values corresponding to the bins\n        original_edges = leftright_to_edges(\n            *calculate_bin_leftright(original_x_without_unit)\n        )\n        final_edges = original_edges[::nx] * x_unit\n        final_newx_left, final_newx_right = edges_to_leftright(final_edges)\n        final_newx = 0.5 * (final_newx_left + final_newx_right)\n        dx_without_unit = (final_newx_right - final_newx_left) / x_unit\n    else:\n        raise RuntimeError(\n            \"\"\"No output grid sent to `bintogrid`.\n                              Please choose one of the following:\n                              [`newx_edges`, `newx`, `dx`, `nx`]\"\"\"\n        )\n\n    # don't complain about zero-divisions in here (to allow infinite uncertainties)\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n\n        # calculate weight integrals for the bin array\n        ok = np.isnan(y_without_unit) == False\n\n        # resample the sums onto that new grid\n        if unc is None:\n            weights = np.ones_like(x_without_unit)\n        else:\n            if weighting == \"inversevariance\":\n                weights = 1 / unc**2\n            else:\n                weights = np.ones_like(x_without_unit)\n\n            # ignore infinite weights (= 0 uncertainties)\n            ok *= np.isfinite(weights)\n\n        if np.any(ok):\n            numerator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=(y_without_unit * weights)[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n            denominator = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=weights[ok],\n                xout_edges=newx_edges_without_unit,\n            )\n\n            # the binned weighted means on the new grid\n            newy = numerator[\"y\"] / denominator[\"y\"]\n\n            # the standard error on the means, for those bins\n            newunc = np.sqrt(1 / denominator[\"y\"])\n\n            # keep track of the number of original bins going into each new bin\n            number_of_original_bins_per_new_bin = resample_while_conserving_flux(\n                xin=x_without_unit[ok],\n                yin=np.ones_like(y_without_unit)[ok],\n                xout_edges=newx_edges_without_unit,\n            )[\"y\"]\n        else:\n            newy = np.nan * newx_without_unit\n            newunc = np.nan * newx_without_unit\n            number_of_original_bins_per_new_bin = np.zeros_like(newx_without_unit)\n\n    # remove any empty bins\n    if drop_nans:\n        ok = np.isfinite(newy)\n    else:\n        ok = np.ones_like(newx_without_unit).astype(bool)\n\n    # if no uncertainties were given, don't return uncertainties\n    result = {}\n\n    # populate the new grid centers + edges + values\n    result[\"x\"] = final_newx[ok]\n    result[\"x_edge_lower\"] = final_newx_left[ok]\n    result[\"x_edge_upper\"] = final_newx_right[ok]\n\n    # populate the new grid values\n    result[\"y\"] = newy[ok] * y_unit\n\n    # populate the new grid value uncertainties\n    if unc is not None:\n        result[\"uncertainty\"] = newunc[ok] * y_unit\n\n    # store how many of the original pixels made it into this new one\n    result[\"N_unbinned/N_binned\"] = number_of_original_bins_per_new_bin[ok]\n    if visualize:\n        fi, ax = plt.subplots(\n            2, 1, figsize=(8, 4), dpi=300, gridspec_kw=dict(height_ratios=[1, 0.2])\n        )\n        plt.sca(ax[0])\n        plot_as_boxes(x, y, xleft=x_left, xright=x_right, color=\"silver\", linewidth=1)\n        ekw = dict(elinewidth=1, linewidth=0)\n        plt.errorbar(x, y, yerr=unc, color=\"silver\", marker=\"s\", **ekw)\n        plt.errorbar(\n            result[\"x\"],\n            result[\"y\"],\n            yerr=result.get(\"uncertainty\", None),\n            xerr=0.5 * (result[\"x_edge_upper\"] - result[\"x_edge_lower\"]) * x_unit,\n            marker=\"o\",\n            color=\"black\",\n            zorder=100,\n            **ekw,\n        )\n        plt.sca(ax[1])\n        plot_as_boxes(\n            result[\"x\"],\n            result[\"N_unbinned/N_binned\"],\n            xleft=result[\"x_edge_lower\"],\n            xright=result[\"x_edge_upper\"],\n        )\n        plt.ylabel(\"$N_{unbinned}/N_{binned}$\")\n        plt.ylim(0, None)\n\n    return result\n</code></pre>"},{"location":"api/#chromatic.imports.calculate_bin_leftright","title":"<code>calculate_bin_leftright(x)</code>","text":"<p>If x is an array of bin centers, calculate the bin edges. (assumes outermost bins are same size as their neighbors)</p>"},{"location":"api/#chromatic.imports.calculate_bin_leftright--parameters","title":"Parameters","text":"<p>x : array     The array of bin centers.</p>"},{"location":"api/#chromatic.imports.calculate_bin_leftright--returns","title":"Returns","text":"<p>l : array     The left edges of the bins. r : array     The right edges of the bins.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def calculate_bin_leftright(x):\n    \"\"\"\n    If x is an array of bin centers, calculate the bin edges.\n    (assumes outermost bins are same size as their neighbors)\n\n    Parameters\n    ----------\n    x : array\n        The array of bin centers.\n\n    Returns\n    ----------\n    l : array\n        The left edges of the bins.\n    r : array\n        The right edges of the bins.\n    \"\"\"\n\n    # what are bin edges (making a guess for those on the ends)\n    # xbinsize = calculate_bin_widths(x)\n    # left = x - xbinsize / 2.0\n    # right = x + xbinsize / 2.0\n\n    # weird corner case!\n    if len(x) == 1:\n        left, right = np.sort([0, 2 * x[0]])\n        return np.array([left]), np.array([right])\n\n    inner_edges = 0.5 * np.diff(x) + x[:-1]\n    first_edge = x[0] - (inner_edges[0] - x[0])\n    last_edge = x[-1] + (x[-1] - inner_edges[-1])\n\n    left = np.hstack([first_edge, inner_edges])\n    right = np.hstack([inner_edges, last_edge])\n\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.imports.edges_to_leftright","title":"<code>edges_to_leftright(edges)</code>","text":"<p>Convert N+1 contiguous edges to two arrays of N left/right edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def edges_to_leftright(edges):\n    \"\"\"\n    Convert N+1 contiguous edges to two arrays of N left/right edges.\n    \"\"\"\n    left, right = edges[:-1], edges[1:]\n    return left, right\n</code></pre>"},{"location":"api/#chromatic.imports.expand_filenames","title":"<code>expand_filenames(filepath)</code>","text":"<p>A wrapper to expand a string or list into a list of filenames.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def expand_filenames(filepath):\n    \"\"\"\n    A wrapper to expand a string or list into a list of filenames.\n    \"\"\"\n    if type(filepath) == list:\n        filenames = filepath\n    elif type(filepath) == Column:\n        filenames = list(filepath)\n    elif \"*\" in filepath:\n        filenames = np.sort(glob.glob(filepath))\n    else:\n        filenames = [filepath]\n    return sorted(filenames)\n</code></pre>"},{"location":"api/#chromatic.imports.leftright_to_edges","title":"<code>leftright_to_edges(left, right)</code>","text":"<p>Convert two arrays of N left/right edges to N+1 continugous edges.</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def leftright_to_edges(left, right):\n    \"\"\"\n    Convert two arrays of N left/right edges to N+1 continugous edges.\n    \"\"\"\n    edges = np.hstack([left, right[-1]])\n    return edges\n</code></pre>"},{"location":"api/#chromatic.imports.name2color","title":"<code>name2color(name)</code>","text":"<p>Return the 3-element RGB array of a given color name.</p>"},{"location":"api/#chromatic.imports.name2color--parameters","title":"Parameters","text":"<p>name : str     The name of a color</p>"},{"location":"api/#chromatic.imports.name2color--returns","title":"Returns","text":"<p>rgb : tuple     3-element RGB color, with numbers from 0.0 to 1.0</p> Source code in <code>chromatic/imports.py</code> <pre><code>def name2color(name):\n    \"\"\"\n    Return the 3-element RGB array of a given color name.\n\n    Parameters\n    ----------\n    name : str\n        The name of a color\n\n    Returns\n    -------\n    rgb : tuple\n        3-element RGB color, with numbers from 0.0 to 1.0\n    \"\"\"\n\n    # give a friendly warning if the color name can't be found\n    try:\n        color_hex = col.cnames[name]\n        return col.hex2color(color_hex)\n    except KeyError:\n        cheerfully_suggest(f\"The color {name} can't be found. (Returning black.)\")\n        return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/#chromatic.imports.one2another","title":"<code>one2another(bottom='white', top='red', alpha_bottom=1.0, alpha_top=1.0, N=256)</code>","text":"<p>Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".</p>"},{"location":"api/#chromatic.imports.one2another--parameters","title":"Parameters","text":"<p>bottom : str     Name of a color for the bottom of cmap (0.0) top : str     Name of a color for the top of the cmap (1.0) alpha_bottom : float     Opacity at the bottom of the cmap alpha_top : float     Opacitiy at the top of the cmap N : int     The number of levels in the listed color map</p>"},{"location":"api/#chromatic.imports.one2another--returns","title":"Returns","text":"<p>cmap : Colormap     A color map that goes linearly from the     bottom to top color (and alpha).</p> Source code in <code>chromatic/imports.py</code> <pre><code>def one2another(bottom=\"white\", top=\"red\", alpha_bottom=1.0, alpha_top=1.0, N=256):\n    \"\"\"\n    Create a cmap that goes smoothly (linearly in RGBA) from \"bottom\" to \"top\".\n\n    Parameters\n    ----------\n    bottom : str\n        Name of a color for the bottom of cmap (0.0)\n    top : str\n        Name of a color for the top of the cmap (1.0)\n    alpha_bottom : float\n        Opacity at the bottom of the cmap\n    alpha_top : float\n        Opacitiy at the top of the cmap\n    N : int\n        The number of levels in the listed color map\n\n    Returns\n    -------\n    cmap : Colormap\n        A color map that goes linearly from the\n        bottom to top color (and alpha).\n    \"\"\"\n\n    # get the RGB values of the bottom and top of the cmap\n    rgb_bottom, rgb_top = name2color(bottom), name2color(top)\n\n    # create linear gradients for all four RGBA channels\n    r = np.linspace(rgb_bottom[0], rgb_top[0], N)\n    g = np.linspace(rgb_bottom[1], rgb_top[1], N)\n    b = np.linspace(rgb_bottom[2], rgb_top[2], N)\n    a = np.linspace(alpha_bottom, alpha_top, N)\n\n    # create (N,4) array + populate a listed colormap\n    colors = np.transpose(np.vstack([r, g, b, a]))\n    cmap = col.ListedColormap(colors, name=\"{bottom}2{top}\".format(**locals()))\n\n    # return the colormap\n    return cmap\n</code></pre>"},{"location":"api/#chromatic.imports.remove_unit","title":"<code>remove_unit(x)</code>","text":"<p>Quick wrapper to remove the unit from a quantity, but not complain if it doesn't have one.</p> Source code in <code>chromatic/imports.py</code> <pre><code>def remove_unit(x):\n    \"\"\"\n    Quick wrapper to remove the unit from a quantity,\n    but not complain if it doesn't have one.\n    \"\"\"\n    try:\n        return x.value\n    except AttributeError:\n        return x\n</code></pre>"},{"location":"api/#chromatic.imports.resample_while_conserving_flux","title":"<code>resample_while_conserving_flux(xin=None, yin=None, xout=None, xin_edges=None, xout_edges=None, replace_nans=0.0, visualize=False, pause=False)</code>","text":"<p>Starting from some initial x and y, resample onto a different grid (either higher or lower resolution), while conserving total flux.</p> <p>When including the entire range of <code>xin</code>, <code>sum(yout) == sum(yin)</code> should be true.</p> <p>When including only part of the range of <code>xin</code>, the integral between any two points should be conserved.</p>"},{"location":"api/#chromatic.imports.resample_while_conserving_flux--parameters","title":"Parameters","text":"<p>xin : array     The original independent variable. yin : array     The original dependent variable (same size as x). xout : array     The new grid of independent variables onto which     you want to resample the y values. Refers to the     center of each bin (use <code>xout_edges</code> for finer     control over the exact edges of the bins) xin_edges : array     The edges of the original independent variable bins.     The left and right edges of the bins are interpreted     to be <code>xin_edges[:-1]</code> and <code>xin_edges[1:]</code>,     respectively, so the associated <code>yin</code> should have exactly     1 fewer element than <code>xin_edges</code>. This provides finer     control over the size of each bin in the input than     simply supplying <code>xin</code>(still a little experimental)     They should probably be sorted? xout_edges : array     The edges of the new grid of bins for the independent     variable, onto which you want to resample the y     values. The left and right edges of the bins will be,     respectively, <code>xout_edges[:-1]</code> and <code>xout_edges[1:]</code>,     so the size of the output array will be     <code>len(xout_edges) - 1</code> replace_nans : float, str     Replace nan values with this value.     <code>replace_nans = 0</code>         will add no flux where nans are     <code>replace_nans = nan</code>         will ensure you get nans returned everywhere         if you try to resample over any nan     <code>replace_nans = 'interpolate'</code>         will try to replace nans by linearly interpolating         from nearby values (not yet implemented) visualize : bool     Should we make a plot showing whether it worked? pause : bool     Should we pause to wait for a key press?</p>"},{"location":"api/#chromatic.imports.resample_while_conserving_flux--returns","title":"Returns","text":"<p>result : dict     A dictionary containing...         <code>x</code> = the center of the output grid         <code>y</code> = the resampled value on the output grid         <code>edges</code> = the edges of the output grid, which will             have one more element than x or y</p> Source code in <code>chromatic/tools/resampling.py</code> <pre><code>def resample_while_conserving_flux(\n    xin=None,\n    yin=None,\n    xout=None,\n    xin_edges=None,\n    xout_edges=None,\n    replace_nans=0.0,\n    visualize=False,\n    pause=False,\n):\n    \"\"\"\n    Starting from some initial x and y, resample onto a\n    different grid (either higher or lower resolution),\n    while conserving total flux.\n\n    When including the entire range of `xin`,\n    `sum(yout) == sum(yin)` should be true.\n\n    When including only part of the range of `xin`,\n    the integral between any two points should be conserved.\n\n    Parameters\n    ----------\n    xin : array\n        The original independent variable.\n    yin : array\n        The original dependent variable (same size as x).\n    xout : array\n        The new grid of independent variables onto which\n        you want to resample the y values. Refers to the\n        center of each bin (use `xout_edges` for finer\n        control over the exact edges of the bins)\n    xin_edges : array\n        The edges of the original independent variable bins.\n        The left and right edges of the bins are interpreted\n        to be `xin_edges[:-1]` and `xin_edges[1:]`,\n        respectively, so the associated `yin` should have exactly\n        1 fewer element than `xin_edges`. This provides finer\n        control over the size of each bin in the input than\n        simply supplying `xin`(still a little experimental)\n        They should probably be sorted?\n    xout_edges : array\n        The edges of the new grid of bins for the independent\n        variable, onto which you want to resample the y\n        values. The left and right edges of the bins will be,\n        respectively, `xout_edges[:-1]` and `xout_edges[1:]`,\n        so the size of the output array will be\n        `len(xout_edges) - 1`\n    replace_nans : float, str\n        Replace nan values with this value.\n        `replace_nans = 0`\n            will add no flux where nans are\n        `replace_nans = nan`\n            will ensure you get nans returned everywhere\n            if you try to resample over any nan\n        `replace_nans = 'interpolate'`\n            will try to replace nans by linearly interpolating\n            from nearby values (not yet implemented)\n    visualize : bool\n        Should we make a plot showing whether it worked?\n    pause : bool\n        Should we pause to wait for a key press?\n\n    Returns\n    -------\n    result : dict\n        A dictionary containing...\n            `x` = the center of the output grid\n            `y` = the resampled value on the output grid\n            `edges` = the edges of the output grid, which will\n                have one more element than x or y\n    \"\"\"\n\n    # make sure there are some reasonable input options\n    assert (xin is not None) or (xin_edges is not None)\n    assert yin is not None\n    assert (xout is not None) or (xout_edges is not None)\n\n    # set up the bins, to calculate cumulative distribution of y\n    if xin_edges is None:\n        # make sure the sizes match up\n        assert len(xin) == len(yin)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin)\n        xin_sorted = xin[s]\n        yin_sorted = yin[s]\n        # estimate some bin edges (might fail for non-uniform grids)\n        xin_left, xin_right = calculate_bin_leftright(xin_sorted)\n        # define an array of edges\n        xin_edges = leftright_to_edges(xin_left, xin_right)\n    else:\n        # make sure the sizes match up\n        assert len(xin_edges) == (len(yin) + 1)\n        # sort to make sure x is strictly increasing\n        s = np.argsort(xin_edges)\n        xin_left, xin_right = edges_to_leftright(xin_edges[s])\n        xin_sorted = (xin_left + xin_right) / 2\n        yin_sorted = yin[s[:-1]]\n\n    # the first element should be the left edge of the first pixel\n    # last element will be right edge of last pixel\n    xin_for_cdf = xin_edges\n\n    # to the left of the first pixel, assume flux is zero\n    yin_for_cdf = np.hstack([0, yin_sorted])\n\n    # correct for any non-finite values\n    bad = np.isnan(yin_for_cdf)\n    if replace_nans == \"interpolate\":\n        raise NotImplementedError(\n            \"The `replace_nans='interpolate'`` option doens't exist yet!\"\n        )\n    yin_for_cdf[bad] = replace_nans\n\n    # calculate the CDF of the flux (at pixel edge locations)\n    cdfin = np.cumsum(yin_for_cdf)\n\n    # create an interpolator for that CDF\n    cdfinterpolator = interp1d(\n        xin_for_cdf,\n        cdfin,\n        kind=\"linear\",\n        bounds_error=False,\n        fill_value=(0.0, np.sum(yin)),\n    )\n\n    # calculate bin edges (of size len(xout)+1)\n    if xout_edges is None:\n        xout_left, xout_right = calculate_bin_leftright(xout)\n        xout_edges = leftright_to_edges(xout_left, xout_right)\n    else:\n        xout_left, xout_right = edges_to_leftright(xout_edges)\n        xout = (xout_left + xout_right) / 2\n\n    xout_for_cdf = leftright_to_edges(xout_left, xout_right)\n\n    # interpolate the CDF onto those bin edges\n    cdfout = cdfinterpolator(xout_for_cdf)\n\n    # take  derivative of the CDF to get flux per resampled bin\n    # (xout is bin center, and yout is the flux in that bin)\n    yout = np.diff(cdfout)\n\n    if visualize:\n        fi, (ax_cdf, ax_pdf) = plt.subplots(2, 1, sharex=True, dpi=300, figsize=(8, 8))\n        inkw = dict(\n            color=\"black\",\n            alpha=1,\n            linewidth=3,\n            marker=\".\",\n            markeredgecolor=\"none\",\n        )\n        outkw = dict(\n            color=\"darkorange\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n        )\n\n        legkw = dict(\n            frameon=False,\n            loc=\"upper left\",\n        )\n\n        xinbinsize = xin_right - xin_left\n        xoutbinsize = xout_right - xout_left\n        # plot the PDFs\n        plt.sca(ax_pdf)\n        plt.ylabel(\"Flux per (Original) Pixel\")\n        plt.xlabel(\"Pixel\")\n        # plot the original pixels (in df/dpixel to compare with resampled)\n        plot_as_boxes(\n            xin_sorted, yin_sorted / xinbinsize, label=\"Original Pixels\", **inkw\n        )\n\n        # what would a bad interpolation look like?\n        interpolate_badly = interp1d(\n            xin_sorted,\n            yin_sorted / xinbinsize,\n            kind=\"linear\",\n            bounds_error=False,\n            fill_value=0.0,\n        )\n        plt.plot(\n            xout,\n            interpolate_badly(xout),\n            color=\"cornflowerblue\",\n            alpha=1,\n            linewidth=1,\n            marker=\".\",\n            markersize=8,\n            markeredgecolor=\"none\",\n            label=\"Silly Simple Interpolation\",\n        )\n\n        # plot the flux-conserving resampled data (again, in df/d\"pixel\")\n        plt.plot(\n            xout, yout / xoutbinsize, label=\"Flux-Conserving Interpolation\", **outkw\n        )\n\n        plt.legend(**legkw)\n\n        # plot the CDFs\n        plt.sca(ax_cdf)\n        plt.ylabel(\"Cumulative Flux (from left)\")\n\n        # plot the original CDF\n        plt.plot(xin_for_cdf, cdfin, label=\"Original Pixels\", **inkw)\n\n        # plot the interpolated CDF\n        plt.plot(xout_for_cdf, cdfout, label=\"Flux-Conserved Resample\", **outkw)\n        if pause:\n            a = input(\n                \"Pausing a moment to check on interpolation; press return to continue.\"\n            )\n\n        print(\"{:&gt;6} = {:.5f}\".format(\"Actual\", np.sum(yin)))\n        print(\n            \"{:&gt;6} = {:.5f}\".format(\n                \"Silly\",\n                np.sum(interpolate_badly(xout) * xoutbinsize),\n            )\n        )\n        print(\"{:&gt;6} = {:.5f}\".format(\"CDF\", np.sum(yout)))\n\n    # return the resampled y-values\n    return {\"x\": xout, \"x_edge_lower\": xout_left, \"x_edge_upper\": xout_right, \"y\": yout}\n</code></pre>"},{"location":"basics/","title":"Basics of \ud83c\udf08 Objects","text":"In\u00a0[1]: Copied! <pre>from chromatic import SimulatedRainbow, version\n</pre> from chromatic import SimulatedRainbow, version In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> <p>To users on the outside, all <code>Rainbow</code> objects will be guaranteed to have a few key properties. We'll make a simple simulated example to show what those are.</p> In\u00a0[3]: Copied! <pre># create a simulated spectroscopic light curve\nr = SimulatedRainbow().inject_noise(signal_to_noise=100)\n</pre> # create a simulated spectroscopic light curve r = SimulatedRainbow().inject_noise(signal_to_noise=100) <p>The <code>.wavelength</code> property is a 1D array containing the wavelengths associated with the flux array. It is a an <code>astropy</code> Quantity, with units of wavelength associated with it.</p> In\u00a0[4]: Copied! <pre>r.wavelength\n</pre> r.wavelength Out[4]:  $[0.5,~0.50502508,~0.51010067,~\\dots,~4.8883402,~4.9374688,~4.9870912] \\; \\mathrm{\\mu m}$  In\u00a0[5]: Copied! <pre># access the 1D array of wavelengths\nprint(f\"The {r.nwave} wavelengths...\")\nprint(f\"  have a shape of {r.wavelength.shape},\")\nprint(f\"  a type of {type(r.wavelength)},\")\nprint(f\"  units of {r.wavelength.unit}, and\")\nprint(f\"  a dtype of {r.wavelength.dtype}\")\n</pre> # access the 1D array of wavelengths print(f\"The {r.nwave} wavelengths...\") print(f\"  have a shape of {r.wavelength.shape},\") print(f\"  a type of {type(r.wavelength)},\") print(f\"  units of {r.wavelength.unit}, and\") print(f\"  a dtype of {r.wavelength.dtype}\") <pre>The 231 wavelengths...\n  have a shape of (231,),\n  a type of &lt;class 'astropy.units.quantity.Quantity'&gt;,\n  units of micron, and\n  a dtype of float64\n</pre> <p>The <code>.time</code> property is a 1D array containing the time associated with the flux array. It is a an <code>astropy</code> Quantity, with units of time associated with it. (Watch out! At some point we may switch it over to being an actual astropy <code>Time</code> object.)</p> In\u00a0[6]: Copied! <pre>r.time\n</pre> r.time Out[6]:  $[-0.10416667,~-0.10277778,~-0.10138889,~\\dots,~0.1,~0.10138889,~0.10277778] \\; \\mathrm{d}$  In\u00a0[7]: Copied! <pre># access the 1D array of times\nprint(f\"The {r.ntime} times...\")\nprint(f\"  have a shape of {r.time.shape},\")\nprint(f\"  a type of {type(r.time)},\")\nprint(f\"  units of {r.time.unit}, and\")\nprint(f\"  a dtype of {r.time.dtype}\")\n</pre> # access the 1D array of times print(f\"The {r.ntime} times...\") print(f\"  have a shape of {r.time.shape},\") print(f\"  a type of {type(r.time)},\") print(f\"  units of {r.time.unit}, and\") print(f\"  a dtype of {r.time.dtype}\") <pre>The 150 times...\n  have a shape of (150,),\n  a type of &lt;class 'astropy.units.quantity.Quantity'&gt;,\n  units of d, and\n  a dtype of float64\n</pre> <p>The <code>.flux</code> property is a 2D array containing the flux associated with each combination of wavelength (row, axis 0) and time (column, axis 1). It can be an <code>astropy</code> Quantity with a variety of possible units ($\\mathrm{photons}$, $\\mathrm{W/m^2/nm}$, $\\mathrm{MJy/sr}$, ...), or it can be unitless and normalized to be close to 1.</p> In\u00a0[8]: Copied! <pre>r.flux\n</pre> r.flux Out[8]: <pre>array([[1.00306552, 0.99110549, 0.99971919, ..., 0.99830583, 0.9981376 ,\n        0.99390858],\n       [1.0187324 , 1.00164312, 0.98981124, ..., 1.008361  , 0.97760781,\n        1.00318572],\n       [1.00871933, 0.99497394, 0.98865849, ..., 0.99283686, 1.00702916,\n        0.9996527 ],\n       ...,\n       [0.99624463, 1.0035061 , 1.00884283, ..., 0.99858548, 1.00658332,\n        1.01503383],\n       [0.99742255, 1.00313738, 0.99398074, ..., 0.99933397, 0.99541488,\n        1.00074317],\n       [1.00430299, 1.01183048, 0.98790279, ..., 0.99245159, 0.99891887,\n        1.0071964 ]], shape=(231, 150))</pre> In\u00a0[9]: Copied! <pre># access the 2D array of fluxes\nprint(f\"The {r.nflux} fluxes...\")\nprint(f\"  have a shape of {r.flux.shape},\")\nprint(f\"  a type of {type(r.flux)},\")\nprint(f\"  a dtype of {r.flux.dtype}\")\n</pre> # access the 2D array of fluxes print(f\"The {r.nflux} fluxes...\") print(f\"  have a shape of {r.flux.shape},\") print(f\"  a type of {type(r.flux)},\") print(f\"  a dtype of {r.flux.dtype}\") <pre>The 34650 fluxes...\n  have a shape of (231, 150),\n  a type of &lt;class 'numpy.ndarray'&gt;,\n  a dtype of float64\n</pre> <p>The <code>.uncertainty</code> property is a 2D array containing the uncertainty associated with each flux point. It should have the same units and scale as <code>flux</code>, whatever those are.</p> In\u00a0[10]: Copied! <pre>r.uncertainty\n</pre> r.uncertainty Out[10]: <pre>array([[0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n       ...,\n       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01],\n       [0.01, 0.01, 0.01, ..., 0.01, 0.01, 0.01]], shape=(231, 150))</pre> In\u00a0[11]: Copied! <pre># access the 2D array of times\nprint(f\"The {r.nflux} uncertainties...\")\nprint(f\"  have a shape of {r.uncertainty.shape},\")\nprint(f\"  a type of {type(r.uncertainty)},\")\nprint(f\"  a dtype of {r.uncertainty.dtype}\")\n</pre> # access the 2D array of times print(f\"The {r.nflux} uncertainties...\") print(f\"  have a shape of {r.uncertainty.shape},\") print(f\"  a type of {type(r.uncertainty)},\") print(f\"  a dtype of {r.uncertainty.dtype}\") <pre>The 34650 uncertainties...\n  have a shape of (231, 150),\n  a type of &lt;class 'numpy.ndarray'&gt;,\n  a dtype of float64\n</pre> <p>The <code>.ok</code> property is a 2D array indicating whether a particular flux data point is good (<code>True</code>) or bad (<code>False</code>). It's a place to keep track of what data should be ignored when fitting or visualizing.</p> In\u00a0[12]: Copied! <pre>r.ok\n</pre> r.ok Out[12]: <pre>array([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]], shape=(231, 150))</pre> In\u00a0[13]: Copied! <pre># access the 2D array of times\nprint(f\"The {r.nflux} `ok` mask values...\")\nprint(f\"  have a shape of {r.ok.shape},\")\nprint(f\"  a type of {type(r.ok)},\")\nprint(f\"  a dtype of {r.ok.dtype}\")\n</pre> # access the 2D array of times print(f\"The {r.nflux} `ok` mask values...\") print(f\"  have a shape of {r.ok.shape},\") print(f\"  a type of {type(r.ok)},\") print(f\"  a dtype of {r.ok.dtype}\") <pre>The 34650 `ok` mask values...\n  have a shape of (231, 150),\n  a type of &lt;class 'numpy.ndarray'&gt;,\n  a dtype of bool\n</pre> <p>Finally, there is a suggest optional <code>.model</code> property that contains a 2D array indicating the model values associated with each point. <code>Rainbow</code> objects should still work fine with no <code>model</code> defined, but having one present expands options for visualization and calculation. In our simulation, the model is simply 1 everywhere.</p> In\u00a0[14]: Copied! <pre>r.model\n</pre> r.model Out[14]: <pre>array([[1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       ...,\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.],\n       [1., 1., 1., ..., 1., 1., 1.]], shape=(231, 150))</pre> In\u00a0[15]: Copied! <pre># access the 2D array of fluxes\nprint(f\"The {r.nflux} model values...\")\nprint(f\"  have a shape of {r.model.shape},\")\nprint(f\"  a type of {type(r.model)},\")\nprint(f\"  a dtype of {r.model.dtype}\")\n</pre> # access the 2D array of fluxes print(f\"The {r.nflux} model values...\") print(f\"  have a shape of {r.model.shape},\") print(f\"  a type of {type(r.model)},\") print(f\"  a dtype of {r.model.dtype}\") <pre>The 34650 model values...\n  have a shape of (231, 150),\n  a type of &lt;class 'numpy.ndarray'&gt;,\n  a dtype of float64\n</pre> <p>With these 5-6 six basic components (<code>time</code>, <code>wavelength</code>, <code>flux</code>, <code>uncertainty</code>, <code>ok</code>, and maybe <code>model</code>), we can build up some delightfully complicated calculations and visualizations for all \ud83c\udf08 objects.</p>"},{"location":"basics/#basics-of-objects","title":"Basics of \ud83c\udf08 Objects\u00b6","text":"<p>The core of the <code>chromatic</code> package is the <code>Rainbow</code> (= \ud83c\udf08) object. Continually saying or typing \u201cspectroscopic light curve\u201d can get tiring, so we chose \"rainbow\" as a shorter name that is a little nicer to say/type. Also, every emphemeral \ud83c\udf08 in nature is itself an expression of brightness as a function of wavelength and of time, so it hopefully kind of makes sense as a name?</p>"},{"location":"basics/#how-do-we-create-a","title":"How do we create a \ud83c\udf08?\u00b6","text":"<p>We can create <code>Rainbow</code> objects in a few different ways.</p> <ul> <li>To load a \ud83c\udf08 in from a file, see Reading/Writing a \ud83c\udf08. We've invested a lot of effort in providing readers/writers in a variety of formats, to allow easy interaction between analyses.</li> <li>To create a \ud83c\udf08 from arrays, see Creating a \ud83c\udf08  from Arrays. As long as you provide 1D arrays of <code>time</code> and <code>wavelength</code> and 2D arrays for <code>flux</code> and <code>uncertainty</code> (and maybe <code>ok</code> and <code>model</code>), you can make a \ud83c\udf08.</li> <li>To generate a simulated \ud83c\udf08, see \ud83c\udf08 Actions and use the tools shown there to inject in the compomnents you want into a <code>SimulatedRainbow()</code> object.</li> </ul>"},{"location":"basics/#what-variables-does-a-have","title":"What variables does a \ud83c\udf08 have?\u00b6","text":""},{"location":"creating/","title":"Creating a \ud83c\udf08 from Arrays","text":"In\u00a0[1]: Copied! <pre>from chromatic import Rainbow, RainbowWithModel, SimulatedRainbow, version\nfrom chromatic import np, plt, u\n</pre> from chromatic import Rainbow, RainbowWithModel, SimulatedRainbow, version from chromatic import np, plt, u In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> In\u00a0[3]: Copied! <pre>N_wavelengths, N_times = 13, 17\nr = RainbowWithModel(\n    wavelength=np.linspace(1, 2, N_wavelengths) * u.micron,\n    time=np.linspace(-0.1, 0.1, N_times) * u.day,\n    flux=np.random.normal(1, 0.01, [N_wavelengths, N_times]),\n    uncertainty=np.ones([N_wavelengths, N_times]) * 0.01,\n    ok=np.random.uniform(0, 1, [N_wavelengths, N_times]) &gt; 0.1,\n    model=np.ones([N_wavelengths, N_times]),\n)\n</pre> N_wavelengths, N_times = 13, 17 r = RainbowWithModel(     wavelength=np.linspace(1, 2, N_wavelengths) * u.micron,     time=np.linspace(-0.1, 0.1, N_times) * u.day,     flux=np.random.normal(1, 0.01, [N_wavelengths, N_times]),     uncertainty=np.ones([N_wavelengths, N_times]) * 0.01,     ok=np.random.uniform(0, 1, [N_wavelengths, N_times]) &gt; 0.1,     model=np.ones([N_wavelengths, N_times]), ) <p>That's it! We've created a new \ud83c\udf08 object just by supplying a few arrays. To make sure it worked, let's make a plot.</p> In\u00a0[4]: Copied! <pre>r.plot_with_model();\n</pre> r.plot_with_model(); <p>First, let's create an array of wavelengths. We'll use astropy units to specify that the units of wavelength are in micron. Setting the units explicitly helps save us from confusion and ruin later on!</p> In\u00a0[5]: Copied! <pre>N_wavelengths = 7\nmy_neat_wavelengths = np.linspace(0.5, 5, N_wavelengths) * u.micron\n</pre> N_wavelengths = 7 my_neat_wavelengths = np.linspace(0.5, 5, N_wavelengths) * u.micron In\u00a0[6]: Copied! <pre>my_neat_wavelengths\n</pre> my_neat_wavelengths Out[6]:  $[0.5,~1.25,~2,~2.75,~3.5,~4.25,~5] \\; \\mathrm{\\mu m}$  <p>Next, let's create some times. Again, we'll give them units of time.</p> In\u00a0[7]: Copied! <pre>N_times = 11\nmy_swell_times = np.linspace(-0.1, 0.1, N_times) * u.day\n</pre> N_times = 11 my_swell_times = np.linspace(-0.1, 0.1, N_times) * u.day In\u00a0[8]: Copied! <pre>my_swell_times\n</pre> my_swell_times Out[8]:  $[-0.1,~-0.08,~-0.06,~-0.04,~-0.02,~0,~0.02,~0.04,~0.06,~0.08,~0.1] \\; \\mathrm{d}$  <p>And finally, let's make some fluxes associated with each of these wavelengths and times. In general, you'll want to assemble this array of fluxes out of a series of spectra or a group of light curves, but for this example the flux will just be totally random. The first dimension (row) of this array should correspond to wavelength, and the second (column) to time.</p> In\u00a0[9]: Copied! <pre>my_great_fluxes = np.random.normal(1, 0.01, size=(N_wavelengths, N_times))\n</pre> my_great_fluxes = np.random.normal(1, 0.01, size=(N_wavelengths, N_times)) <p>With those arrays, we can create a \ud83c\udf08 by feeding them in as keywords to <code>Rainbow</code>:</p> In\u00a0[10]: Copied! <pre>r = Rainbow(wavelength=my_neat_wavelengths, time=my_swell_times, flux=my_great_fluxes)\n</pre> r = Rainbow(wavelength=my_neat_wavelengths, time=my_swell_times, flux=my_great_fluxes) <p>Ta-da! Now those wavelengths, times, and fluxes have been connected into one \ud83c\udf08!</p> In\u00a0[11]: Copied! <pre>r\n</pre> r Out[11]: <pre>&lt;\ud83c\udf08(7w, 11t)&gt;</pre> In\u00a0[12]: Copied! <pre>r.paint();\n</pre> r.paint(); <p>We'll use the same wavelength and time grids as before, but let's define some uncertainties and fluxes together.</p> In\u00a0[13]: Copied! <pre>my_cool_uncertainties = np.ones((N_wavelengths, N_times))\nmy_cool_uncertainties *= np.linspace(0.01, 0.05, N_wavelengths)[:, np.newaxis]\n</pre> my_cool_uncertainties = np.ones((N_wavelengths, N_times)) my_cool_uncertainties *= np.linspace(0.01, 0.05, N_wavelengths)[:, np.newaxis] In\u00a0[14]: Copied! <pre>my_cool_fluxes = np.random.normal(1, my_cool_uncertainties)\n</pre> my_cool_fluxes = np.random.normal(1, my_cool_uncertainties) <p>To include the uncertainty values, just add an <code>uncertainty</code> keyword:</p> In\u00a0[15]: Copied! <pre>r = Rainbow(\n    wavelength=my_neat_wavelengths,\n    time=my_swell_times,\n    flux=my_cool_fluxes,\n    uncertainty=my_cool_uncertainties,\n)\n</pre> r = Rainbow(     wavelength=my_neat_wavelengths,     time=my_swell_times,     flux=my_cool_fluxes,     uncertainty=my_cool_uncertainties, ) <p>Huzzah! Now there's a rainbow that has an uncertainty associated with each flux. These uncertainties will be helpful if you want to compare your data to models, or if for downweighting more uncertain points when binning together in time or wavelength.</p> In\u00a0[16]: Copied! <pre>fi, ax = plt.subplots(1, 2, figsize=(10, 3))\nr.paint(quantity=\"uncertainty\", ax=ax[0])\nr.paint(ax=ax[1]);\n</pre> fi, ax = plt.subplots(1, 2, figsize=(10, 3)) r.paint(quantity=\"uncertainty\", ax=ax[0]) r.paint(ax=ax[1]); <p>Imagine you have a time series of centroid positions (one for each time), or perhaps you recorded the background flux that was subtracted during spectral extraction (one for each wavelength and time), or you have other quantities that would be useful to keep connected to your time-series spectra. Let's make some of these, as examples:</p> In\u00a0[17]: Copied! <pre>my_wobbly_centroids = np.random.normal(5, 0.02, N_times) * u.pixel\nmy_messy_backgrounds = (\n    np.random.normal(10, 0.1, (N_wavelengths, N_times)) * u.photon / u.s\n)\nmy_stellar_spectrum = np.random.uniform(2, 3, N_wavelengths) * u.W / u.m**2\n</pre> my_wobbly_centroids = np.random.normal(5, 0.02, N_times) * u.pixel my_messy_backgrounds = (     np.random.normal(10, 0.1, (N_wavelengths, N_times)) * u.photon / u.s ) my_stellar_spectrum = np.random.uniform(2, 3, N_wavelengths) * u.W / u.m**2 <p>You can populate additional arrays inside a rainbow by providing them as additional keyword arguments. Any names are allowed, except for a few protected keywords (<code>filepath</code>, <code>format</code>, <code>wavelike</code>, <code>timelike</code>, <code>fluxlike</code>, <code>metadata</code>).</p> In\u00a0[18]: Copied! <pre>r = Rainbow(\n    wavelength=my_neat_wavelengths,\n    time=my_swell_times,\n    flux=my_cool_fluxes,\n    uncertainty=my_cool_uncertainties,\n    centroid=my_wobbly_centroids,\n    background=my_messy_backgrounds,\n    stellar_spectrum=my_stellar_spectrum,\n)\n</pre> r = Rainbow(     wavelength=my_neat_wavelengths,     time=my_swell_times,     flux=my_cool_fluxes,     uncertainty=my_cool_uncertainties,     centroid=my_wobbly_centroids,     background=my_messy_backgrounds,     stellar_spectrum=my_stellar_spectrum, ) <p>Arrays will be sorted into <code>wavelike</code>, <code>timelike</code>, and <code>fluxlike</code> dictionaries based on their shape.</p> In\u00a0[19]: Copied! <pre>r.wavelike\n</pre> r.wavelike Out[19]: <pre>{'wavelength': &lt;Quantity [0.5 , 1.25, 2.  , 2.75, 3.5 , 4.25, 5.  ] micron&gt;,\n 'stellar_spectrum': &lt;Quantity [2.68787903, 2.9861121 , 2.33055303, 2.66172938, 2.72250446,\n            2.22785687, 2.91293746] W / m2&gt;,\n 'original_wave_index': array([0, 1, 2, 3, 4, 5, 6])}</pre> In\u00a0[20]: Copied! <pre>r.timelike\n</pre> r.timelike Out[20]: <pre>{'time': &lt;Quantity [-0.1 , -0.08, -0.06, -0.04, -0.02,  0.  ,  0.02,  0.04,  0.06,\n             0.08,  0.1 ] d&gt;,\n 'centroid': &lt;Quantity [5.00387048, 5.00680101, 5.01022732, 4.98063709, 4.99744738,\n            5.04174749, 4.99392361, 4.9864031 , 5.02669244, 5.0184722 ,\n            4.99635052] pix&gt;,\n 'original_time_index': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])}</pre> In\u00a0[21]: Copied! <pre>r.fluxlike\n</pre> r.fluxlike Out[21]: <pre>{'flux': array([[1.01020822, 0.99386146, 1.00115644, 1.00575066, 0.99880139,\n         0.99955156, 0.99579782, 1.00318728, 1.00892393, 1.02077407,\n         0.98873937],\n        [1.01182772, 0.98101469, 0.99886028, 0.98400078, 1.02714422,\n         1.01733158, 1.00605123, 1.0003978 , 0.98961182, 0.97561804,\n         1.00815119],\n        [1.02433601, 1.02110207, 1.00275341, 0.97969887, 0.99775499,\n         0.97411611, 1.00744283, 1.01532328, 1.0153852 , 1.00112604,\n         1.01474963],\n        [1.07659638, 0.99553728, 1.06216382, 0.99610598, 1.0263184 ,\n         1.0343441 , 0.99193166, 1.03716325, 1.00272716, 1.00358906,\n         0.93368362],\n        [0.9730424 , 0.98979906, 1.01845529, 1.03342945, 1.10085678,\n         0.98676317, 1.03441737, 0.98264844, 1.01436845, 1.02311503,\n         1.06023541],\n        [0.95478962, 1.03392778, 0.99636974, 0.99398537, 1.03018286,\n         1.05019301, 1.01523944, 0.98982075, 1.02134049, 0.98051122,\n         1.06383166],\n        [0.9599934 , 1.08084868, 1.04402361, 0.95129916, 0.97442209,\n         0.91779995, 0.90788352, 0.90952354, 0.99711581, 1.01623735,\n         0.91246378]]),\n 'uncertainty': array([[0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n         0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n         0.01      ],\n        [0.01666667, 0.01666667, 0.01666667, 0.01666667, 0.01666667,\n         0.01666667, 0.01666667, 0.01666667, 0.01666667, 0.01666667,\n         0.01666667],\n        [0.02333333, 0.02333333, 0.02333333, 0.02333333, 0.02333333,\n         0.02333333, 0.02333333, 0.02333333, 0.02333333, 0.02333333,\n         0.02333333],\n        [0.03      , 0.03      , 0.03      , 0.03      , 0.03      ,\n         0.03      , 0.03      , 0.03      , 0.03      , 0.03      ,\n         0.03      ],\n        [0.03666667, 0.03666667, 0.03666667, 0.03666667, 0.03666667,\n         0.03666667, 0.03666667, 0.03666667, 0.03666667, 0.03666667,\n         0.03666667],\n        [0.04333333, 0.04333333, 0.04333333, 0.04333333, 0.04333333,\n         0.04333333, 0.04333333, 0.04333333, 0.04333333, 0.04333333,\n         0.04333333],\n        [0.05      , 0.05      , 0.05      , 0.05      , 0.05      ,\n         0.05      , 0.05      , 0.05      , 0.05      , 0.05      ,\n         0.05      ]]),\n 'background': &lt;Quantity [[10.03565344,  9.99759549,  9.96458101,  9.95117376,\n             10.03411903, 10.11258019,  9.892919  , 10.08948257,\n              9.95346572, 10.06128729,  9.97247096],\n            [10.06981906,  9.93325211,  9.89220306, 10.00012239,\n             10.0276249 , 10.03752262, 10.09114004,  9.98635752,\n             10.16683955,  9.98143175,  9.98139883],\n            [ 9.80958362, 10.05571226,  9.95992075, 10.03650276,\n              9.97951386,  9.91435346, 10.07299095,  9.98848591,\n              9.98306572,  9.89910326,  9.9348941 ],\n            [10.03703066, 10.03559497, 10.1029053 , 10.13712616,\n             10.02810479,  9.9280569 , 10.05208044,  9.9974178 ,\n              9.97491009,  9.96357686,  9.96024983],\n            [ 9.9991671 , 10.01023929, 10.02480152, 10.07750667,\n             10.09979032,  9.95740713,  9.83472515, 10.18854637,\n              9.86964368,  9.76457272, 10.03366578],\n            [ 9.98012268, 10.03214447,  9.96959754,  9.8968178 ,\n              9.79197039,  9.89606796, 10.0862695 ,  9.97259719,\n              9.8003119 , 10.22187683, 10.0466687 ],\n            [10.23133956,  9.95825094, 10.09226366,  9.98744719,\n              9.9304289 ,  9.87473107,  9.92872297,  9.94421173,\n              9.9106291 ,  9.95240276,  9.97918513]] ph / s&gt;}</pre> <p>Let's also attach a model to our <code>Rainbow</code> to unlike some snazzy model plotting features. The <code>.attach_model()</code> action can be used to do this.</p> In\u00a0[22]: Copied! <pre>m = r.attach_model(model=np.ones_like(my_cool_fluxes))\n</pre> m = r.attach_model(model=np.ones_like(my_cool_fluxes)) In\u00a0[23]: Copied! <pre>type(r)\n</pre> type(r) Out[23]: <pre>chromatic.rainbows.rainbow.Rainbow</pre> In\u00a0[24]: Copied! <pre>type(m)\n</pre> type(m) Out[24]: <pre>chromatic.rainbows.withmodel.RainbowWithModel</pre> <p>The <code>.attach_model()</code> action generates a new type of object, the <code>RainbowWithModel</code>. These objects have more abilities that require comparing data to a model. Below, we can call <code>m.plot_with_model()</code> but we wouldn't be able to call <code>r.plot_with_model()</code> because <code>r</code> doesn't have any model associated with it.</p> In\u00a0[25]: Copied! <pre>m.plot_with_model();\n</pre> m.plot_with_model(); <p>You can also add arrays directly to a core dictionary by (a) providing a key and an array with the right shape or (b) setting an attribute with an array that would fit in one of the <code>timelike</code>, <code>wavelike</code>, or <code>fluxlike</code> dictionaries. The latter option will try to guess where an array belongs based on its shape, which should mostly work. The following two methods should be identical:</p> In\u00a0[26]: Copied! <pre>my_imaginary_temperatures = np.random.normal(77, 0.3, N_times)\nr.timelike[\"detector_temperature\"] = my_imaginary_temperatures\nr.detector_temperature = my_imaginary_temperatures\n</pre> my_imaginary_temperatures = np.random.normal(77, 0.3, N_times) r.timelike[\"detector_temperature\"] = my_imaginary_temperatures r.detector_temperature = my_imaginary_temperatures In\u00a0[27]: Copied! <pre>s = SimulatedRainbow()\n</pre> s = SimulatedRainbow() In\u00a0[28]: Copied! <pre>s.author = \"Zach Berta-Thompson\"\ns.warning = \"Watch out! These data are entirely imaginary!\"\n</pre> s.author = \"Zach Berta-Thompson\" s.warning = \"Watch out! These data are entirely imaginary!\" <p>You can also edit the <code>.metadata</code> dictionary directly.</p> In\u00a0[29]: Copied! <pre>s.metadata[\"and another thing\"] = \"Be kind!\"\n</pre> s.metadata[\"and another thing\"] = \"Be kind!\" <p>When we look at the metadata, you'll notice there are a already few other entries that have been automatically populated. If at all possible, it's probably best to try to avoid overwriting those.</p> In\u00a0[30]: Copied! <pre>s.metadata\n</pre> s.metadata Out[30]: <pre>{'name': None,\n 'history': [\"SimulatedRainbow(\\n   tlim=u.Quantity(np.array([-2.5,  2.5]))*u.Unit('h'),\\n   dt=u.Quantity(np.float64(2.0))*u.Unit('min'),\\n   wlim=u.Quantity(np.array([0.5, 5. ]))*u.Unit('micron'),\\n   R=100)\"],\n 'R': 100,\n 'wscale': 'log',\n 'tscale': 'linear',\n 'author': 'Zach Berta-Thompson',\n 'warning': 'Watch out! These data are entirely imaginary!',\n 'and another thing': 'Be kind!'}</pre> <p>Wahoo! You've done it! Now you can create a \ud83c\udf08 from whatever arrays and/or data you have available!</p>"},{"location":"creating/#creating-a-from-arrays","title":"Creating a \ud83c\udf08  from Arrays\u00b6","text":"<p>You can create a <code>Rainbow</code> object from arrays representing wavelength, time, flux, and any other array quantities that have the same shape as one of those first three (see Basics of \ud83c\udf08 Objects). Here, we show how to construct \ud83c\udf08s from arrays by creating some (very cartoonish) simulated datasets of time-series spectra.</p>"},{"location":"creating/#shortest-example","title":"Shortest Example\u00b6","text":"<p>This example shows, in the fewest code lines possible, how to create a \ud83c\udf08 by supplying your own custom arrays. It doesn't explain things very carefully though, so please read on to the other examples for a friendlier introduction. In this example, we populate a \ud83c\udf08 with both data and model, but if you don't plan to use the any model comparison features you can simply skip the <code>model=</code> keyword.</p>"},{"location":"creating/#simplest-example","title":"Simplest Example\u00b6","text":"<p>For the simplest example, we'll make a \ud83c\udf08 out of <code>wavelength</code>, <code>time</code>, and <code>flux</code>.</p>"},{"location":"creating/#slightly-more-complicated-example","title":"Slightly More Complicated Example\u00b6","text":"<p>For a tiny bit more complexity, let's also add uncertainties when defining our \ud83c\udf08.</p>"},{"location":"creating/#most-comprehensive-example","title":"Most Comprehensive Example\u00b6","text":"<p>For completeness, let's also add some more quantities that align with either the wavelengths, the times, or the fluxes.</p>"},{"location":"creating/#metadata-example","title":"Metadata Example\u00b6","text":"<p>Except for some protected words (<code>wavelength</code>, <code>time</code>, <code>flux</code>, <code>uncertainty</code>, <code>ok</code>, <code>model</code>, other class method names, and a few others), any other attributes you set for a rainbow object will be stored in the <code>.metadata</code> core dictionary, so they can be saved and shared. This can be a nice way to document important human-readable information that's useful for interpretting the data.</p>"},{"location":"designing/","title":"Designing New \ud83c\udf08 Features","text":"In\u00a0[1]: Copied! <pre>from chromatic import SimulatedRainbow\n</pre> from chromatic import SimulatedRainbow In\u00a0[2]: Copied! <pre>r = SimulatedRainbow().inject_noise()\n</pre> r = SimulatedRainbow().inject_noise() In\u00a0[3]: Copied! <pre>r.wavelike.keys()\n</pre> r.wavelike.keys() Out[3]: <pre>dict_keys(['wavelength', 'original_wave_index'])</pre> In\u00a0[4]: Copied! <pre>r.timelike.keys()\n</pre> r.timelike.keys() Out[4]: <pre>dict_keys(['time', 'original_time_index'])</pre> In\u00a0[5]: Copied! <pre>r.fluxlike.keys()\n</pre> r.fluxlike.keys() Out[5]: <pre>dict_keys(['flux', 'model', 'uncertainty'])</pre> In\u00a0[6]: Copied! <pre>r.metadata.keys()\n</pre> r.metadata.keys() Out[6]: <pre>dict_keys(['name', 'history', 'R', 'wscale', 'tscale', 'signal_to_noise'])</pre> <p>When you retrieve variables with something like <code>.wavelength</code>, <code>.time</code>, <code>.flux</code>, data is being pulled directly from these dictionaries.</p> <p>We defined a small lexicon of things that <code>Rainbow</code> objects can do. If you want to write a new feature, hopefully it fits into one of these categories. If not, we can certainly discuss adding new ones!</p> <ul> <li>actions return a new <code>Rainbow</code> object as the result. As such, they can be chained together into commands like <code>r.bin(R=5).normalize().plot()</code>.</li> <li>wavelike return a wavelike-shaped array from a <code>Rainbow</code>, with one quantity for each wavelength.</li> <li>timelike return a timelike-shaped array from a <code>Rainbow</code>, with one quantity for each time.</li> <li>visualizations create some graphical representation of the data in a <code>Rainbow</code>.</li> </ul> <p>Each of these categories has its own directory inside <code>chromatic/rainbows/</code> where the corresponding code should be stored. The <code>.help()</code> method attached to any <code>Rainbow</code> will list everything you can do with it.</p> In\u00a0[7]: Copied! <pre>r.help()\n</pre> r.help() <pre>\nHooray for you! You asked for help on what you can do\nwith this \ud83c\udf08 object. Here's a quick reference of a few\navailable options for things to try.\n\n-----------\n| actions |\n-----------\n\n\ud83c\udf08\ud83e\uddee\ud83d\udcdd | +-*/                         \n   Do basic math operations with two Rainbows.\n\ud83c\udf08\ud83d\uddc2\ud83d\udd2a | .[:,:]()                     \n   Index, slice, or mask a Rainbow to get a subset.\n\ud83c\udf08\ud83d\udea7\ud83c\udf0a | .align_wavelengths()         \n   Align spectra with different wavelength grids onto one shared axis.\n\ud83c\udf08\ud83e\uddfa\ud83e\uddf1 | .bin()                       \n   Bin to a new wavelength or time grid.\n\ud83c\udf08\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1\ud83c\udf08 | .compare()                   \n   Connect to other \ud83c\udf08 objects for easy comparison.\n\ud83c\udf08\ud83d\udc08\u23f0 | .concatenate_in_time()       \n   Stitch together two Rainbows with identical wavelengths.\n\ud83c\udf08\ud83d\udc08\ud83c\udf0a | .concatenate_in_wavelength() \n   Stitch together two Rainbows with identical times.\n\ud83c\udf08\ud83d\udea9\ud83d\udc40 | .flag_outliers()             \n   Flag outlier data points.\n\ud83c\udf08\u23f2\ud83c\udf9e | .fold()                      \n   Fold times relative to a particular period and epoch.\n\ud83c\udf08\ud83e\uddfa\u23f0 | .get_average_lightcurve_as_rainbow() \n   Bin down to a single integrated light curve.\n\ud83c\udf08\ud83e\uddfa\ud83c\udf0a | .get_average_spectrum_as_rainbow() \n   Bin down to a single integrated spectrum.\n\ud83c\udf08\ud83c\udfa7\ud83c\udfb2 | .inject_noise()              \n   Inject (uncorrelated, simple) random noise.\n\ud83c\udf08\ud83c\udfa7\ud83c\udfb9 | .inject_systematics()        \n   Inject (correlated, wobbly) systematic noise.\n\ud83c\udf08\u2b50\ufe0f\ud83d\udc7b | .inject_spectrum()           \n   Inject a static stellar spectrum.\n\ud83c\udf08\ud83e\ude90\ud83d\ude9e | .inject_transit()            \n   Inject a transit signal.\n\ud83c\udf08\ud83e\uded3\ud83d\ude11 | .normalize()                 \n   Normalize by dividing through by a typical spectrum (and/or light curve).\n\ud83c\udf08\ud83c\udff4\u200d\u2620\ufe0f\ud83d\udec1 | .remove_trends()             \n   Remove smooth trends in time and/or wavelength.\n\ud83c\udf08\ud83d\ude87\ud83c\udf0a | .shift()                     \n   Doppler shift wavelengths.\n\ud83c\udf08\ud83c\udf71\ud83d\udc87 | .trim()                      \n   Trim away wavelengths or times.\n\n----------------\n| get/timelike |\n----------------\n\n\u23f0\ud83d\udc0b\ud83c\udf08 | .get_average_lightcurve()    \n   Get the weighted average light curve.\n\u23f0\ud83d\udd0e\ud83c\udf08 | .get_for_time()              \n   Get a quantity associated with a time index.\n\u23f0\ud83d\udd96\ud83c\udf08 | .get_median_lightcurve()     \n   Get the median light curve.\n\u23f0\ud83d\udc4c\ud83c\udf08 | .get_ok_data_for_time()      \n   Get a quantity associated with a time index.\n\u23f0\ud83d\udef0\ud83c\udf08 | .get_times_as_astropy()      \n   Get the times as an astropy Time object.\n\u23f0\ud83d\ude80\ud83c\udf08 | .set_times_from_astropy()    \n   Set the times from an astropy Time object (modifies in-place).\n\n----------------\n| get/wavelike |\n----------------\n\n\ud83c\udf0a\ud83d\udc0b\ud83c\udf08 | .get_average_spectrum()      \n   Get the weighted average spectrum.\n\ud83c\udf0a\ud83d\udd0e\ud83c\udf08 | .get_for_wavelength()        \n   Get a quantity associated with a wavelength index.\n\ud83c\udf0a\ud83c\udfaf\ud83c\udf08 | .get_measured_scatter()      \n   Get the measured scatter on the time series for each wavelength.\n\ud83c\udf0a\ud83d\udd96\ud83c\udf08 | .get_median_spectrum()       \n   Get the median spectrum.\n\ud83c\udf0a\ud83d\udc4c\ud83c\udf08 | .get_ok_data_for_wavelength() \n   Get a quantity associated with a wavelength index.\n\ud83c\udf0a\ud83d\udc8e\ud83c\udf08 | .get_spectral_resolution()   \n   Get the spectral resolution (R=w/dw).\n\n-----------\n| helpers |\n-----------\n\n\ud83d\ude4b\ud83c\udf08\ud83d\udcc4 | .help()                      \n   Get one-line help summaries of available methods.\n\ud83d\udcd3\ud83c\udf08\ud83e\udeb5 | .history()                   \n   Get the history that went into this Rainbow.\n\ud83d\udcbe\ud83c\udf08\ud83d\udcfc | .save()                      \n   Save this Rainbow out to a permanent file.\n\n------------------\n| visualizations |\n------------------\n\n\ud83c\udfa8\ud83d\udcfd\u23f0 | .animate_lightcurves()       \n   Animate a sequence of light curves across different wavelengths.\n\ud83c\udfa8\ud83d\udcfd\ud83c\udf0a | .animate_spectra()           \n   Animate a sequence of spectra across different times.\n\ud83c\udfa8\ud83d\uddbc\ud83d\udcfa | .imshow()                    \n   Paint a map of flux across wavelength and time.\n\ud83c\udfa8\ud83d\udd79\ud83d\udcfa | .imshow_interact()           \n   Show flux map and lightcurves with interactive wavelength selection.\n\ud83c\udfa8\ud83d\udd8c\ud83d\udcfa | .pcolormesh()                \n   Paint a map of flux across wavelength and time (with non-uniform grids).\n\ud83c\udfa8\ud83d\udd8c\ud83e\uddf6 | .plot()                      \n   Plot a sequence of light curves with vertical offsets.\n\n------------------------------\n| visualizations/diagnostics |\n------------------------------\n\n\ud83c\udfa8\ud83d\uddc2\ud83d\udcfa | .paint_quantities()          \n   Show multiple 2D (wavelength and time) quantities as imshow maps.\n\ud83c\udfa8\ud83d\uddc2\ud83e\uddf6 | .plot_quantities()           \n   Show multiple 1D (wavelength or time) quantities as scatter plots.\n\n-------------------------\n| visualizations/models |\n-------------------------\n\n\ud83c\udfa8\ud83d\udcfa\ud83c\udfa2 | .paint_with_models()         \n   Paint a flux map with model components.\n\ud83c\udfa8\ud83d\udd8c\ud83c\udfa2 | .plot_one_wavelength_with_models() \n   Plot one wavelength's light curve with model components.\n\ud83c\udfa8\ud83d\udcfd\ud83c\udfa2 | .animate_with_models()       \n   Animate all wavelengths' light curves with model components.\n\ud83c\udfa8\ud83e\uddf6\ud83c\udfa2 | .plot_with_model()           \n   Plot a sequence of light curves with their models.\n\n---------------------------\n| visualizations/timelike |\n---------------------------\n\n\ud83c\udfa8\u23f0\ud83d\udc0b | .plot_average_lightcurve()   \n   Plot the weighted average flux per time.\n\ud83c\udfa8\u23f0\ud83d\udd96 | .plot_median_lightcurve()    \n   Plot the median flux per time.\n\n---------------------------\n| visualizations/wavelike |\n---------------------------\n\n\ud83c\udfa8\ud83c\udf0a\ud83d\udd2d | .plot_average_spectrum()     \n   Plot the weighted average flux per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83d\udc8e | .plot_spectral_resolution()  \n   Plot the spectral resolution per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83c\udfa7 | .plot_noise_comparison()     \n   Plot the measured and expected scatter per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83e\udd66 | .plot_noise_comparison_in_bins() \n   Plot measured and expected scatter in different size bins.\n</pre>"},{"location":"designing/#designing-new-features","title":"Designing New \ud83c\udf08 Features\u00b6","text":"<p>The <code>Rainbow</code> class is the heart of <code>chromatic</code>. We aim for it to be as intuitive and easy-to-use as possible, to enable transparent and repeatable analysis of spectrosopic time-series datasets. This page collects a few explanations that might be useful if you're trying to develop new  <code>Rainbow</code> features.</p>"},{"location":"designing/#how-are-rainbow-objects-organized","title":"How are <code>Rainbow</code> objects organized?\u00b6","text":"<p>The user mainly interacts with quantities like time, wavelength, and flux via the <code>.time</code>, <code>.wavelength</code>, <code>.flux</code> attributes. However, if you more closely at what's happening inside a <code>Rainbow</code> object, you'll see that these are properties that pull data from some core dictionaries. The general user probably doesn't need to interact with the core dictionaries, but if you're developing new features for <code>Rainbow</code> objects you will want to understand what's happening a little more clearly.</p> <p>The core dictionaries are designed to store quantities that have dimensions like either <code>wavelength</code>, <code>time</code>, or <code>flux</code>:</p> <ul> <li><code>.wavelike[...]</code> contains everything that has the same dimensions as <code>wavelength</code>. This dictionary will contain at least a <code>'wavelength'</code> key, with the actual wavelengths themselves. It might also contain information like the average spectrum of the star, the average S/N at each wavelength, the number of original detector pixels that wound up in this particular wavelength bin, and/or a mask of what wavelengths should be considered good or bad (no matter the time point).</li> <li><code>.timelike[...]</code> contains everything that has the same dimensons as <code>time</code>. This dictionary will contain at least a <code>'time'</code> key, with the actual times themselves. It might also contain information like a broadband light curve of the transit, the x or y position of the spectrum on the detector, the temperature of the detector, and/or a mask of what times should be considered good or bad (no matter the wavelength).</li> <li><code>.fluxlike[...]</code> contains everything that has the same dimensons as <code>flux</code>. This dictionary will contain at least a <code>'flux'</code> key, with the actual fluxes themselves. It should also contain an <code>'uncertainty'</code> keyword with the uncertainties associated with those fluxes (or maybe <code>None</code>). It might also contain information about other quantities that depend on both time and wavelength, such as the centroid of the spectral trace, the maximum fraction of saturation, and/or a mask of what individual points should be considered good or bad.</li> </ul> <p>There is one more core dictionary:</p> <ul> <li><code>.metadata</code> contains general information that might be useful to hang onto, to pass along to another derived object, or to save out to a file.</li> </ul>"},{"location":"designing/#what-can-rainbow-objects-do","title":"What can <code>Rainbow</code> objects do?\u00b6","text":""},{"location":"designing/#how-do-we-add-new-abilities-to-rainbow-objects","title":"How do we add new abilities to <code>Rainbow</code> objects?\u00b6","text":"<p>If you want to add a new method to a <code>Rainbow</code> object, there are a few general steps you'll probably want to follow.</p> <ol> <li>Install in development mode (see Installation), so you can modify the code package directly and test the code in place.</li> <li>Decide a category of \"things a <code>Rainbow</code> can do\" in which it belongs.</li> <li>Find the directory for that category in the <code>chromatic</code> code package. For example, \"actions\" that return new <code>Rainbow</code> objects will be in <code>chromatic/rainbows/actions/</code>.</li> <li>Look at another example in that directory to get a sense for the general layout. For example, you'll notice that \"actions\" generally create a copy of <code>self</code>, make some changes to that copy, and then return it as the new object.</li> <li>Add your new function, either to an existing <code>.py</code> file where it would make sense or to its own new <code>.py</code> file. The first argument to your function should be <code>self</code>, which is the <code>Rainbow</code> object itself, and then any additional arguments should follow afterward. A good way to test out your new function in a notebook or a small isolated script might look something like the following:</li> </ol> <pre>from chromatic import *\nr = SimulatedRainbow().inject_noise()\ndef snazzy_new_action(self, x=2):\n    \n    # create a copy of the original object\n    new = self._create_copy()\n    \n    # do something\n    new.fluxlike['flux'] = new.flux**x\n    \n    # return the modified copy\n    return new\noutput = snazzy_new_action(r)\n</pre> <p>This allows you to develop and test your new function without having to worry about it being imported properly into the core <code>Rainbow</code> definition.</p> <ol> <li>Connect your new function into the <code>Rainbow</code> class definition. Normally, we might define a new method directly in the same file as the class definition itself, but we wanted to split the method definitions into multiple files and directories, to make things easier to find. Importing your new function to become a <code>Rainbow</code> method takes a few steps. For example, let's imagine you're making a new \"action\" called <code>snazzy_new_action</code> and it's located in <code>chromatic/rainbows/actions/snazzy.py</code>:<ul> <li>In <code>rainbows/actions/snazzy.py</code>, include line of code like <code>__all__ = ['snazzy_new_action']</code> at the top. The <code>__all__</code> list defines what would get imported via a line like <code>from chromatic.rainbows.actions.snazzy import *</code> (things not in <code>__all__</code> will need to be explicitly imported).</li> <li>In <code>rainbows/actions/__init__.py</code>, include a line of code like <code>from .snazzy import *</code>, so that imports from <code>chromatic.rainbows.actions</code> will know how to find things in <code>snazzy.py</code>.</li> <li>In <code>rainbows/rainbow.py</code>, down at the very bottom of the class definition for <code>Rainbow</code>, add <code>snazzy_new_action</code> to the list of <code>from .actions import (...)</code>. This, finally, will mean that all <code>Rainbow</code> objects will have access to your new method, and we can do things like <code>r.snazzy_new_action()</code> from any <code>Rainbow</code>.</li> <li>In <code>rainbows/actions/descriptions.txt</code>, add a row describing your snazzy new action. This table defines what appears in the <code>.help()</code> method.</li> </ul> </li> <li>Write a test for your new feature. This is a function that somehow tests whether your new feature works; the simplest form would be \"does this function run\", a slightly fancier version would be \"are its outputs accurate.\" To write a test:<ul> <li>Look in <code>chromatic/tests/</code> to find a bunch of <code>test_*.py</code> files with examples of automated tests in them.</li> <li>Create a function that has <code>*test*</code> somewhere in its name and store it somewhere in the <code>tests</code> directory. At a minimum, this function should test that the bit of code you wrote runs without breaking.</li> <li>Run <code>pytest</code> from the command line within the main repository directory. This will run your test function (along with all the other tests) and tell you whether it passed or failed. Make sure it passes!</li> </ul> </li> </ol>"},{"location":"designing/#have-fun-good-luck","title":"Have fun! Good luck!\u00b6","text":""},{"location":"documentation/","title":"Writing \ud83c\udf08 Documentation","text":""},{"location":"documentation/#writing-documentation","title":"Writing \ud83c\udf08 Documentation\u00b6","text":"<p>If you're contributing a new feature to <code>chromatic</code>, please consider also contributing some documentation to explain how your feature works. Here's the very short version of how to add to the documentation:</p> <ol> <li>Install in development mode (see Installation), so you have access to <code>mkdocs</code> and the various extensions needed to render the documentation.</li> <li>Decide whether your explanation would fit well within an existing page or whether you need a new one. In the <code>docs/</code> directory, find the appropraite <code>.ipynb</code> notebook file or create a new one. If you create a new one, add it to the <code>nav:</code> section of the <code>mkdocs.yml</code> file in the main repository directory so that <code>mkdocs</code> will know to include it.</li> <li>Write your example and explanation in a <code>.ipynb</code> file. Your audience should be smart people who want to use the code but don't have much experience with it yet. Be friendly and encouraging!</li> <li>From the Terminal, run <code>mkdocs serve</code>. This will convert all of the source notebooks into a live website, and give you a little address that you can copy and paste into a browser window. While that <code>mkdocs serve</code> command is still running, small changes you make to existing <code>.ipynb</code> source files will appear (sometimes after a few minutes) on the live locally-hosted webserver.</li> <li>Once you're happy with your new documentation, before committing it to the repository, please run \"Kernal &gt; Restart &amp; Clear Output\" or something similar to remove all outputs from the source notebook file. The <code>chromatic</code> repository will hang onto all changes that you commit to it, so it would very quickly get annoyingly large unless we leave the outputs out of committed notebook files. Double check the outputs are all gone, save your notebook, and then commit it to the <code>git</code> repository (see Contributing \ud83c\udf08 Code with GitHub).</li> </ol> <p>Periodically, after reviewing and copy-editing the documentation, we'll deploy the newest version up to the web at zkbt.github.io/chromatic/ for all to enjoy.</p>"},{"location":"example-timeseries-spectra/","title":"\ud83c\udf08\u23f0\ud83c\udf0a time-series spectra","text":"<p>This example notebook provides quick visualizations to characterize a set of time-series stellar spectra for a transiting exoplanet, using the <code>chromatic</code> toolkit. This notebook can be run before you have fit any models to the data, requiring only the data themselves and a few details about the planet. To access this notebook, you might want to:</p> <ul> <li>Download it from GitHub (click 'Raw' to download the <code>.ipynb</code> file) and run on your own computer.</li> <li>Open in Google Colab, load your data into that interactive session, and run everything online.</li> </ul> <p>You should be able to modify just code cells in the \ud83d\udcbb Load the Data + \ud83e\ude90 Describe the Planet section, and then run the entire notebook to automatically generate visualizations.</p> In\u00a0[1]: Copied! <pre>!pip install chromatic-lightcurves --upgrade --quiet\n</pre> !pip install chromatic-lightcurves --upgrade --quiet <p>Once <code>chromatic</code> is installed, you should have access to all the tools you need!</p> In\u00a0[2]: Copied! <pre>from chromatic import *\n</pre> from chromatic import * In\u00a0[3]: Copied! <pre>version()\n</pre> version() Out[3]: <pre>'0.5.0'</pre> <p>Let's load the data into a <code>chromatic</code> \ud83c\udf08 object. If you encounter errors loading your dataset, try specifying the file format with <code>read_rainbow(filename, format=...)</code> as described in the Reading/Writing a \ud83c\udf08. Let's also associated a title with this object, which will automatically appear in most plots.</p> In\u00a0[4]: Copied! <pre>filename = \"example-datasets/stsci/jw02734*x1dints.fits\"\nrainbow = read_rainbow(filename)\nrainbow.title = \"WASP-96b | NIRISS | x1dints\"\n</pre> filename = \"example-datasets/stsci/jw02734*x1dints.fits\" rainbow = read_rainbow(filename) rainbow.title = \"WASP-96b | NIRISS | x1dints\" <pre>\ud83c\udf08\ud83e\udd16 This file contains data for 2 spectroscopic orders. Because no\n`order=` keyword was supplied, we're defaulting to first order. You can\nhide this warning by expliciting stating which order you want to load.\nFor this file, the options include [1 2].\n\n</pre> <pre>\ud83c\udf08\ud83e\udd16 The 2048 input wavelengths were not monotonically increasing.\n&lt;\ud83c\udf08(2048w, 280t)&gt; has been sorted from lowest to highest wavelength.\nIf you want to recover the original wavelength order, the original\nwavelength indices are available in `rainbow.original_wave_index`.\n\n</pre> <p>Let's define a few basic parameters describing the planet. These will help make the plots easier to interpret and be used to mask out the transit for noise characterization. If you need a period, epoch, and duration for your transit, you might find them in the NASA Exoplanet Archive.</p> In\u00a0[5]: Copied! <pre>period = 3.4252577 * u.day\nt0 = 2459111.30170 * u.day\nduration = 2.4264 * u.hour * 1.1\n</pre> period = 3.4252577 * u.day t0 = 2459111.30170 * u.day duration = 2.4264 * u.hour * 1.1 <p>Let's set some very basic plotting defaults. These will try to sync up the vertical limits of light curve plots with the color maps of 2D flux maps.</p> In\u00a0[6]: Copied! <pre>vmin = 0.98\nvmax = 1.005\nylim = [vmin, vmax]\n</pre> vmin = 0.98 vmax = 1.005 ylim = [vmin, vmax] <p>Let's make some plots to get an overall sense for the dataset. First of all, let's normalize by dividing through by the median stellar spectrum and display a 2D map of the flux.</p> In\u00a0[7]: Copied! <pre>normalized = rainbow.normalize()\n</pre> normalized = rainbow.normalize() In\u00a0[8]: Copied! <pre>normalized.pcolormesh(vmin=vmin, vmax=vmax, filename=\"unbinned-2D-flux.png\");\n</pre> normalized.pcolormesh(vmin=vmin, vmax=vmax, filename=\"unbinned-2D-flux.png\"); <pre>\ud83c\udf08\ud83e\udd16 In using`.pcolormesh`, [display pixels] / [data pixels] =\n[455. 404.] / [ 280 2048] = [1.62328546 0.19729243] ('Time', 'Wavelength')\nIs less than the suggested threshold of 2.\n\nThis suggests that aliasing/moir\u00e9 might be a problem, where too many\ndata pixels are trying to be displayed with too few pixels, and the\nchoices `matplotlib` makes for how to do that might not be intuitive.\n\nHere are possible solutions:\n    - Use `.bin()` to decrease the number of data pixels in wavelength\n    and/or time, effectively averaging before displaying, rather than\n    asking `matplotlib` to decide how to visually average adjacent data.\n    - Increase the `dpi` of the figure in which this appears, so there are\n    enough display pixels to represent all the data pixels being shown.\n    - Use `.imshow()` instead of `.pcolormesh`, which can do better\n    built-in handling of antialiasing for large data arrays. Since `.imshow()`\n    can only show uniform wavelength and time grids, non-uniform grids will\n    be labeled via index instead of actual value.\n\n</pre> <p>Let's trim any bad wavelengths off the edges and phase-fold these data to the planet's known period and time of mid-transit. This latter step is just to make the times easier to interpret.</p> In\u00a0[9]: Copied! <pre>tidied = normalized.trim().fold(period=period, t0=t0)\n</pre> tidied = normalized.trim().fold(period=period, t0=t0) In\u00a0[10]: Copied! <pre>tidied.pcolormesh(vmin=vmin, vmax=vmax, filename=\"trimmed-2D-flux.png\");\n</pre> tidied.pcolormesh(vmin=vmin, vmax=vmax, filename=\"trimmed-2D-flux.png\"); <pre>\ud83c\udf08\ud83e\udd16 In using`.pcolormesh`, [display pixels] / [data pixels] =\n[455. 404.] / [ 280 2038] = [1.62328546 0.1982605 ] ('Time', 'Wavelength')\nIs less than the suggested threshold of 2.\n\nThis suggests that aliasing/moir\u00e9 might be a problem, where too many\ndata pixels are trying to be displayed with too few pixels, and the\nchoices `matplotlib` makes for how to do that might not be intuitive.\n\nHere are possible solutions:\n    - Use `.bin()` to decrease the number of data pixels in wavelength\n    and/or time, effectively averaging before displaying, rather than\n    asking `matplotlib` to decide how to visually average adjacent data.\n    - Increase the `dpi` of the figure in which this appears, so there are\n    enough display pixels to represent all the data pixels being shown.\n    - Use `.imshow()` instead of `.pcolormesh`, which can do better\n    built-in handling of antialiasing for large data arrays. Since `.imshow()`\n    can only show uniform wavelength and time grids, non-uniform grids will\n    be labeled via index instead of actual value.\n\n</pre> <p>Now let's bin to a fixed wavelength resolution $R=\\lambda/\\Delta\\lambda$ and cadence $dt$. Be averaging together multiple wavelengths and/or times, we'll decrease the noise (at the cost of worse resolution); some features may become more apparent with stronger binning, as long as the bins aren't so large to smooth features away. We'll normalize again after binning, once the noise has been averaged down a little bit.</p> In\u00a0[11]: Copied! <pre>binned = tidied.bin(R=100, dt=2 * u.minute).normalize()\n</pre> binned = tidied.bin(R=100, dt=2 * u.minute).normalize() In\u00a0[12]: Copied! <pre>binned.pcolormesh(vmin=vmin, vmax=vmax, filename=\"binned-2D-flux.png\");\n</pre> binned.pcolormesh(vmin=vmin, vmax=vmax, filename=\"binned-2D-flux.png\"); <p>If we want to see how these flux maps line up to light curves, we can use the interactive exploration tool to drag and select different wavelength ranges, and plot the (unweighted) average light curves for them. Being able to select particular wavelength ranges allows us to zoom in on features of interest or trends with wavelength.</p> In\u00a0[13]: Copied! <pre>binned.imshow_interact(ylim=ylim, filename=\"interactive-2D-flux.html\")\n</pre> binned.imshow_interact(ylim=ylim, filename=\"interactive-2D-flux.html\") <p>Let's make an animation the flips through wavelength bins. Animations can be useful way to to let your eyes take in a lot of light curves in quick succession, to see how trends or noise sources change with wavelength.</p> In\u00a0[14]: Copied! <pre>binned.animate_lightcurves(ylim=ylim, filename=\"animated-lightcurves.gif\")\n</pre> binned.animate_lightcurves(ylim=ylim, filename=\"animated-lightcurves.gif\") <pre>&lt;IPython.core.display.Image object&gt;</pre> <p>Let's look at the weighted average light curve, integrating together all the available wavelengths. The binned uncertainties should be smaller than any individual wavelength bin, so this should hopefully be a very precise light curve!</p> In\u00a0[15]: Copied! <pre>binned.plot_average_lightcurve(ylim=ylim, filename=\"integrated-lightcurve.png\")\n</pre> binned.plot_average_lightcurve(ylim=ylim, filename=\"integrated-lightcurve.png\") <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/utilities.py:169: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap', 'norm' will be ignored\n  plt.scatter(x, y, **scatterkw)\n</pre> <p>Finally, let's bin the data to very low spectral resolution, and then plot a stack of light curves for the separate wavelengths.</p> In\u00a0[16]: Copied! <pre>really_binned = binned.bin(R=5)\n</pre> really_binned = binned.bin(R=5) In\u00a0[17]: Copied! <pre>really_binned.plot(spacing=(vmax - vmin) / 2, filename=\"stack-of-lightcurves.png\");\n</pre> really_binned.plot(spacing=(vmax - vmin) / 2, filename=\"stack-of-lightcurves.png\"); In\u00a0[18]: Copied! <pre>out_of_transit = (\n    rainbow.trim().mask_transit(period=period, t0=t0, duration=duration).normalize()\n)\n</pre> out_of_transit = (     rainbow.trim().mask_transit(period=period, t0=t0, duration=duration).normalize() ) In\u00a0[19]: Copied! <pre>out_of_transit.pcolormesh(\n    vmin=vmin, vmax=vmax, filename=\"trimmed-2D-flux-with-transit-removed.png\"\n);\n</pre> out_of_transit.pcolormesh(     vmin=vmin, vmax=vmax, filename=\"trimmed-2D-flux-with-transit-removed.png\" ); <pre>\ud83c\udf08\ud83e\udd16 In using`.pcolormesh`, [display pixels] / [data pixels] =\n[455. 404.] / [ 280 2038] = [1.62328546 0.1982605 ] ('Time', 'Wavelength')\nIs less than the suggested threshold of 2.\n\nThis suggests that aliasing/moir\u00e9 might be a problem, where too many\ndata pixels are trying to be displayed with too few pixels, and the\nchoices `matplotlib` makes for how to do that might not be intuitive.\n\nHere are possible solutions:\n    - Use `.bin()` to decrease the number of data pixels in wavelength\n    and/or time, effectively averaging before displaying, rather than\n    asking `matplotlib` to decide how to visually average adjacent data.\n    - Increase the `dpi` of the figure in which this appears, so there are\n    enough display pixels to represent all the data pixels being shown.\n    - Use `.imshow()` instead of `.pcolormesh`, which can do better\n    built-in handling of antialiasing for large data arrays. Since `.imshow()`\n    can only show uniform wavelength and time grids, non-uniform grids will\n    be labeled via index instead of actual value.\n\n</pre> <p>Because unbinned data have the small wavelength and time intervals, and therefore contain the relatively few photons per bin, their expected uncertainties will be large. As we bin to larger wavelength intervals (lower resolution) and longer time intervals, the expected uncertainties will decrease, revealing increasingly subtle features.</p> In\u00a0[20]: Copied! <pre>resolutions = [None, 30, 10, 3]\nif rainbow.ntime &gt; 1e4:\n    dt = 1 * u.minute\nelse:\n    dt = None\n\nbinned_rainbows = {}\nN = len(resolutions)\nfi, ax = plt.subplots(N, 1, sharex=True, figsize=(8, N * 3))\nfor i, R in enumerate(resolutions):\n    binned_rainbows[R] = out_of_transit.bin(R=R, dt=dt).normalize()\n    binned_rainbows[R].pcolormesh(ax=ax[i])\n    plt.title(f\"R={R}, dt={binned_rainbows[R].dt:.2} | {ax[i].get_title()}\")\nrainbow.savefig(\"binned-2D-flux-no-transit.png\")\n</pre> resolutions = [None, 30, 10, 3] if rainbow.ntime &gt; 1e4:     dt = 1 * u.minute else:     dt = None  binned_rainbows = {} N = len(resolutions) fi, ax = plt.subplots(N, 1, sharex=True, figsize=(8, N * 3)) for i, R in enumerate(resolutions):     binned_rainbows[R] = out_of_transit.bin(R=R, dt=dt).normalize()     binned_rainbows[R].pcolormesh(ax=ax[i])     plt.title(f\"R={R}, dt={binned_rainbows[R].dt:.2} | {ax[i].get_title()}\") rainbow.savefig(\"binned-2D-flux-no-transit.png\") <pre>\ud83c\udf08\ud83e\udd16 In using`.pcolormesh`, [display pixels] / [data pixels] =\n[612. 265.] / [ 280 2038] = [2.18631142 0.1301143 ] ('Time', 'Wavelength')\nIs less than the suggested threshold of 2.\n\nThis suggests that aliasing/moir\u00e9 might be a problem, where too many\ndata pixels are trying to be displayed with too few pixels, and the\nchoices `matplotlib` makes for how to do that might not be intuitive.\n\nHere are possible solutions:\n    - Use `.bin()` to decrease the number of data pixels in wavelength\n    and/or time, effectively averaging before displaying, rather than\n    asking `matplotlib` to decide how to visually average adjacent data.\n    - Increase the `dpi` of the figure in which this appears, so there are\n    enough display pixels to represent all the data pixels being shown.\n    - Use `.imshow()` instead of `.pcolormesh`, which can do better\n    built-in handling of antialiasing for large data arrays. Since `.imshow()`\n    can only show uniform wavelength and time grids, non-uniform grids will\n    be labeled via index instead of actual value.\n\n</pre> <p>If the data were perfect and simple, we would expect the scatter in the out-of-transit flux to match the expected uncertainties for each wavelength. Let's compare the expected and measured scatters, after binning down in wavelength and time. If the measured scatter does not decrease as expected when averaging bins together, there is probably some systematic noise that is correlated across wavelength and/or time that should be addressed.</p> In\u00a0[21]: Copied! <pre>fi, ax = plt.subplots(\n    N, 2, sharey=True, figsize=(8, N * 3), gridspec_kw=dict(width_ratios=[2, 1])\n)\nfor i, R in enumerate(binned_rainbows):\n    binned_rainbows[R].plot_noise_comparison(ax=ax[i, 0], method=\"MAD\")\n    plt.title(\n        f\"R={R}, dt={binned_rainbows[R].dt:.2} | with transit removed\\n{ax[i,0].get_title()}\"\n    )\n    binned_rainbows[R].plot_noise_comparison_in_bins(ax=ax[i, 1], method=\"MAD\")\nrainbow.savefig(\"noise-comparison-no-transit.png\")\n</pre> fi, ax = plt.subplots(     N, 2, sharey=True, figsize=(8, N * 3), gridspec_kw=dict(width_ratios=[2, 1]) ) for i, R in enumerate(binned_rainbows):     binned_rainbows[R].plot_noise_comparison(ax=ax[i, 0], method=\"MAD\")     plt.title(         f\"R={R}, dt={binned_rainbows[R].dt:.2} | with transit removed\\n{ax[i,0].get_title()}\"     )     binned_rainbows[R].plot_noise_comparison_in_bins(ax=ax[i, 1], method=\"MAD\") rainbow.savefig(\"noise-comparison-no-transit.png\") <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/wavelike/noise_comparison.py:132: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(plot_x, yplot, **this_scatterkw)\n</pre> <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/wavelike/noise_comparison.py:132: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(plot_x, yplot, **this_scatterkw)\n</pre> <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/wavelike/noise_comparison.py:132: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(plot_x, yplot, **this_scatterkw)\n</pre> <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/wavelike/noise_comparison.py:132: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(plot_x, yplot, **this_scatterkw)\n</pre> <p>It's possible that excess scatter might be the result of very smooth trends in time. Let's repeat the above analysis of the dataset binned to different wavelength and time intervals, but also remove a smooth quadratic trend in time, which could imagine might be either instrumental or astrophysical.</p> In\u00a0[22]: Copied! <pre>fi, ax = plt.subplots(N, 1, sharex=True, figsize=(8, N * 3))\nfor i, R in enumerate(binned_rainbows):\n    with_trend_removed = binned_rainbows[R].remove_trends(method=\"polyfit\", deg=2)\n    with_trend_removed.pcolormesh(ax=ax[i])\n    plt.title(f\"R={R}, dt={binned_rainbows[R].dt:.2} | {ax[i].get_title()}\")\nrainbow.savefig(\"binned-2D-flux-no-transit-no-trends.png\")\n</pre> fi, ax = plt.subplots(N, 1, sharex=True, figsize=(8, N * 3)) for i, R in enumerate(binned_rainbows):     with_trend_removed = binned_rainbows[R].remove_trends(method=\"polyfit\", deg=2)     with_trend_removed.pcolormesh(ax=ax[i])     plt.title(f\"R={R}, dt={binned_rainbows[R].dt:.2} | {ax[i].get_title()}\") rainbow.savefig(\"binned-2D-flux-no-transit-no-trends.png\") <pre>\ud83c\udf08\ud83e\udd16 In using`.pcolormesh`, [display pixels] / [data pixels] =\n[612. 265.] / [ 280 2038] = [2.18631142 0.1301143 ] ('Time', 'Wavelength')\nIs less than the suggested threshold of 2.\n\nThis suggests that aliasing/moir\u00e9 might be a problem, where too many\ndata pixels are trying to be displayed with too few pixels, and the\nchoices `matplotlib` makes for how to do that might not be intuitive.\n\nHere are possible solutions:\n    - Use `.bin()` to decrease the number of data pixels in wavelength\n    and/or time, effectively averaging before displaying, rather than\n    asking `matplotlib` to decide how to visually average adjacent data.\n    - Increase the `dpi` of the figure in which this appears, so there are\n    enough display pixels to represent all the data pixels being shown.\n    - Use `.imshow()` instead of `.pcolormesh`, which can do better\n    built-in handling of antialiasing for large data arrays. Since `.imshow()`\n    can only show uniform wavelength and time grids, non-uniform grids will\n    be labeled via index instead of actual value.\n\n</pre> In\u00a0[23]: Copied! <pre>fi, ax = plt.subplots(\n    N, 2, sharey=True, figsize=(8, N * 3), gridspec_kw=dict(width_ratios=[2, 1])\n)\nfor i, R in enumerate(binned_rainbows):\n    with_trend_removed = binned_rainbows[R].remove_trends(method=\"polyfit\", deg=2)\n    with_trend_removed.plot_noise_comparison(ax=ax[i, 0], method=\"MAD\")\n    plt.title(\n        f\"R={R}, dt={binned_rainbows[R].dt:.2} | with transit + trends removed\\n{ax[i,0].get_title()}\"\n    )\n    with_trend_removed.plot_noise_comparison_in_bins(ax=ax[i, 1], method=\"MAD\")\nrainbow.savefig(\"noise-comparison-no-transit-no-trends.png\")\n</pre> fi, ax = plt.subplots(     N, 2, sharey=True, figsize=(8, N * 3), gridspec_kw=dict(width_ratios=[2, 1]) ) for i, R in enumerate(binned_rainbows):     with_trend_removed = binned_rainbows[R].remove_trends(method=\"polyfit\", deg=2)     with_trend_removed.plot_noise_comparison(ax=ax[i, 0], method=\"MAD\")     plt.title(         f\"R={R}, dt={binned_rainbows[R].dt:.2} | with transit + trends removed\\n{ax[i,0].get_title()}\"     )     with_trend_removed.plot_noise_comparison_in_bins(ax=ax[i, 1], method=\"MAD\") rainbow.savefig(\"noise-comparison-no-transit-no-trends.png\") <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/wavelike/noise_comparison.py:132: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(plot_x, yplot, **this_scatterkw)\n\ud83c\udf08\ud83e\udd16 The `remove_trends` function was applied to this `Rainbow`,\nmaking it very plausible that some long-timescale signals\nand/or noise have been suppressed. Be suspicious of binned\nscatters on long timescales.\n\n</pre> <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/wavelike/noise_comparison.py:132: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(plot_x, yplot, **this_scatterkw)\n\ud83c\udf08\ud83e\udd16 The `remove_trends` function was applied to this `Rainbow`,\nmaking it very plausible that some long-timescale signals\nand/or noise have been suppressed. Be suspicious of binned\nscatters on long timescales.\n\n</pre> <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/wavelike/noise_comparison.py:132: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(plot_x, yplot, **this_scatterkw)\n\ud83c\udf08\ud83e\udd16 The `remove_trends` function was applied to this `Rainbow`,\nmaking it very plausible that some long-timescale signals\nand/or noise have been suppressed. Be suspicious of binned\nscatters on long timescales.\n\n</pre> <pre>/Users/zabe0091/Dropbox/zach/code/chromatic/chromatic/rainbows/visualizations/wavelike/noise_comparison.py:132: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(plot_x, yplot, **this_scatterkw)\n\ud83c\udf08\ud83e\udd16 The `remove_trends` function was applied to this `Rainbow`,\nmaking it very plausible that some long-timescale signals\nand/or noise have been suppressed. Be suspicious of binned\nscatters on long timescales.\n\n</pre> <p>That's it! Hopefully these automated visualizations can serve as a useful starting place for understanding the dataset you're working with and for comparing to other analyses.</p> In\u00a0[24]: Copied! <pre>rainbow.help()\n</pre> rainbow.help() <pre>\nHooray for you! You asked for help on what you can do\nwith this \ud83c\udf08 object. Here's a quick reference of a few\navailable options for things to try.\n\n-----------\n| actions |\n-----------\n\n\ud83c\udf08\ud83e\uddee\ud83d\udcdd | +-*/                         \n   Do basic math operations with two Rainbows.\n\ud83c\udf08\ud83d\uddc2\ud83d\udd2a | .[:,:]()                     \n   Index, slice, or mask a Rainbow to get a subset.\n\ud83c\udf08\ud83d\udea7\ud83c\udf0a | .align_wavelengths()         \n   Align spectra with different wavelength grids onto one shared axis.\n\ud83c\udf08\ud83e\uddfa\ud83e\uddf1 | .bin()                       \n   Bin to a new wavelength or time grid.\n\ud83c\udf08\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1\ud83c\udf08 | .compare()                   \n   Connect to other \ud83c\udf08 objects for easy comparison.\n\ud83c\udf08\ud83d\udc08\u23f0 | .concatenate_in_time()       \n   Stitch together two Rainbows with identical wavelengths.\n\ud83c\udf08\ud83d\udc08\ud83c\udf0a | .concatenate_in_wavelength() \n   Stitch together two Rainbows with identical times.\n\ud83c\udf08\ud83d\udea9\ud83d\udc40 | .flag_outliers()             \n   Flag outlier data points.\n\ud83c\udf08\u23f2\ud83c\udf9e | .fold()                      \n   Fold times relative to a particular period and epoch.\n\ud83c\udf08\ud83e\uddfa\u23f0 | .get_average_lightcurve_as_rainbow() \n   Bin down to a single integrated light curve.\n\ud83c\udf08\ud83e\uddfa\ud83c\udf0a | .get_average_spectrum_as_rainbow() \n   Bin down to a single integrated spectrum.\n\ud83c\udf08\ud83c\udfa7\ud83c\udfb2 | .inject_noise()              \n   Inject (uncorrelated, simple) random noise.\n\ud83c\udf08\ud83c\udfa7\ud83c\udfb9 | .inject_systematics()        \n   Inject (correlated, wobbly) systematic noise.\n\ud83c\udf08\u2b50\ufe0f\ud83d\udc7b | .inject_spectrum()           \n   Inject a static stellar spectrum.\n\ud83c\udf08\ud83e\ude90\ud83d\ude9e | .inject_transit()            \n   Inject a transit signal.\n\ud83c\udf08\ud83e\uded3\ud83d\ude11 | .normalize()                 \n   Normalize by dividing through by a typical spectrum (and/or light curve).\n\ud83c\udf08\ud83c\udff4\u200d\u2620\ufe0f\ud83d\udec1 | .remove_trends()             \n   Remove smooth trends in time and/or wavelength.\n\ud83c\udf08\ud83d\ude87\ud83c\udf0a | .shift()                     \n   Doppler shift wavelengths.\n\ud83c\udf08\ud83c\udf71\ud83d\udc87 | .trim()                      \n   Trim away wavelengths or times.\n\n----------------\n| get/timelike |\n----------------\n\n\u23f0\ud83d\udc0b\ud83c\udf08 | .get_average_lightcurve()    \n   Get the weighted average light curve.\n\u23f0\ud83d\udd0e\ud83c\udf08 | .get_for_time()              \n   Get a quantity associated with a time index.\n\u23f0\ud83d\udd96\ud83c\udf08 | .get_median_lightcurve()     \n   Get the median light curve.\n\u23f0\ud83d\udc4c\ud83c\udf08 | .get_ok_data_for_time()      \n   Get a quantity associated with a time index.\n\u23f0\ud83d\udef0\ud83c\udf08 | .get_times_as_astropy()      \n   Get the times as an astropy Time object.\n\u23f0\ud83d\ude80\ud83c\udf08 | .set_times_from_astropy()    \n   Set the times from an astropy Time object (modifies in-place).\n\n----------------\n| get/wavelike |\n----------------\n\n\ud83c\udf0a\ud83d\udc0b\ud83c\udf08 | .get_average_spectrum()      \n   Get the weighted average spectrum.\n\ud83c\udf0a\ud83d\udd0e\ud83c\udf08 | .get_for_wavelength()        \n   Get a quantity associated with a wavelength index.\n\ud83c\udf0a\ud83c\udfaf\ud83c\udf08 | .get_measured_scatter()      \n   Get the measured scatter on the time series for each wavelength.\n\ud83c\udf0a\ud83d\udd96\ud83c\udf08 | .get_median_spectrum()       \n   Get the median spectrum.\n\ud83c\udf0a\ud83d\udc4c\ud83c\udf08 | .get_ok_data_for_wavelength() \n   Get a quantity associated with a wavelength index.\n\ud83c\udf0a\ud83d\udc8e\ud83c\udf08 | .get_spectral_resolution()   \n   Get the spectral resolution (R=w/dw).\n\n-----------\n| helpers |\n-----------\n\n\ud83d\ude4b\ud83c\udf08\ud83d\udcc4 | .help()                      \n   Get one-line help summaries of available methods.\n\ud83d\udcd3\ud83c\udf08\ud83e\udeb5 | .history()                   \n   Get the history that went into this Rainbow.\n\ud83d\udcbe\ud83c\udf08\ud83d\udcfc | .save()                      \n   Save this Rainbow out to a permanent file.\n\n------------------\n| visualizations |\n------------------\n\n\ud83c\udfa8\ud83d\udcfd\u23f0 | .animate_lightcurves()       \n   Animate a sequence of light curves across different wavelengths.\n\ud83c\udfa8\ud83d\udcfd\ud83c\udf0a | .animate_spectra()           \n   Animate a sequence of spectra across different times.\n\ud83c\udfa8\ud83d\uddbc\ud83d\udcfa | .imshow()                    \n   Paint a map of flux across wavelength and time.\n\ud83c\udfa8\ud83d\udd79\ud83d\udcfa | .imshow_interact()           \n   Show flux map and lightcurves with interactive wavelength selection.\n\ud83c\udfa8\ud83d\udd8c\ud83d\udcfa | .pcolormesh()                \n   Paint a map of flux across wavelength and time (with non-uniform grids).\n\ud83c\udfa8\ud83d\udd8c\ud83e\uddf6 | .plot()                      \n   Plot a sequence of light curves with vertical offsets.\n\n------------------------------\n| visualizations/diagnostics |\n------------------------------\n\n\ud83c\udfa8\ud83d\uddc2\ud83d\udcfa | .paint_quantities()          \n   Show multiple 2D (wavelength and time) quantities as imshow maps.\n\ud83c\udfa8\ud83d\uddc2\ud83e\uddf6 | .plot_quantities()           \n   Show multiple 1D (wavelength or time) quantities as scatter plots.\n\n---------------------------\n| visualizations/timelike |\n---------------------------\n\n\ud83c\udfa8\u23f0\ud83d\udc0b | .plot_average_lightcurve()   \n   Plot the weighted average flux per time.\n\ud83c\udfa8\u23f0\ud83d\udd96 | .plot_median_lightcurve()    \n   Plot the median flux per time.\n\n---------------------------\n| visualizations/wavelike |\n---------------------------\n\n\ud83c\udfa8\ud83c\udf0a\ud83d\udd2d | .plot_average_spectrum()     \n   Plot the weighted average flux per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83d\udc8e | .plot_spectral_resolution()  \n   Plot the spectral resolution per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83c\udfa7 | .plot_noise_comparison()     \n   Plot the measured and expected scatter per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83e\udd66 | .plot_noise_comparison_in_bins() \n   Plot measured and expected scatter in different size bins.\n</pre>"},{"location":"example-timeseries-spectra/#time-series-spectra","title":"\ud83c\udf08\u23f0\ud83c\udf0a time-series spectra\u00b6","text":""},{"location":"example-timeseries-spectra/#make-sure-chromatic-is-installed","title":"\ud83d\udcbe Make Sure <code>chromatic</code> is Installed\u00b6","text":"<p>If you don't already have <code>chromatic</code> installed, run the following command.</p>"},{"location":"example-timeseries-spectra/#load-the-data-describe-the-planet","title":"\ud83e\uddd1\u200d\ud83d\udcbb Load the Data + \ud83e\ude90 Describe the Planet\u00b6","text":""},{"location":"example-timeseries-spectra/#make-basic-visualizations","title":"\ud83c\udfa8 Make Basic Visualizations\u00b6","text":""},{"location":"example-timeseries-spectra/#characterize-the-noise","title":"\ud83c\udf2b Characterize the Noise\u00b6","text":"<p>After looking closely at the transit in the data, it can be useful to filter or mask out the transit. By creating a dataset that we expect to be mostly flat, we can characterize the noise properties. First, let's simply mask out the transit and look only at the data before and after it.</p>"},{"location":"example-timeseries-spectra/#explore-further","title":"\ud83d\uddfa Explore Further!\u00b6","text":"<p>You can explore other options to visualize or work with your \ud83c\udf08 data by using the built-in <code>.help()</code> method. On any <code>Rainbow</code> object, run this to list available options.</p>"},{"location":"github/","title":"Contributing \ud83c\udf08 Code with GitHub","text":""},{"location":"github/#contributing-code-with-github","title":"Contributing \ud83c\udf08 Code with GitHub\u00b6","text":"<p>There are oodles of great tutorials on various aspects of contributing to collaborative code projects with GitHub. This page is meant to provide a quick, recipe-like answer to the question \"how do I contribute to the <code>chromatic</code> package?\"</p>"},{"location":"github/#should-i-submit-an-issue-to-the-chromatic-github-repository","title":"Should I submit an Issue to the <code>chromatic</code> GitHub repository?\u00b6","text":"<p>Yes! You tried out <code>chromatic</code> and maybe thought it had some neat features, but you encountered something that didn't work quite how you expected it to, a question that you couldn't find an answer to in the documentation, or a feature that you wished existed. Those would all be great motivations to go to the <code>chromatic</code> GitHub repository to submit an Issue!</p> <p>But...we know you! You're saying to yourself \"Oh, gosh, they have their hands full, I don't want to bother them right now. I'm sure I'm the only one having this problem or with this question about how <code>chromatic</code> works. My idea probably matters only to me and not anyone else. I don't want to make more workf for other folks.\" You're wringing your hands and anxiously worrying about about whether to submit an Issue.</p> <p>Still, can we please encourage you share your problem, ask your question, or make your suggestion? Your experience and curiosity and creativity would be extremely valuable contributions to the package, and make it better for everyone. It's super exciting to hear that someone new else trying to use this code package, and every bit of discussion about how to improve it is super helpful. I promise we're very friendly! So, please, hop on over and submit an Issue!</p>"},{"location":"github/#how-do-we-get-started-with-git-and-github","title":"How do we get started with <code>git</code> and GitHub?\u00b6","text":"<p>Yay! You're interested in contributing some code to the <code>chromatic</code> package. The first step will be to make sure you have a basic familiarity with <code>git</code> and GitHub as tools for safe and collaborative coding.</p> <p>Christina Hedges has written some great resources on coding-related workflows for astronomy, including a tutorial for getting started with <code>git</code> and GitHub that you can watch here. If you're entirely new to these tools, please work through her tutorial and then come back here. If you haven't created one yet, make yourself a GitHub account</p>"},{"location":"github/#how-do-we-contribute-new-code","title":"How do we contribute new code?\u00b6","text":"<p>Because we have more than one person working on <code>chromatic</code> code, let's please use separate <code>git</code> branches for developing new features. Using branches allows us to write code in parallel and merge it together later, without constantly having to make sure that everything everybody writes is up-to-date everywhere all at once. We're generally trying to follow something like the Gitflow Workflow, to allow us to make changes to the shared code that are a little bit buffered from the published code used by non-developers.</p> <p>There three branches you should know about, and only two you should probably interact with:</p> <ul> <li>The <code>main</code> branch hosts the published version of the code for public users. New <code>pip</code> versions of the code will be released from the <code>main</code> branch. Most developers should never interact directly with the <code>main</code> branch.</li> <li>The <code>develop</code> branch is the active branch for shared development. New feature branches should be created from the <code>develop</code> branch and once they're reviewed be merged back into <code>develop</code>. Occasionally, and only after careful testing and documentation edits, the <code>develop</code> branch will get merged into the <code>main</code> branch and published to <code>pip</code>.</li> <li>Your <code>add-amazing-awesome-new-feature</code> branch (where you replace the name with something more specific and informative) is a temporary branch that you created off of <code>develop</code> to add your amazing awesome new feature. You should make your changes and commits to that branch, and when you're ready to discuss to your contribution (either as a draft or a mostly finished product), you should submit a Pull Request from this feature branch into the <code>develop</code> branch. Features should be tested well enough that they won't break <code>develop</code> when they get merged into it (but if they do, possibly due to a temporary conflict with another feature branch, it's OK because the <code>main</code> branch is still safe). Once it's merged, your feature branch will be deleted, and you can start a new one to add a different new feature.</li> </ul> <p>With these branches, here's what writing some new code for <code>chromatic</code> might look like for you. The following describes using <code>git</code> from the Terminal prompt. In practice, you might interact with <code>git</code> mostly through atom, GitHub Desktop, or some other tool.</p> <ol> <li>Discuss your plans in an Issue. You might start from trying to address an existing issue, or you might add a new issue of your own. Either way, it's really helpful to let other folks know \"here's what I'm trying to do\" to avoid duplicate or unfocused efforts. If you're not already a Collaborator on the <code>chromatic</code> repository, we can add you at this point!</li> <li>Use the Installation instructions to compelete the Developer Installation. This will download the <code>develop</code> branch of the <code>chromatic</code> repository onto your computer and set up your environment to point to the repository's directory.</li> </ol> <pre><code>git clone https://github.com/zkbt/chromatic.git\ncd chromatic\npip install -e '.[develop]'\n</code></pre> <ol> <li>Create a new feature branch off of <code>develop</code>. Check out that branch, so that all commits you make will be associated with that branch.</li> </ol> <pre><code>git checkout develop\ngit branch add-amazing-awesome-new-feature\ngit checkout add-amazing-awesome-new-feature\n</code></pre> <ol> <li>Write your code. Follow some of the tips for Designing New \ud83c\udf08 Features to get started, to make sure it imports correctly, and to write some useful tests. Once you've saved some changes to the code, commit those changes to your feature branch. (You can confirm you're on your feature branch by running <code>git branch</code> and seeing which branch has the <code>*</code>.)</li> </ol> <pre><code>git add .\ngit commit -m \"{include informative commit message here}\"\n</code></pre> <p>Up to this point, whatever changes you have committed are still only stored on your computer.</p> <ol> <li>To start sharing your new code, push your branch up to GitHub. The first time you run this push command, you'll probably get some instructions about how to link your local branch to a new remote one that you're about to create; follow them.</li> </ol> <pre><code>git push\n</code></pre> <p>Now your branch and most recently pushed commits should appear in the GitHub list of branches.</p> <ol> <li>To ask for your code to be reviewed, either because you think it's finished or because you've completed enough of a draft to be useful to start discussing, submit a Pull Request asking us to pull the code from your feature branch into <code>develop</code>. We'll probably discuss a few aspects of it and suggest some changes, which can be implemented by continuing to push new commits to your feature branch as long as the Pull Request is still open. Once it's tested and works and we're all happy with it, we'll merge the code into <code>develop</code>, from where it will eventually then be merged into the <code>main</code> branch and released in the latest <code>pip</code> version.</li> <li>\ud83c\udf08\ud83c\udf89\ud83e\udd29 Celebrate!</li> </ol>"},{"location":"github/#what-kinds-of-files-should-we-commit","title":"What kinds of files should we commit?\u00b6","text":"<p>Every file change commit to the repository will be stored and able to be recovered in the future. That's great for being able to go back to previous versions in the code's history, but it means that the repository could very easily get very big if we include lots of large files in our commits. Large files are extra troublesome if they change frequently, because then we're storing a new copy of every large file in our repository.</p> <p>Let's try to keep the <code>chromatic</code> repository relatively slim. To do that, please:</p> <ul> <li>Avoid committing large data, image, or movie files to the repository. If you think you need to include a large file (anything over ~1 MB), raise an Issue to discuss your plans. There might be a better alternative.</li> <li>Avoid committing scratch jupyter notebook files where you're testing out new code. The only notebooks that should be committed to <code>chromatic</code> are ones meant to serve as public documentation; those should be stored in the <code>docs/</code> folder as described in Writing \ud83c\udf08 Documentation and their outputs should be cleared before saving.</li> </ul>"},{"location":"github/#wait-i-have-a-question-thats-not-answered-here","title":"Wait, I have a question that's not answered here!\u00b6","text":"<p>This page is a whirlwind tour! We probably missed lots of important information. If you have a question, no matter how small or large or seemingly basic, please ask Zach or submit an Issue.</p>"},{"location":"installation/","title":"Installation","text":"In\u00a0[1]: Copied! <pre>import chromatic\n\nchromatic.version()\n</pre> import chromatic  chromatic.version() Out[1]: <pre>'0.5.0'</pre> <p>Happy <code>chromatic</code>-ing!</p>"},{"location":"installation/#installation","title":"Installation\u00b6","text":"<p>For installing this code we assume you have a Python environment set up, into which you can install packages via <code>pip</code>. If so, please continue to one of the installation options below.</p> <p>If this isn't the case, we recommend installing the Anaconda Python distribution, and using <code>conda</code> to manage the <code>python</code> environment(s) you have installed on your computer. One tutorial (of many) about how to get started with Python and creating <code>conda</code> environments is available here.</p>"},{"location":"installation/#basic-installation","title":"Basic Installation\u00b6","text":"<p>If you want to install into your current environment, the basic installation should be pretty simple. From the Terminal or Anaconda Prompt, please run</p> <pre><code>pip install chromatic-lightcurves\n</code></pre> <p>and it should install everything, along with all the necessary dependencies.</p> <p>If you previously installed this package and need to grab a newer version, run</p> <pre><code>pip install --upgrade chromatic-lightcurves\n</code></pre> <p>to download any officially released updates.</p>"},{"location":"installation/#basic-installation-in-new-conda-environment","title":"Basic Installation in New <code>conda</code> Environment\u00b6","text":"<p>If you are at all worried about the installation messing up other existing packages on your computer or if you're having trouble getting a tricky dependency to install, please consider installing into a new <code>conda</code> environment. Environments are independent of each other, so what you install into one shouldn't affect others.</p> <p>From the Terminal or Anaconda Prompt, please run</p> <pre><code>conda create -n my-neato-chromatic-environment python=3.10\n</code></pre> <p>to create a new, empty environment centered on a recent-ish version of <code>python</code>. You may want to choose a shorter name for your neato <code>chromatic</code> environment, as it's something you'll need to type every time you want to use this environment. Run</p> <pre><code>conda activate my-neato-chromatic-environment\n</code></pre> <p>to enter than environment. You can check that you're in it by running</p> <pre><code>conda env list\n</code></pre> <p>and looking for a little star next to the environment name. Now, from within this environment, run</p> <pre><code>pip install --upgrade chromatic-lightcurves\n</code></pre> <p>to install <code>chromatic</code> and all its dependencies (or follow the Developer Installation instructions immediately below), into this specific environment.</p> <p>One thing to watch out for is that if you haven't installed whatever tools you use to work with <code>python</code> (such as <code>jupyter</code> or <code>spyder</code>) into this environment, you might not be able to open them or you might open them from your base environment without access to <code>chromatic</code>. To fix that, run</p> <pre><code>conda install jupyter spyder\n</code></pre> <p>from inside your environment. From now on, whenever you want to use this environment, activate it with</p> <pre><code>conda activate my-neato-chromatic-environment\n</code></pre> <p>and then open your <code>python</code> interface from within that environment, as with any one of these</p> <pre><code>jupyter notebook \njupyter lab\nspyder \n</code></pre> <p>Good luck!</p>"},{"location":"installation/#developer-installation","title":"Developer Installation\u00b6","text":"<p>If you want to install this code while being able to edit and develop it, you can clone its GitHub repository onto your own computer. This allows you to edit it for your own sake and/or to draft changes that can be contributed to the public package (see Contributing \ud83c\udf08 Code with GitHub).</p> <p>To install directly as an editable package on your local computer, run</p> <pre><code>git clone https://github.com/zkbt/chromatic.git\ncd chromatic\npip install -e '.[develop]'\n</code></pre> <p>The <code>-e .</code> will point your environment's <code>chromatic</code> package to your local folder, meaning that any changes you make in the repository will be reflected in what Python sees when it tries to <code>import chromatic</code>. Including the <code>[develop]</code> after the <code>.</code> will install both the dependencies for the package itself and the extra dependencies required for development (= testing and documentation).</p>"},{"location":"installation/#did-it-work","title":"Did it work?\u00b6","text":"<p>You can quickly test whether your installation worked, and what version you have, by running the Python code</p>"},{"location":"io/","title":"Reading/Writing a \ud83c\udf08","text":"In\u00a0[1]: Copied! <pre>from chromatic import read_rainbow, version\n</pre> from chromatic import read_rainbow, version In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> In\u00a0[3]: Copied! <pre>rainbow = read_rainbow(\n    \"example-datasets/stsci/jw02734002001_04101_00001-seg00*_nis_x1dints.fits\"\n)\n</pre> rainbow = read_rainbow(     \"example-datasets/stsci/jw02734002001_04101_00001-seg00*_nis_x1dints.fits\" ) <pre>\ud83c\udf08\ud83e\udd16 This file contains data for 2 spectroscopic orders. Because no\n`order=` keyword was supplied, we're defaulting to first order. You can\nhide this warning by expliciting stating which order you want to load.\nFor this file, the options include [1 2].\n\n</pre> <pre>\ud83c\udf08\ud83e\udd16 The 2048 input wavelengths were not monotonically increasing.\n&lt;\ud83c\udf08(2048w, 280t)&gt; has been sorted from lowest to highest wavelength.\nIf you want to recover the original wavelength order, the original\nwavelength indices are available in `rainbow.original_wave_index`.\n\n</pre> <p>Then, to save a file, try just using the <code>.save()</code> method. Again, it will try to guess the file format from the filename.</p> In\u00a0[4]: Copied! <pre>rainbow.save(\"example-datasets/chromatic/ero-transit-wasp-96b.rainbow.npy\")\n</pre> rainbow.save(\"example-datasets/chromatic/ero-transit-wasp-96b.rainbow.npy\") <p>The sections below provide more details on some of the available file formats for reading and writing files, but the basic process is what you've already seen: use <code>read_rainbow()</code> and <code>.save()</code> to load and save spectroscopic light curve data with a variety of formats!</p> <p>Download Example Inputs: If you want to test out any of these readers, you'll need data files in each format to test on. You can download some example datasets from this link. Simply extract that <code>.zip</code> file into the directory from which you'll be running this notebook. Another source of files you might want to try reading would be the simulated data generated for the ers-transit Spring 2022 Data Challenge.</p> In\u00a0[5]: Copied! <pre>r = read_rainbow(\"example-datasets/chromatic/test.rainbow.npy\")\n</pre> r = read_rainbow(\"example-datasets/chromatic/test.rainbow.npy\") <p>The <code>Rainbow</code> reader will try to guess the format of the file from the filepath. If that doesn't work for some reason, in this case you can feed in the keyword <code>format='rainbow_npy'</code>, to require the use of the <code>from_rainbow_npy</code> reader needed for these files.</p> In\u00a0[6]: Copied! <pre>r = read_rainbow(\"example-datasets/chromatic/test.rainbow.fits\")\n</pre> r = read_rainbow(\"example-datasets/chromatic/test.rainbow.fits\") <p>The <code>Rainbow</code> reader will try to guess the format of the file from the filepath. If that doesn't work for some reason, in this case you can feed in the keyword <code>format='rainbow_FITS'</code>, to require the use of the <code>from_rainbow_FITS</code> reader needed for these files.</p> In\u00a0[7]: Copied! <pre>r = read_rainbow(\"example-datasets/chromatic/test.rainbow.txt\")\n</pre> r = read_rainbow(\"example-datasets/chromatic/test.rainbow.txt\") <p>If the file-format guess fails, you can feed in the keyword <code>format='text'</code> to tell the reader to expect one of these files.</p> In\u00a0[8]: Copied! <pre>r = read_rainbow(\"example-datasets/stsci/*_x1dints.fits\")\n</pre> r = read_rainbow(\"example-datasets/stsci/*_x1dints.fits\") <pre>\ud83c\udf08\ud83e\udd16 This file contains data for 2 spectroscopic orders. Because no\n`order=` keyword was supplied, we're defaulting to first order. You can\nhide this warning by expliciting stating which order you want to load.\nFor this file, the options include [1 2].\n\n</pre> <pre>\ud83c\udf08\ud83e\udd16 The 2048 input wavelengths were not monotonically increasing.\n&lt;\ud83c\udf08(2048w, 280t)&gt; has been sorted from lowest to highest wavelength.\nIf you want to recover the original wavelength order, the original\nwavelength indices are available in `rainbow.original_wave_index`.\n\n</pre> <p>If the file-format guess fails, you can feed in the keyword <code>format='x1dints'</code> to tell the reader to expect one of these files. This reader was rewritten on 13 July 2022 to read in the JWST/ERO <code>x1dints</code> datasets. It might not work on earlier simulated <code>x1dints</code> files like those in the simulated datasets available here; for those, try using the <code>format='x1dints_kludge'</code> keyword.</p> In\u00a0[9]: Copied! <pre>s3 = read_rainbow(\"example-datasets/eureka/S3_example_SpecData.h5\")\n</pre> s3 = read_rainbow(\"example-datasets/eureka/S3_example_SpecData.h5\") <pre>\ud83c\udf08\ud83e\udd16 Times are being estimated from the 'BJD_TDB'\nkeyword but being interpreted as \"modified\"\nBJD_TDB (\"modified\" = BJD_TDB - 2400000.5).\nThis accounts for an earlier version of Eureka;\nin the latest version times should likely be\nlisted as 'BMJD_TDB' to be more honest about\nthe fact that they're modified.\n\nIf we're interpreting times wrongly, please\nraise an issue on the chromatic github!\n\n</pre> In\u00a0[10]: Copied! <pre>s4 = read_rainbow(\"example-datasets/eureka/S4_example_LCData.h5\")\n</pre> s4 = read_rainbow(\"example-datasets/eureka/S4_example_LCData.h5\") <pre>\ud83c\udf08\ud83e\udd16 Times are being estimated from the 'BJD_TDB'\nkeyword but being interpreted as \"modified\"\nBJD_TDB (\"modified\" = BJD_TDB - 2400000.5).\nThis accounts for an earlier version of Eureka;\nin the latest version times should likely be\nlisted as 'BMJD_TDB' to be more honest about\nthe fact that they're modified.\n\nIf we're interpreting times wrongly, please\nraise an issue on the chromatic github!\n\n</pre> In\u00a0[11]: Copied! <pre>s5 = read_rainbow(\"example-datasets/eureka/S5*Table_Save_*.txt\")\n</pre> s5 = read_rainbow(\"example-datasets/eureka/S5*Table_Save_*.txt\") <p>If the file-format guess fails, you can feed in the keywords <code>format='eureka_s3'</code>, <code>format='eureka_s4'</code>, or <code>format='eureka_s5'</code> to tell the reader what file(s) to expect. (Older versions of Eureka! used text files for earlier stages, with filenames like <code>S3_*_Table_Save.txt</code>; that format will continue work with <code>format='eureka_txt'</code>.)</p> In\u00a0[12]: Copied! <pre>spectra = read_rainbow(\"example-datasets/xarray/stellar-spec.xc\")\n</pre> spectra = read_rainbow(\"example-datasets/xarray/stellar-spec.xc\") In\u00a0[13]: Copied! <pre>raw_lcs = read_rainbow(\"example-datasets/xarray/raw-light-curves.xc\")\n</pre> raw_lcs = read_rainbow(\"example-datasets/xarray/raw-light-curves.xc\") In\u00a0[14]: Copied! <pre>fitted_lcs = read_rainbow(\"example-datasets/xarray/fitted-light-curves.xc\")\n</pre> fitted_lcs = read_rainbow(\"example-datasets/xarray/fitted-light-curves.xc\") <p><code>chromatic</code> can write out files in a variety of different file formats. By pairing with the available readers, this makes it possible to effectively switch one file format to another, simply by reading one file in and saving it out as another. To demonstrate the readers, let's create a simple simulated dataset.</p> In\u00a0[15]: Copied! <pre>from chromatic import SimulatedRainbow\n</pre> from chromatic import SimulatedRainbow In\u00a0[16]: Copied! <pre>simulated = SimulatedRainbow().inject_transit().inject_systematics().inject_noise()\n</pre> simulated = SimulatedRainbow().inject_transit().inject_systematics().inject_noise() In\u00a0[17]: Copied! <pre>simulated.save(\"example-datasets/chromatic/test.rainbow.npy\")\n</pre> simulated.save(\"example-datasets/chromatic/test.rainbow.npy\") In\u00a0[18]: Copied! <pre>simulated.save(\"example-datasets/chromatic/test.rainbow.fits\")\n</pre> simulated.save(\"example-datasets/chromatic/test.rainbow.fits\") <pre>WARNING: VerifyWarning: Keyword name 'injected_transit_method' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\nWARNING: VerifyWarning: Keyword name 'injected_transit_parameters' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n\ud83c\udf08\ud83e\udd16 metadata item 'injected_transit_parameters' cannot be saved to FITS header\n\nWARNING: VerifyWarning: Keyword name 'systematics_components' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n\ud83c\udf08\ud83e\udd16 metadata item 'systematics_components' cannot be saved to FITS header\n\nWARNING: VerifyWarning: Keyword name 'systematics_equation' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n\ud83c\udf08\ud83e\udd16 metadata item 'systematics_equation' cannot be saved to FITS header\n\nWARNING: VerifyWarning: Keyword name 'signal_to_noise' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n</pre> In\u00a0[19]: Copied! <pre>spectra = simulated.save(\"example-datasets/xarray/stellar-spec.xc\")\n</pre> spectra = simulated.save(\"example-datasets/xarray/stellar-spec.xc\") <pre>\ud83c\udf08\ud83e\udd16 The required metadata keyword `author` was not found.\nBefore saving, please set it with `rainbow.author = ?`\n\n\ud83c\udf08\ud83e\udd16 The required metadata keyword `contact` was not found.\nBefore saving, please set it with `rainbow.contact = ?`\n\n\ud83c\udf08\ud83e\udd16 The required metadata keyword `code` was not found.\nBefore saving, please set it with `rainbow.code = ?`\n\n</pre> In\u00a0[20]: Copied! <pre>raw_lcs = simulated.save(\"example-datasets/xarray/raw-light-curves.xc\")\n</pre> raw_lcs = simulated.save(\"example-datasets/xarray/raw-light-curves.xc\") <pre>\ud83c\udf08\ud83e\udd16 The required metadata keyword `data_origin` was not found.\nBefore saving, please set it with `rainbow.data_origin = ?`\n\n</pre> In\u00a0[21]: Copied! <pre>fitted_lcs = simulated.save(\"example-datasets/xarray/fitted-light-curves.xc\")\n</pre> fitted_lcs = simulated.save(\"example-datasets/xarray/fitted-light-curves.xc\") In\u00a0[22]: Copied! <pre>simulated.save(\"example-datasets/chromatic/test.rainbow.txt\")\n</pre> simulated.save(\"example-datasets/chromatic/test.rainbow.txt\") In\u00a0[23]: Copied! <pre>from chromatic import available_readers, available_writers\n</pre> from chromatic import available_readers, available_writers In\u00a0[24]: Copied! <pre>list(available_readers)\n</pre> list(available_readers) Out[24]: <pre>['from_x1dints',\n 'from_x1dints_kludge',\n 'from_eureka_S3_txt',\n 'from_eureka_SpecData',\n 'from_eureka_S3',\n 'from_eureka_LCData',\n 'from_eureka_S4',\n 'from_eureka_channels',\n 'from_eureka_S5',\n 'from_rainbow_npy',\n 'from_rainbow_FITS',\n 'from_text',\n 'from_xarray_stellar_spectra',\n 'from_xarray_raw_light_curves',\n 'from_xarray_fitted_light_curves',\n 'from_nres',\n 'from_atoca',\n 'from_espinoza',\n 'from_dossantos',\n 'from_feinstein_numpy',\n 'from_feinstein_h5',\n 'from_schlawin',\n 'from_coulombe',\n 'from_kirk_fitted_light_curves',\n 'from_kirk_stellar_spectra',\n 'from_radica',\n 'from_aylin',\n 'from_carter_and_may']</pre> In\u00a0[25]: Copied! <pre>list(available_writers)\n</pre> list(available_writers) Out[25]: <pre>['to_rainbow_npy',\n 'to_rainbow_FITS',\n 'to_xarray_stellar_spectra',\n 'to_xarray_raw_light_curves',\n 'to_xarray_fitted_light_curves',\n 'to_text']</pre> <p>If you would like help implementing a new reader/writer and/or incorporating your format as a default for <code>chromatic</code>, please consider submitting an Issue!</p>"},{"location":"io/#readingwriting-a","title":"Reading/Writing a \ud83c\udf08\u00b6","text":"<p>This page describes how to read and/or write <code>Rainbow</code> objects, using a variety of format definitions that have been included with the main <code>chromatic</code> package.</p>"},{"location":"io/#quickstart","title":"Quickstart\u00b6","text":"<p>To get started reading files, if you have a file that you think contains flux as a function of wavelength and time (\"time-series spectra\" or \"multiwavelength light curves\" or some such), try just using the default <code>read_rainbow</code> function. It will try to guess the file format from the file name.</p>"},{"location":"io/#reading-files","title":"Reading Files\u00b6","text":"<p><code>chromatic</code> can load data from a variety of different file formats. Whether these are time-series spectra or binned spectroscopic light curves, there's a good chance that the <code>read_rainbow</code> function might be able to load them into a \ud83c\udf08. By writing custom readers for different data formats, we hope to make it easier to use <code>chromatic</code> to compare the results of different analyses.</p>"},{"location":"io/#chromatic-rainbow-files-rainbownpy","title":"<code>chromatic</code> rainbow files (<code>*.rainbow.npy</code>)\u00b6","text":"<p>The <code>chromatic</code> toolkit saves files in its own default format, which can then be shared and loaded back in. These files directly encode the core dictionaries in binary files, so they load and save quickly. They have the extension <code>.rainbow.npy</code> and can be written from any <code>Rainbow</code> object.</p>"},{"location":"io/#chromatic-rainbow-fits-files-rainbowfits","title":"<code>chromatic</code> rainbow FITS files (<code>*.rainbow.fits</code>)\u00b6","text":"<p>Because you might want to share a <code>Rainbow</code> object with someone not using Python, we define a FITS-based file format. The Flexible Image Transport System is common in astronomy, so there's a good chance someone will be able to load this file into whatever coding language they're using. These files have the extension <code>.rainbow.fits</code>, and they will load a tiny bit more slowly than <code>.rainbow.npy</code> files; they can be written from any <code>Rainbow</code> object.</p>"},{"location":"io/#generic-text-files-txt-csv","title":"generic text files (<code>*.txt</code>, <code>*.csv</code>)\u00b6","text":"<p>Text files are slower to read or write, but everyone can make them. This reader will try to load one giant text file in which light curves for all wavelengths are stacked on top of each other or spectra for all times are stacked on top of each other. The text file should at least have columns that look like:</p> <ul> <li><code>wavelength</code> for wavelength in microns</li> <li><code>time</code> for time in days (preferably BJD$_{\\rm TDB}$)</li> <li><code>flux</code> for flux in any units</li> <li><code>uncertainty</code> for flux uncertainties in the same units as <code>flux</code> Additional columns will also be read, and they will be stored in the <code>.fluxlike</code> core dictionary.</li> </ul>"},{"location":"io/#stsci-jwst-pipeline-outputs-x1dintsfits","title":"STScI <code>jwst</code> pipeline outputs (<code>x1dints.fits</code>)\u00b6","text":"<p>The <code>jwst</code> pipeline developed at the Space Telescope Science Institute will produce extract 1D stellar spectra for time-series observations with the James Webb Space Telescope. Details about the pipeline itself are available here.</p> <p>These files typically end with the <code>_x1dints.fits</code> suffix. Each file contains a number of individual \"integrations\" (= time points). Because the datasets can get large, sometimes a particular observation might be split into multiple segments, each with its own file. As such, the reader for these files is designed to handle either a single file or a path with a <code>*</code> in it that points to a group of files from an observation that's been split into segments.</p>"},{"location":"io/#eureka-pipeline-outputs-s345h5txt","title":"<code>eureka</code> pipeline outputs (<code>S[3|4|5].[h5|txt]</code>)\u00b6","text":"<p>The Eureka! pipeline is one of many community tools being designed to extract spectra from JWST data. The current outputs have filenames that look like <code>S3*SpecData.h5</code> for Stage 3 (extracted spectra), <code>S4*LCData.h5</code> for Stage 4 (raw binned light curves), and a group of files <code>*S5_*_Table_Save_*.txt</code> for Stage 5 (fitted binned light curves) for all channels. Any of these three stages can be read with <code>chromatic</code>.</p>"},{"location":"io/#xarray-based-ers-format-xc","title":"<code>xarray</code>-based ERS format (<code>*.xc</code>)\u00b6","text":"<p>Natasha Batalha, Lili Alderson, Munazza Alam, and Hannah Wakeford put together some specifications for a standard format for publishing datasets. The details may still change a little bit (as of 13 July 2022), but <code>chromatic</code> can currently read a version their <code>stellar-spec</code>, <code>raw-light-curves</code>, and <code>fitted-light-curves</code> formats.</p>"},{"location":"io/#writing-files","title":"Writing Files\u00b6","text":""},{"location":"io/#chromatic-rainbow-files-rainbownpy","title":"<code>chromatic</code> rainbow files (<code>*.rainbow.npy</code>)\u00b6","text":"<p>The default file format for saving files encodes the core dictionaries in binary files, using the extension <code>.rainbow.npy</code>. This is a file that can be read directly back into <code>chromatic</code>. (Indeed, the commands below created the file that we read above.)</p>"},{"location":"io/#chromatic-rainbow-fits-files-rainbowfits","title":"<code>chromatic</code> rainbow FITS files (<code>*.rainbow.fits</code>)\u00b6","text":"<p>If you want to share your Rainbow object with someone who might not be using Python, consider sharing a <code>.rainbow.fits</code> file. This is a normal FITS file that many astronomers will have a way of reading. The primary extension has no data but a header that might contain some metadata. The three other extensions <code>fluxlike</code>, <code>wavelike</code>, and <code>timelike</code> contain quantities that have shapes of <code>(nwave, ntime)</code>, <code>(nwave)</code>, <code>(ntime)</code>, respectively.</p>"},{"location":"io/#xarray-based-ers-format-xc","title":"<code>xarray</code>-based ERS format (<code>*.xc</code>)\u00b6","text":"<p><code>chromatic</code> can write out to the standard <code>xarray</code>-based format described above. These writers will generally raise warnings if important metadata is missing.</p>"},{"location":"io/#generic-text-files-txt-csv","title":"generic text files (<code>*.txt</code>, <code>*.csv</code>)\u00b6","text":"<p>Text files provide a more generally readable file format, even though they may be slower to read or write. This writer will create one giant text file that stacks the light curves for all wavelengths on top of each other (if the <code>group_by='wavelength'</code> keyword is set) or the spectra for all times on top of each other (if the <code>group_by='time'</code> keyword is set). The resulting text file should at least have columns that look like:</p> <ul> <li><code>wavelength</code> for wavelength in microns</li> <li><code>time</code> for time in days (preferably BJD$_{\\rm TDB}$)</li> <li><code>flux</code> for flux in any units</li> <li><code>uncertainty</code> for flux uncertainties in the same units as <code>flux</code></li> </ul>"},{"location":"io/#other-file-formats","title":"Other File Formats\u00b6","text":"<p>Naturally, you might want to use other readers or writers than have already been listed here, to be able to interpret outputs from other analyses or to output the inputs needed for various light curve analyses. We've already added a number of custom readers and writers. Here are the currently available file formats:</p>"},{"location":"io/#adding-a-custom-reader","title":"Adding a Custom Reader\u00b6","text":"<p>You might want to create a new reader or writer, to allow chromatic to interact with your own datasets or tools. To facilitate this, templates are available with human-friendly instructions for how to add a new reader or writer.</p> <p>If you want to try to incorporate a new format, modify the templates for a reader or writer to create your own <code>from_abcdefgh</code> or <code>to_abcdefgh</code> functions. These functions can be passed directly the <code>format=</code> keyword for <code>read_rainbow(filepath, format=from_abcdefgh)</code> or <code>rainbow.save(filepath, format=to_abcdefgh)</code>.</p>"},{"location":"models/","title":"\ud83c\udf08 Models","text":"In\u00a0[1]: Copied! <pre>from chromatic import SimulatedRainbow, read_rainbow, version\nfrom chromatic import plt, np, u\n</pre> from chromatic import SimulatedRainbow, read_rainbow, version from chromatic import plt, np, u In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> <p>If we read in a <code>Rainbow</code> from a data file, it might not have a <code>model</code> set yet. We can attach an array of model values and turn a <code>Rainbow</code> into a <code>RainbowWithModel</code> object using the <code>.attach_model()</code> method.</p> In\u00a0[3]: Copied! <pre>data = read_rainbow(\"example-datasets/chromatic/ero-transit-wasp-96b.rainbow.npy\")\ndata\n</pre> data = read_rainbow(\"example-datasets/chromatic/ero-transit-wasp-96b.rainbow.npy\") data Out[3]: <pre>&lt;\ud83c\udf08(2048w, 280t)&gt;</pre> In\u00a0[4]: Copied! <pre>data_with_model = data.attach_model(\n    model=np.ones_like(data.flux),\n    planet_model=np.ones_like(data.flux),\n    systematics_model=np.ones_like(data.flux),\n)\ndata_with_model\n</pre> data_with_model = data.attach_model(     model=np.ones_like(data.flux),     planet_model=np.ones_like(data.flux),     systematics_model=np.ones_like(data.flux), ) data_with_model Out[4]: <pre>&lt;\ud83c\udf08WithModel(2048w, 280t)&gt;</pre> <p>The <code>.attach_model()</code> function requires at least the overall <code>model</code> be supplied; this is an array meant to represent what our <code>flux</code> would look like if there were no noise. We can also add additional model components, like <code>planet_model</code> and <code>systematics_model</code> in the example above, to be able to track and visualize them separately. Obviously, \"ones everywhere\" is probably not good a good model for a real dataset, so you'd probably want to replace the <code>np.ones_like</code> above with something like the outputs from a model optimization or sampling routine.</p> <p>If we created a simulated dataset with a <code>SimulatedRainbow()</code> object, the model behind that simulation is automatically stored inside the object. Let's generate a simulated dataset and use it as an example.</p> In\u00a0[5]: Copied! <pre>simulated = SimulatedRainbow().inject_transit().inject_systematics().inject_noise()\nsimulated\n</pre> simulated = SimulatedRainbow().inject_transit().inject_systematics().inject_noise() simulated Out[5]: <pre>&lt;Simulated\ud83c\udf08(231w, 150t)&gt;</pre> <p>The <code>SimulatedRainbow()</code> object inherits from the <code>RainbowWithModel</code> object, so it has all its powers. This includes a <code>.residuals</code> property, that automatically calculates <code>flux</code> - <code>model</code> based on their current values.</p> In\u00a0[6]: Copied! <pre>simulated.residuals\n</pre> simulated.residuals Out[6]: <pre>array([[-1.54274185e-02, -2.11613930e-02,  1.96794302e-02, ...,\n        -1.42413591e-02,  1.08995097e-02,  1.40988609e-02],\n       [ 1.39588135e-02, -4.40468012e-03,  2.14354362e-04, ...,\n         8.79747818e-03,  6.80077828e-03, -8.49484703e-04],\n       [ 8.79950190e-03, -1.17793728e-02, -5.08046235e-03, ...,\n         2.86677782e-03,  2.83041194e-02,  6.84105191e-03],\n       ...,\n       [-6.18526011e-04, -8.93859761e-03,  1.61111510e-02, ...,\n         7.66116728e-05, -6.01644189e-03,  2.65801170e-02],\n       [ 1.75553107e-02,  3.56492253e-03,  4.50526766e-03, ...,\n         7.58031271e-03,  5.89232071e-03,  5.03282861e-03],\n       [-1.02146302e-02,  1.51226223e-02, -8.18132259e-03, ...,\n         4.21944172e-03,  2.64228744e-03, -5.80961084e-03]],\n      shape=(231, 150))</pre> <p>In the case of our simulation, we shouldn't be surprised that the model is a good fit and the <code>residuals</code> look like they are drawn from a zero-mean normal distribution characterized by the <code>uncertainty</code></p> In\u00a0[7]: Copied! <pre>plt.figure(figsize=(8, 2))\nplt.hist((simulated.residuals / simulated.uncertainty).flatten(), bins=100)\nplt.xlabel(\"(data - model)/uncertainty\");\n</pre> plt.figure(figsize=(8, 2)) plt.hist((simulated.residuals / simulated.uncertainty).flatten(), bins=100) plt.xlabel(\"(data - model)/uncertainty\"); In\u00a0[8]: Copied! <pre>simulated.paint_with_models();\n</pre> simulated.paint_with_models(); <p>This defaults to showing the model components <code>systematics_model</code> and <code>planet_model</code>, but we can change which models get displayed with the <code>models=</code> keyword argument. This won't change the residuals, which are always calculated as <code>flux</code> - <code>model</code>. The example below shows a few more common keyword arguments we often want to change.</p> In\u00a0[9]: Copied! <pre>simulated.paint_with_models(models=[\"model\"], xaxis=\"wavelength\", cmap=\"gray\");\n</pre> simulated.paint_with_models(models=[\"model\"], xaxis=\"wavelength\", cmap=\"gray\"); In\u00a0[10]: Copied! <pre>fi, ax = plt.subplots(1, 2, sharey=True, sharex=True)\nbinned = simulated.bin(R=5)\nbinned.plot_with_model(ax=ax[0])\n(binned / binned.systematics_model).plot_with_model(ax=ax[1]);\n</pre> fi, ax = plt.subplots(1, 2, sharey=True, sharex=True) binned = simulated.bin(R=5) binned.plot_with_model(ax=ax[0]) (binned / binned.systematics_model).plot_with_model(ax=ax[1]); <p>We can also plot the residuals from the model along the side.</p> In\u00a0[11]: Copied! <pre>binned.plot_with_model_and_residuals();\n</pre> binned.plot_with_model_and_residuals(); In\u00a0[12]: Copied! <pre>binned.animate_with_models();\n</pre> binned.animate_with_models(); <pre>&lt;IPython.core.display.Image object&gt;</pre> <p>That animation function calls a helper function that you might want to use on its own, if you want to make a multicomponent lightcurve plot for a single wavelength. Here, it's being used to plot the first wavelength in the \ud83c\udf08.</p> In\u00a0[13]: Copied! <pre>binned.plot_one_wavelength_with_models(i_wavelength=0);\n</pre> binned.plot_one_wavelength_with_models(i_wavelength=0);"},{"location":"models/#models","title":"\ud83c\udf08 Models\u00b6","text":"<p>Often, we want to compare the data of a \ud83c\udf08 to a model for the flux as a function of wavelength and time. <code>chromatic</code> provides a few tools to simplify performing and visualizing these comparisons. This page provides a quick tour of some of those features.</p>"},{"location":"models/#creating-a-withmodel-object","title":"Creating a <code>\ud83c\udf08WithModel</code> object\u00b6","text":"<p>All <code>Rainbow</code> objects guarantee access to 5 core quantities (<code>wavelength</code>, <code>time</code>, <code>flux</code>, <code>uncertainty</code>, <code>ok</code>). The <code>RainbowWithModel</code> object adds a <code>model</code> quantity (with the same shape as <code>flux</code>) to this list and provides new functions that make use of that model.</p>"},{"location":"models/#calculating-model-residuals","title":"Calculating \ud83c\udf08 model residuals\u00b6","text":""},{"location":"models/#visualizing-model-comparisons","title":"Visualizing \ud83c\udf08 model comparisons\u00b6","text":"<p>A few helpers exist for visualizing comparisons between model and data. Because they are trying to do a lot, these generally take lots of optional keyword arguments, but we've tried to make the defaults a pretty as possible.</p>"},{"location":"models/#paint_with_models","title":".paint_with_models()\u00b6","text":"<p>If we have a lot of data, the most compact way to visualize it is often to show a 2D map of the data or model flux as a function of time and wavelength.</p>"},{"location":"models/#plot_with_model","title":"<code>.plot_with_model()</code>\u00b6","text":"<p>If our data are binned down to a small number of wavelengths, then it might work to plot the data to model comparison as transit light curves. The left panel below plots the data with the complete model, both of which still contain the systematics. The right panel has divided out both the <code>flux</code> and <code>model</code> arrays by the systematics model, thus making the transit much clearer.</p>"},{"location":"models/#animate_with_models","title":"<code>.animate_with_models()</code>\u00b6","text":"<p>Finally, if we want to be able to look at multiple model components with light curve plots for many wavelengths, the simplest way might be to make an animation that flips through wavelength.</p>"},{"location":"models/#other-visualizations","title":"other visualizations\u00b6","text":"<p>Please remember you can always check for what other visualization or actions are available for a given \ud83c\udf08 with the <code>.help()</code> method. All the \ud83c\udf08 Visualizations methods still work for \ud83c\udf08 with models attached!</p> <p>If you would like another kind of a visualization that isn't shown here, please submit an Issue to discuss it.</p>"},{"location":"quickstart/","title":"\ud83c\udf08 Quickstart","text":"In\u00a0[1]: Copied! <pre>from chromatic import download_from_mast, read_rainbow, version\nfrom astroquery.mast import Observations\nimport astropy.units as u\n</pre> from chromatic import download_from_mast, read_rainbow, version from astroquery.mast import Observations import astropy.units as u In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> In\u00a0[3]: Copied! <pre>downloaded = download_from_mast(\n    proposal_id=\"2734\", instrument_name=\"NIRISS/SOSS\", target_name=\"WASP-96\"\n)\n</pre> downloaded = download_from_mast(     proposal_id=\"2734\", instrument_name=\"NIRISS/SOSS\", target_name=\"WASP-96\" ) <pre>INFO: Found cached file ./mastDownload/JWST/jw02734002001_04101_00001-seg001_nis/jw02734002001_04101_00001-seg001_nis_x1dints.fits with expected size 55373760. [astroquery.query]\n</pre> <pre>INFO: Found cached file ./mastDownload/JWST/jw02734002001_04101_00001-seg002_nis/jw02734002001_04101_00001-seg002_nis_x1dints.fits with expected size 55373760. [astroquery.query]\n</pre> <pre>INFO: Found cached file ./mastDownload/JWST/jw02734002001_04101_00001-seg003_nis/jw02734002001_04101_00001-seg003_nis_x1dints.fits with expected size 44308800. [astroquery.query]\n</pre> In\u00a0[4]: Copied! <pre>downloaded\n</pre> downloaded Out[4]: Table length=3 Local PathStatusMessageURL str106str8objectobject ./mastDownload/JWST/jw02734002001_04101_00001-seg001_nis/jw02734002001_04101_00001-seg001_nis_x1dints.fitsCOMPLETENoneNone ./mastDownload/JWST/jw02734002001_04101_00001-seg002_nis/jw02734002001_04101_00001-seg002_nis_x1dints.fitsCOMPLETENoneNone ./mastDownload/JWST/jw02734002001_04101_00001-seg003_nis/jw02734002001_04101_00001-seg003_nis_x1dints.fitsCOMPLETENoneNone In\u00a0[5]: Copied! <pre>filenames = downloaded[\"Local Path\"]\nrainbow = read_rainbow(filenames)\n</pre> filenames = downloaded[\"Local Path\"] rainbow = read_rainbow(filenames) <pre>\ud83c\udf08\ud83e\udd16 This file contains data for 2 spectroscopic orders. Because no\n`order=` keyword was supplied, we're defaulting to first order. You can\nhide this warning by expliciting stating which order you want to load.\nFor this file, the options include [1 2].\n\n</pre> <pre>\ud83c\udf08\ud83e\udd16 The 2048 input wavelengths were not monotonically increasing.\n&lt;\ud83c\udf08(2048w, 280t)&gt; has been sorted from lowest to highest wavelength.\nIf you want to recover the original wavelength order, the original\nwavelength indices are available in `rainbow.original_wave_index`.\n\n</pre> <p>The \ud83c\udf08 object we just loaded provides easy access to the different dimensions we might want from the dataset, arrays like wavelength, time, flux, or uncertainty. If appropriate, quantities will have <code>astropy</code> Units.</p> In\u00a0[6]: Copied! <pre>rainbow.wavelength\n</pre> rainbow.wavelength Out[6]:  $[0.84693545,~0.84781346,~0.84869148,~\\dots,~2.830602,~2.831589,~2.8325759] \\; \\mathrm{\\mu m}$  In\u00a0[7]: Copied! <pre>rainbow.time\n</pre> rainbow.time Out[7]:  $[2459751.7,~2459751.7,~2459751.7,~\\dots,~2459751.9,~2459751.9,~2459751.9] \\; \\mathrm{d}$  In\u00a0[8]: Copied! <pre>rainbow.flux\n</pre> rainbow.flux Out[8]:  $[[{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  \\dots,~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}]] \\; \\mathrm{MJy}$  In\u00a0[9]: Copied! <pre>rainbow.uncertainty\n</pre> rainbow.uncertainty Out[9]:  $[[{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  \\dots,~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}],~  [{\\rm NaN},~{\\rm NaN},~{\\rm NaN},~\\dots,~{\\rm NaN},~{\\rm NaN},~{\\rm NaN}]] \\; \\mathrm{MJy}$  <p>The absolute flux doesn't matter that much for many transit analyses, so let's use <code>.normalize()</code> to normalize out the median spectrum of the star, converting the data to relative brightness within each wavelength. This \ud83c\udf08 action returns another \ud83c\udf08 object, just with the brightness normalized.</p> In\u00a0[10]: Copied! <pre>normalized = rainbow.normalize()\n</pre> normalized = rainbow.normalize() <p>The dataset is really large, so for making some simple visualization it might help to average over bins of wavelength and/or time. Let's use <code>.bin()</code> to bin onto a (logarithmically) uniform wavelength grid, returning the binned \ud83c\udf08.</p> In\u00a0[11]: Copied! <pre>binned = normalized.bin(R=200, dt=4 * u.minute)\n</pre> binned = normalized.bin(R=200, dt=4 * u.minute) <pre>/Users/zabe0091/miniconda3/envs/exoatlas/lib/python3.13/site-packages/astropy/units/quantity.py:1901: RuntimeWarning: All-NaN slice encountered\n  result = super().__array_function__(function, types, args, kwargs)\n</pre> <p>The times in this dataset are measured relative to some arbitrary time in the distant past. To make them easier to interpret we can phase-fold the times so they're measured relative to the mid-transit time, when the planet is directly between the star and us, according to the planet's orbital properties from the NASA Exoplanet Archive. Let's use <code>.fold()</code> to change the times, returning a phase-folded \ud83c\udf08.</p> In\u00a0[12]: Copied! <pre>folded = binned.fold(period=3.4252602 * u.day, t0=2456258.0621 * u.day)\n</pre> folded = binned.fold(period=3.4252602 * u.day, t0=2456258.0621 * u.day) <p>We can visualize the dataset by making a map of the star's brightness across both wavelength and time, an image in which each the brightness along row corresponds to a transit light curve at that wavelength. Let's use <code>.paint()</code> to create this map for the normalized, binned, folded \ud83c\udf08.</p> In\u00a0[13]: Copied! <pre>folded.paint();\n</pre> folded.paint(); <p>It might be nice to look closely at the light curves within a particular wavelength range. Let's use <code>.imshow_interact()</code> to interactively explore the \ud83c\udf08. Click and drag on the panel on the left to select the wavelegnth range to display as a light curve on the right.</p> In\u00a0[14]: Copied! <pre>folded.imshow_interact()\n</pre> folded.imshow_interact() <p>What a dataset! It looks like there's something a little odd happening at about 2 microns (probably contamination from another star or spectrograph order) and a starspot crossing just after mid-transit, but otherwise it's a remarkably beautiful transit from a very impressive telescope!</p> In\u00a0[15]: Copied! <pre>(\n    rainbow.normalize()\n    .flag_outliers()\n    .bin(R=10)\n    .fold(period=3.4252602 * u.day, t0=2456258.0621 * u.day)\n    .plot()\n);\n</pre> (     rainbow.normalize()     .flag_outliers()     .bin(R=10)     .fold(period=3.4252602 * u.day, t0=2456258.0621 * u.day)     .plot() ); <pre>/Users/zabe0091/miniconda3/envs/exoatlas/lib/python3.13/site-packages/astropy/units/quantity.py:1901: RuntimeWarning: All-NaN slice encountered\n  result = super().__array_function__(function, types, args, kwargs)\n</pre> In\u00a0[16]: Copied! <pre>rainbow.save(\"jwst-wasp96b.rainbow.npy\")\n</pre> rainbow.save(\"jwst-wasp96b.rainbow.npy\") In\u00a0[17]: Copied! <pre>rainbow.help()\n</pre> rainbow.help() <pre>\nHooray for you! You asked for help on what you can do\nwith this \ud83c\udf08 object. Here's a quick reference of a few\navailable options for things to try.\n\n-----------\n| actions |\n-----------\n\n\ud83c\udf08\ud83e\uddee\ud83d\udcdd | +-*/                         \n   Do basic math operations with two Rainbows.\n\ud83c\udf08\ud83d\uddc2\ud83d\udd2a | .[:,:]()                     \n   Index, slice, or mask a Rainbow to get a subset.\n\ud83c\udf08\ud83d\udea7\ud83c\udf0a | .align_wavelengths()         \n   Align spectra with different wavelength grids onto one shared axis.\n\ud83c\udf08\ud83e\uddfa\ud83e\uddf1 | .bin()                       \n   Bin to a new wavelength or time grid.\n\ud83c\udf08\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1\ud83c\udf08 | .compare()                   \n   Connect to other \ud83c\udf08 objects for easy comparison.\n\ud83c\udf08\ud83d\udc08\u23f0 | .concatenate_in_time()       \n   Stitch together two Rainbows with identical wavelengths.\n\ud83c\udf08\ud83d\udc08\ud83c\udf0a | .concatenate_in_wavelength() \n   Stitch together two Rainbows with identical times.\n\ud83c\udf08\ud83d\udea9\ud83d\udc40 | .flag_outliers()             \n   Flag outlier data points.\n\ud83c\udf08\u23f2\ud83c\udf9e | .fold()                      \n   Fold times relative to a particular period and epoch.\n\ud83c\udf08\ud83e\uddfa\u23f0 | .get_average_lightcurve_as_rainbow() \n   Bin down to a single integrated light curve.\n\ud83c\udf08\ud83e\uddfa\ud83c\udf0a | .get_average_spectrum_as_rainbow() \n   Bin down to a single integrated spectrum.\n\ud83c\udf08\ud83c\udfa7\ud83c\udfb2 | .inject_noise()              \n   Inject (uncorrelated, simple) random noise.\n\ud83c\udf08\ud83c\udfa7\ud83c\udfb9 | .inject_systematics()        \n   Inject (correlated, wobbly) systematic noise.\n\ud83c\udf08\u2b50\ufe0f\ud83d\udc7b | .inject_spectrum()           \n   Inject a static stellar spectrum.\n\ud83c\udf08\ud83e\ude90\ud83d\ude9e | .inject_transit()            \n   Inject a transit signal.\n\ud83c\udf08\ud83e\uded3\ud83d\ude11 | .normalize()                 \n   Normalize by dividing through by a typical spectrum (and/or light curve).\n\ud83c\udf08\ud83c\udff4\u200d\u2620\ufe0f\ud83d\udec1 | .remove_trends()             \n   Remove smooth trends in time and/or wavelength.\n\ud83c\udf08\ud83d\ude87\ud83c\udf0a | .shift()                     \n   Doppler shift wavelengths.\n\ud83c\udf08\ud83c\udf71\ud83d\udc87 | .trim()                      \n   Trim away wavelengths or times.\n\n----------------\n| get/timelike |\n----------------\n\n\u23f0\ud83d\udc0b\ud83c\udf08 | .get_average_lightcurve()    \n   Get the weighted average light curve.\n\u23f0\ud83d\udd0e\ud83c\udf08 | .get_for_time()              \n   Get a quantity associated with a time index.\n\u23f0\ud83d\udd96\ud83c\udf08 | .get_median_lightcurve()     \n   Get the median light curve.\n\u23f0\ud83d\udc4c\ud83c\udf08 | .get_ok_data_for_time()      \n   Get a quantity associated with a time index.\n\u23f0\ud83d\udef0\ud83c\udf08 | .get_times_as_astropy()      \n   Get the times as an astropy Time object.\n\u23f0\ud83d\ude80\ud83c\udf08 | .set_times_from_astropy()    \n   Set the times from an astropy Time object (modifies in-place).\n\n----------------\n| get/wavelike |\n----------------\n\n\ud83c\udf0a\ud83d\udc0b\ud83c\udf08 | .get_average_spectrum()      \n   Get the weighted average spectrum.\n\ud83c\udf0a\ud83d\udd0e\ud83c\udf08 | .get_for_wavelength()        \n   Get a quantity associated with a wavelength index.\n\ud83c\udf0a\ud83c\udfaf\ud83c\udf08 | .get_measured_scatter()      \n   Get the measured scatter on the time series for each wavelength.\n\ud83c\udf0a\ud83d\udd96\ud83c\udf08 | .get_median_spectrum()       \n   Get the median spectrum.\n\ud83c\udf0a\ud83d\udc4c\ud83c\udf08 | .get_ok_data_for_wavelength() \n   Get a quantity associated with a wavelength index.\n\ud83c\udf0a\ud83d\udc8e\ud83c\udf08 | .get_spectral_resolution()   \n   Get the spectral resolution (R=w/dw).\n\n-----------\n| helpers |\n-----------\n\n\ud83d\ude4b\ud83c\udf08\ud83d\udcc4 | .help()                      \n   Get one-line help summaries of available methods.\n\ud83d\udcd3\ud83c\udf08\ud83e\udeb5 | .history()                   \n   Get the history that went into this Rainbow.\n\ud83d\udcbe\ud83c\udf08\ud83d\udcfc | .save()                      \n   Save this Rainbow out to a permanent file.\n\n------------------\n| visualizations |\n------------------\n\n\ud83c\udfa8\ud83d\udcfd\u23f0 | .animate_lightcurves()       \n   Animate a sequence of light curves across different wavelengths.\n\ud83c\udfa8\ud83d\udcfd\ud83c\udf0a | .animate_spectra()           \n   Animate a sequence of spectra across different times.\n\ud83c\udfa8\ud83d\uddbc\ud83d\udcfa | .imshow()                    \n   Paint a map of flux across wavelength and time.\n\ud83c\udfa8\ud83d\udd79\ud83d\udcfa | .imshow_interact()           \n   Show flux map and lightcurves with interactive wavelength selection.\n\ud83c\udfa8\ud83d\udd8c\ud83d\udcfa | .pcolormesh()                \n   Paint a map of flux across wavelength and time (with non-uniform grids).\n\ud83c\udfa8\ud83d\udd8c\ud83e\uddf6 | .plot()                      \n   Plot a sequence of light curves with vertical offsets.\n\n------------------------------\n| visualizations/diagnostics |\n------------------------------\n\n\ud83c\udfa8\ud83d\uddc2\ud83d\udcfa | .paint_quantities()          \n   Show multiple 2D (wavelength and time) quantities as imshow maps.\n\ud83c\udfa8\ud83d\uddc2\ud83e\uddf6 | .plot_quantities()           \n   Show multiple 1D (wavelength or time) quantities as scatter plots.\n\n---------------------------\n| visualizations/timelike |\n---------------------------\n\n\ud83c\udfa8\u23f0\ud83d\udc0b | .plot_average_lightcurve()   \n   Plot the weighted average flux per time.\n\ud83c\udfa8\u23f0\ud83d\udd96 | .plot_median_lightcurve()    \n   Plot the median flux per time.\n\n---------------------------\n| visualizations/wavelike |\n---------------------------\n\n\ud83c\udfa8\ud83c\udf0a\ud83d\udd2d | .plot_average_spectrum()     \n   Plot the weighted average flux per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83d\udc8e | .plot_spectral_resolution()  \n   Plot the spectral resolution per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83c\udfa7 | .plot_noise_comparison()     \n   Plot the measured and expected scatter per wavelength.\n\ud83c\udfa8\ud83c\udf0a\ud83e\udd66 | .plot_noise_comparison_in_bins() \n   Plot measured and expected scatter in different size bins.\n</pre>"},{"location":"quickstart/#quickstart","title":"\ud83c\udf08 Quickstart\u00b6","text":"<p>This page shows how to load a time-series spectroscopic dataset, do some basic calculations with it, generate some visualizations, and then save it out to another format.</p>"},{"location":"quickstart/#download","title":"\ud83d\udcbe Download\u00b6","text":"<p>Let's download the JWST Early Release Observation of WASP-96b, one of first exoplanet transit datasets to be gathered by the telescope. We'll get the default pipeline <code>x1dints</code> (Stage 2) outputs; there are lots of reasons why we shouldn't use these particular pipeline files for science, but they're useful for a quick initial look.</p>"},{"location":"quickstart/#read","title":"\ud83e\uddd1\u200d\ud83d\udcbb Read\u00b6","text":"<p>Next, let's load that transit dataset into a <code>Rainbow</code> (\ud83c\udf08) object. These <code>chromatic</code> \ud83c\udf08 objects keep track of how the brightness of source changes across both wavelength and time.</p>"},{"location":"quickstart/#calculate","title":"\ud83e\uddee Calculate\u00b6","text":""},{"location":"quickstart/#visualize","title":"\ud83c\udfa8 Visualize\u00b6","text":""},{"location":"quickstart/#build","title":"\ud83e\uddf6 Build\u00b6","text":"<p>Because many of the actions possible with \ud83c\udf08 objects return other \ud83c\udf08 objects, it's possible to connect multiple steps into a single command, building up complicated analysis stories with relatively succinct code.</p>"},{"location":"quickstart/#save","title":"\ud83d\udcbe Save\u00b6","text":"<p>Let's convert these data into a different format by saving it as a new file, which we might send around to share with our colleagues or publish along with a paper. <code>chromatic</code> can read and save \ud83c\udf08 datasets with a variety of formats, to try to ease collaboration across different pipelines and toolkits.</p>"},{"location":"quickstart/#learn","title":"\ud83d\udcda Learn\u00b6","text":"<p>That's it! This quick tutorial highlighted <code>chromatic</code>'s abilities to...</p> <ul> <li>load in time-series spectra or multiwavelength light curves from formats like <code>x1dints</code></li> <li>access core data variables like <code>wavelength</code>, <code>time</code>, <code>flux</code>, <code>uncertainty</code></li> <li>perform calculations like <code>.normalize</code>, <code>.bin</code>, <code>.fold</code></li> <li>visualize the data with <code>.paint</code>, <code>.imshow_interact</code>, <code>.plot</code></li> </ul> <p>Hopefully, you're now curious to read through the User Guide to learn more about options for reading \ud83c\udf08s, doing actions with \ud83c\udf08s, visualizing \ud83c\udf08s in different ways, and more! You can also run the <code>.help()</code> method associated with any \ud83c\udf08 object to get a quick summary of what other methods are available for it&gt;</p>"},{"location":"visualizing/","title":"\ud83c\udf08 Visualizations","text":"In\u00a0[1]: Copied! <pre>from chromatic import SimulatedRainbow, version\nfrom chromatic import plt, np, u\n</pre> from chromatic import SimulatedRainbow, version from chromatic import plt, np, u In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> <p>We'll demonstrate some of the standard visualizations by generating a simulated rainbow with a cute wavelength-dependent transit signal injected into it.</p> In\u00a0[3]: Copied! <pre>s = SimulatedRainbow(dw=0.1 * u.micron, dt=4 * u.minute)\n\n# inject an interesting transit signal\ntheta = np.linspace(0, 2 * np.pi, s.nwave)\nplanet_radius = np.sin(theta) * 0.05 + 0.2\nr = s.inject_transit(planet_radius=planet_radius).inject_noise()\n</pre> s = SimulatedRainbow(dw=0.1 * u.micron, dt=4 * u.minute)  # inject an interesting transit signal theta = np.linspace(0, 2 * np.pi, s.nwave) planet_radius = np.sin(theta) * 0.05 + 0.2 r = s.inject_transit(planet_radius=planet_radius).inject_noise() In\u00a0[4]: Copied! <pre>r.paint();\n</pre> r.paint(); <p>There are a few options you might want to change.</p> In\u00a0[5]: Copied! <pre>r.paint(cmap=\"gray\", colorbar=False, w_unit=\"nm\", t_unit=\"minute\");\n</pre> r.paint(cmap=\"gray\", colorbar=False, w_unit=\"nm\", t_unit=\"minute\"); <p>Or, you might want to plot a different quantity instead.</p> In\u00a0[6]: Copied! <pre>r.paint(quantity=\"model\");\n</pre> r.paint(quantity=\"model\"); In\u00a0[7]: Copied! <pre>r.imshow();\n</pre> r.imshow(); <p><code>.pcolormesh()</code> and <code>.scatter()</code> offer more flexibility for less uniform grids. <code>.paint()</code> will automatically choose between <code>.imshow()</code> and <code>.pcolormesh()</code>.</p> In\u00a0[8]: Copied! <pre>r.imshow_interact();\n</pre> r.imshow_interact(); <p>Like <code>imshow()</code>, there are options you might like to change (see Vega documentation for color schemes):</p> In\u00a0[9]: Copied! <pre>r.imshow_interact(cmap=\"magma\", t_unit=\"h\", w_unit=\"nm\", ylim=[0.93, 1.01])\n</pre> r.imshow_interact(cmap=\"magma\", t_unit=\"h\", w_unit=\"nm\", ylim=[0.93, 1.01]) In\u00a0[10]: Copied! <pre>r.pcolormesh();\n</pre> r.pcolormesh(); In\u00a0[11]: Copied! <pre>r.pcolormesh()\nplt.yscale(\"log\")\n</pre> r.pcolormesh() plt.yscale(\"log\") <p><code>.imshow</code> is faster for uniform grids, and <code>.scatter</code> offer more flexibility for less uniform grids. <code>.paint()</code> will automatically choose between <code>.imshow()</code> and <code>.pcolormesh()</code>.</p> In\u00a0[12]: Copied! <pre>r.scatter()\n</pre> r.scatter() <pre>\ud83c\udf08\ud83e\udd16 Times are linearly spaced, and wavelengths are linearly spaced.\nIt's likely that `.imshow` or `.pcolormesh` are better ways to display this Rainbow.\n\n</pre> <p><code>.scatter</code> is often a good way to quickly display <code>Rainbow</code> objects with wavelengths and times that are sparse or extremely non-uniform. <code>.imshow</code> and <code>.pcolormesh</code> will be faster and may look more professional for smooth, contiguous wavelength and time grids.</p> In\u00a0[13]: Copied! <pre>r.plot();\n</pre> r.plot(); <p>Eep! Since it plots an individual light curve for each wavelength, plotting lots of wavelengths at once will cause things to overlap and blend together. You might want to bin your rainbow before plotting it.</p> In\u00a0[14]: Copied! <pre>r.bin(R=3).plot();\n</pre> r.bin(R=3).plot(); <p>There are a few options you might want to change.</p> In\u00a0[15]: Copied! <pre>r.bin(R=3).plot(\n    cmap=\"copper\",\n    w_unit=\"nm\",\n    t_unit=\"minute\",\n    plotkw=dict(marker=\"s\", markersize=2, linewidth=0),\n    textkw=dict(color=\"black\", fontweight=\"bold\"),\n)\n</pre> r.bin(R=3).plot(     cmap=\"copper\",     w_unit=\"nm\",     t_unit=\"minute\",     plotkw=dict(marker=\"s\", markersize=2, linewidth=0),     textkw=dict(color=\"black\", fontweight=\"bold\"), ) Out[15]: <pre>&lt;Axes: xlabel='Time ($\\\\mathrm{min}$)', ylabel='Relative Flux (+ offsets)'&gt;</pre> <p>This visualization returns an <code>ax</code> object, which is the Axes where the plot was generated. You can feed that <code>ax</code> into another plot command to overplot on top.</p> In\u00a0[16]: Copied! <pre>ax = r.bin(R=3).plot(\n    errorbar=False, plotkw=dict(alpha=0.2, markeredgecolor=\"none\", linewidth=0)\n)\nr.bin(R=3, dt=15 * u.minute).plot(ax=ax);\n</pre> ax = r.bin(R=3).plot(     errorbar=False, plotkw=dict(alpha=0.2, markeredgecolor=\"none\", linewidth=0) ) r.bin(R=3, dt=15 * u.minute).plot(ax=ax); In\u00a0[17]: Copied! <pre>r.animate_lightcurves();\n</pre> r.animate_lightcurves(); <pre>&lt;IPython.core.display.Image object&gt;</pre> In\u00a0[18]: Copied! <pre>r.animate_spectra();\n</pre> r.animate_spectra(); <pre>&lt;IPython.core.display.Image object&gt;</pre> In\u00a0[19]: Copied! <pre>r.paint_quantities(maxcol=4);\n</pre> r.paint_quantities(maxcol=4); In\u00a0[20]: Copied! <pre>r.plot_quantities(xaxis=\"time\");\n</pre> r.plot_quantities(xaxis=\"time\"); In\u00a0[21]: Copied! <pre>r.plot_quantities(xaxis=\"wavelength\");\n</pre> r.plot_quantities(xaxis=\"wavelength\");"},{"location":"visualizing/#visualizations","title":"\ud83c\udf08 Visualizations\u00b6","text":"<p><code>chromatic</code> provides built-in visualizations for \ud83c\udf08 datasets, trying to make it easier to look closely at complicated data and to facilitate standardized comparisons across different analyses.</p>"},{"location":"visualizing/#paint","title":"\ud83c\udf08.paint()\u00b6","text":"<p>Display the flux as an image, with each pixel representing the flux for a particular time and wavelength, automatically using the best option between <code>.imshow()</code> and <code>.pcolormesh()</code> (described in more detail below).</p>"},{"location":"visualizing/#imshow","title":"\ud83c\udf08.imshow()\u00b6","text":"<p>Display the flux as an image, with each pixel representing the flux for a particular time and wavelength, using a constant pixel grid. <code>.imshow()</code> works best for <code>Rainbow</code> objects with wavelengths and times that are approximately uniformly spaced, either linearly or logarithmically.</p>"},{"location":"visualizing/#imshow_interact","title":"\ud83c\udf08.imshow_interact()\u00b6","text":"<p>Display the flux as an image on the left, with the ability to drag and select which wavelengths you would like to appear in the light curve plot on the right.</p>"},{"location":"visualizing/#pcolormesh","title":"\ud83c\udf08.pcolormesh()\u00b6","text":"<p>Display the flux as an image, with each pixel representing the flux for a particular time and wavelength, but using <code>plt.pcolormesh</code> which allows pixels to stretch and squeeze based on the actual locations of their edges. Use <code>.imshow()</code> if you want wavelength/time bins to appear with the uniform sizes; use <code>.pcolormesh()</code> if you want to allow them to transform with the axes or non-uniform edges. It accepts most of the same options as <code>.imshow()</code>.</p>"},{"location":"visualizing/#scatter","title":"\ud83c\udf08.scatter()\u00b6","text":"<p>Display the flux as a sparesely populated image, with each pixel representing the flux for a particular time and wavelength, using <code>plt.scatter</code> to paint fluxes only where valid wavelength and time points exist. It accepts most of the same options as <code>.imshow()</code>.</p>"},{"location":"visualizing/#plot","title":"\ud83c\udf08.plot()\u00b6","text":"<p>Display the flux by plotting a sequence of light curves for different wavelengths, with each curve representing the flux for a particular wavelength.</p>"},{"location":"visualizing/#animate_lightcurves","title":"\ud83c\udf08.animate_lightcurves()\u00b6","text":"<p>Display the flux by animating a sequence of light curve plots, flipping through different wavelengths.</p>"},{"location":"visualizing/#animate_spectra","title":"\ud83c\udf08.animate_spectra()\u00b6","text":"<p>Display the flux by animating a sequence of spectrum plots, flipping through different times.</p>"},{"location":"visualizing/#paint_quantities","title":"\ud83c\udf08.paint_quantities()\u00b6","text":"<p>Visualize all the <code>imshow</code>-able quantities (things with same shape as <code>flux</code>).</p>"},{"location":"visualizing/#plot_quantities","title":"\ud83c\udf08.plot_quantities()\u00b6","text":"<p>Plot quantities that can be expressed as time series or as spectra (things with same shape as <code>time</code> or <code>wavelength</code>).</p>"},{"location":"visualizing/#how-do-we-do-more-with-these-plots","title":"How do we do more with these plots?\u00b6","text":"<p>Whatever plots you make, you can continue to call additional <code>plt</code> commands after the plot has been generated. For example, you could add a title with <code>plt.title()</code> or save the figure with <code>plt.savefig(filename)</code>.</p>"},{"location":"visualizing/#whats-next","title":"What's next?\u00b6","text":"<p>To find other visualization options for your \ud83c\udf08 object, run the <code>.help()</code> and try things out! If you would like another kind of a visualization that doesn't already exist, please submit an Issue to discuss it.</p>"},{"location":"tools/binning/","title":"Binning Data to a New Grid","text":"In\u00a0[1]: Copied! <pre>from chromatic import bintogrid, bintoR, version\nimport numpy as np, matplotlib.pyplot as plt\n\nplt.matplotlib.rcParams[\"figure.figsize\"] = (8, 3)\nplt.matplotlib.rcParams[\"figure.dpi\"] = 300\n</pre> from chromatic import bintogrid, bintoR, version import numpy as np, matplotlib.pyplot as plt  plt.matplotlib.rcParams[\"figure.figsize\"] = (8, 3) plt.matplotlib.rcParams[\"figure.dpi\"] = 300 In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> <p>Let's create a fake input dataset with an input grid that is uniformly spaced in <code>x</code>.</p> In\u00a0[3]: Copied! <pre>N = 100\nx = np.linspace(1, 5, N)\ny = x**2\n</pre> N = 100 x = np.linspace(1, 5, N) y = x**2 <p>Then, let's use <code>bintogrid</code> to bin these input arrays onto a new grid with wider spacing. The results of this function are a dictionary that contain:</p> <ul> <li><code>x</code> = the center of the output grid</li> <li><code>y</code> = the resampled value on the output grid</li> <li><code>x_edge_lower</code> = the lower edges of the output grid</li> <li><code>x_edge_upper</code> = the upper edges of the output grid</li> <li><code>N_unbinned/N_binned</code> = the approximate number of input bins that contributed to each output bin</li> </ul> In\u00a0[4]: Copied! <pre>binned = bintogrid(x, y, dx=0.5)\nlist(binned.keys())\n</pre> binned = bintogrid(x, y, dx=0.5) list(binned.keys()) Out[4]: <pre>['x', 'x_edge_lower', 'x_edge_upper', 'y', 'N_unbinned/N_binned']</pre> <p>Let's compare the results on a plot. The resampled values line up very neatly with the</p> In\u00a0[5]: Copied! <pre>plt.scatter(x, y, alpha=0.5, label=\"input\")\nplt.scatter(binned[\"x\"], binned[\"y\"], s=100, alpha=0.5, label=\"output\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(frameon=False);\n</pre> plt.scatter(x, y, alpha=0.5, label=\"input\") plt.scatter(binned[\"x\"], binned[\"y\"], s=100, alpha=0.5, label=\"output\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend(frameon=False); <p>This code should work similarly even if the input arrays are non-uniform in <code>x</code>, which can be a nice way to arrange a heterogeneous dataset into something easier to work with.</p> In\u00a0[6]: Copied! <pre>x = np.sort(np.random.uniform(1, 5, N))\ny = x**2\nbinned = bintogrid(x, y, dx=0.5)\nplt.scatter(x, y, alpha=0.5, label=\"input\")\nplt.scatter(binned[\"x\"], binned[\"y\"], s=100, alpha=0.5, label=\"output\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(frameon=False);\n</pre> x = np.sort(np.random.uniform(1, 5, N)) y = x**2 binned = bintogrid(x, y, dx=0.5) plt.scatter(x, y, alpha=0.5, label=\"input\") plt.scatter(binned[\"x\"], binned[\"y\"], s=100, alpha=0.5, label=\"output\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend(frameon=False); <p>The <code>bintoR</code> function is a wrapper around <code>bintogrid</code> that provides a quick way to bin onto logarithmic grid. In spectroscopy, it's common to want to work with wavelengths $\\lambda$ that are spaced according to a constant value of $R = \\lambda/\\Delta \\lambda = 1 / \\Delta [\\ln \\lambda]$. This quantity $R$ is often called the spectral resolution.</p> In\u00a0[7]: Copied! <pre>binned = bintoR(x, y, R=10)\nplt.scatter(x, y, alpha=0.5, label=\"input\")\nplt.scatter(binned[\"x\"], binned[\"y\"], s=100, alpha=0.5, label=\"output\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(frameon=False);\n</pre> binned = bintoR(x, y, R=10) plt.scatter(x, y, alpha=0.5, label=\"input\") plt.scatter(binned[\"x\"], binned[\"y\"], s=100, alpha=0.5, label=\"output\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend(frameon=False); <p>This may seem like a weird way to define a new output grid, but its usefulness becomes apparent when we plot on logarithmic axes. It's uniform in log space!</p> In\u00a0[8]: Copied! <pre>plt.scatter(x, y, alpha=0.5, label=\"input\")\nplt.scatter(binned[\"x\"], binned[\"y\"], s=100, alpha=0.5, label=\"output\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(frameon=False)\nplt.xscale(\"log\");\n</pre> plt.scatter(x, y, alpha=0.5, label=\"input\") plt.scatter(binned[\"x\"], binned[\"y\"], s=100, alpha=0.5, label=\"output\") plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend(frameon=False) plt.xscale(\"log\"); <p>For any real measurements, there are probably uncertainties associated with them. Both <code>bintogrid</code> and <code>bintoR</code> will try their best to propagate uncertainties by using inverse-variance weighting and its maximum likelihood estimate for the binned uncertainty.</p> <p>Let's look at a similar example as before, but with some uncertainties associated with each input <code>y</code> value.</p> In\u00a0[9]: Copied! <pre>x = np.linspace(1, 5, N)\nuncertainty = np.ones_like(x) * 5\ny = np.random.normal(x**2, uncertainty)\n</pre> x = np.linspace(1, 5, N) uncertainty = np.ones_like(x) * 5 y = np.random.normal(x**2, uncertainty) <p>Let's resample it to a new grid, providing the known uncertainties on the original points. Notice that the result now also includes an <code>uncertainty</code> key.</p> In\u00a0[10]: Copied! <pre>binned = bintogrid(x, y, uncertainty, dx=0.5)\nlist(binned.keys())\n</pre> binned = bintogrid(x, y, uncertainty, dx=0.5) list(binned.keys()) Out[10]: <pre>['x',\n 'x_edge_lower',\n 'x_edge_upper',\n 'y',\n 'uncertainty',\n 'N_unbinned/N_binned']</pre> <p>When we plot the input and output values, we can see that the typical output uncertainties are smaller than the typical input uncertainties, because we've effectively averaged together a few data points and therefore decreased the uncertainty for the new values.</p> In\u00a0[11]: Copied! <pre>kw = dict(linewidth=0, elinewidth=1, marker=\"o\", alpha=0.5, markeredgecolor=\"none\")\nplt.errorbar(x, y, uncertainty, label=\"input\", **kw)\nplt.errorbar(\n    binned[\"x\"], binned[\"y\"], binned[\"uncertainty\"], label=\"output\", markersize=10, **kw\n)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(frameon=False);\n</pre> kw = dict(linewidth=0, elinewidth=1, marker=\"o\", alpha=0.5, markeredgecolor=\"none\") plt.errorbar(x, y, uncertainty, label=\"input\", **kw) plt.errorbar(     binned[\"x\"], binned[\"y\"], binned[\"uncertainty\"], label=\"output\", markersize=10, **kw ) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend(frameon=False); <p>When we bin onto a logarithmic grid with <code>bintoR</code>, we can see that the uncertainties typically smaller for the higher values of <code>x</code>, where more input points are getting averaged together to make each output point.</p> In\u00a0[12]: Copied! <pre>binned = bintoR(x, y, uncertainty, R=10)\nplt.errorbar(x, y, uncertainty, label=\"input\", **kw)\nplt.errorbar(\n    binned[\"x\"], binned[\"y\"], binned[\"uncertainty\"], label=\"output\", markersize=10, **kw\n)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(frameon=False)\nplt.xscale(\"log\");\n</pre> binned = bintoR(x, y, uncertainty, R=10) plt.errorbar(x, y, uncertainty, label=\"input\", **kw) plt.errorbar(     binned[\"x\"], binned[\"y\"], binned[\"uncertainty\"], label=\"output\", markersize=10, **kw ) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend(frameon=False) plt.xscale(\"log\");"},{"location":"tools/binning/#binning-data-to-a-new-grid","title":"Binning Data to a New Grid\u00b6","text":"<p>Often we have some <code>y</code> values the correspond to a particular grid of <code>x</code> values, and we want to resample them onto a different grid of <code>x</code> values. Interpolation is one way to do this, but it won't necessarily provide reasonable averages over wiggly features. There are lots of different features we might hope for in a resampling routine, but one common one is that we'd like get the same answer when integrating between two <code>x</code> limits, whether we're using the original or the resampled grid of quantities. One way to ensure such integrals are conserved is to calculate the cumulative distribution function (= the integral up to a particular limit) of the original arrays, interpolate onto the new grid, and differentiate; Diamond-Lowe et al. 2020 provides a literature example of this algorthim being used for exoplanet transmission spectrum observations.</p> <p>For <code>chromatic</code>, the tools used to achieve this are the <code>bintogrid</code> and <code>bintoR</code> functions, which we demonstrate below.</p>"},{"location":"tools/binning/#how-do-we-bin-some-input-arrays","title":"How do we bin some input arrays?\u00b6","text":""},{"location":"tools/binning/#how-do-we-bin-with-uncertainties","title":"How do we bin with uncertainties?\u00b6","text":""},{"location":"tools/binning/#how-do-we-customize-the-output-grid","title":"How do we customize the output grid?\u00b6","text":"<p>There are a few different options you can use to specify the exact output grid you would like.</p> <p>For <code>bintogrid</code>, the options are:</p> <ul> <li><code>nx</code> = the number of adjacent inputs points that should be binned together to create the output grid (for example, \"bin every 3 points together\")</li> <li><code>dx</code> = the spacing for a linearly-uniform output grid</li> <li><code>newx</code> = a custom output grid, referring to the centers of the new bins</li> <li><code>newx_edges</code>= a custom output grid, referring to the edges of the new bins. The left and right edges of the bins will be, respectively, <code>newx_edges[:-1]</code> and <code>newx_edges[1:]</code>, so the size of the output array will be <code>len(newx_edges) - 1</code></li> </ul> <p>For <code>bintoR</code>, the options are:</p> <ul> <li><code>R</code> = the spectral resolution R=x/dx for a logarithmically-uniform output grid</li> <li><code>xlim</code> = a two-element list indicating the min and max values of x for the new logarithmic output grid. If not supplied, this will center the first output bin on the first value of <code>x</code></li> </ul>"},{"location":"tools/colormaps/","title":"Creating Custom Colormaps","text":"In\u00a0[1]: Copied! <pre>from chromatic import one2another, version\n</pre> from chromatic import one2another, version In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> In\u00a0[3]: Copied! <pre>one2another(\"orchid\", \"black\")\n</pre> one2another(\"orchid\", \"black\") Out[3]: orchid2black  underbad over  In\u00a0[4]: Copied! <pre>one2another(\"black\", \"orchid\")\n</pre> one2another(\"black\", \"orchid\") Out[4]: black2orchid  underbad over  In\u00a0[5]: Copied! <pre>one2another(bottom=\"orchid\", top=\"orchid\", alpha_bottom=0)\n</pre> one2another(bottom=\"orchid\", top=\"orchid\", alpha_bottom=0) Out[5]: orchid2orchid  underbad over  <p>That's it! You can use these colormaps anywhere you'd set <code>cmap=</code> in a <code>matplotlib</code> function.</p> In\u00a0[6]: Copied! <pre>import numpy as np, matplotlib.pyplot as plt\n\n# make a custom cmap\nmy_fancy_cmap = one2another(\"orchid\", \"black\")\n\n# make some fake data\nx, y = np.random.uniform(-1, 1, [2, 1000])\nz = np.random.normal(0, 1, [10, 10])\n\n# use the cmap\nfi, ax = plt.subplots(1, 2, constrained_layout=True)\nax[0].scatter(x, y, c=x**2 + y**2, cmap=my_fancy_cmap)\nax[0].axis(\"scaled\")\nax[1].imshow(z, cmap=my_fancy_cmap)\nax[1].axis(\"scaled\");\n</pre> import numpy as np, matplotlib.pyplot as plt  # make a custom cmap my_fancy_cmap = one2another(\"orchid\", \"black\")  # make some fake data x, y = np.random.uniform(-1, 1, [2, 1000]) z = np.random.normal(0, 1, [10, 10])  # use the cmap fi, ax = plt.subplots(1, 2, constrained_layout=True) ax[0].scatter(x, y, c=x**2 + y**2, cmap=my_fancy_cmap) ax[0].axis(\"scaled\") ax[1].imshow(z, cmap=my_fancy_cmap) ax[1].axis(\"scaled\");"},{"location":"tools/colormaps/#creating-custom-colormaps","title":"Creating Custom Colormaps\u00b6","text":"<p>Colormaps provide a way to translate from numbers to colors. The <code>matplotlib</code> colormaps are lovely, but sometimes we just want to say \"I'd like a colormap that goes from this color to that color.\" The <code>one2another</code> colormap generator does just that.</p>"},{"location":"tools/colormaps/#how-do-we-make-a-new-colormap","title":"How do we make a new colormap?\u00b6","text":"<p>All we need to specify is the color at the bottom and the color at the top. If we want to get really fancy, we can specify different <code>alpha</code> (= opacity) values for either of these limits.</p>"},{"location":"tools/spectra/","title":"Retrieving Model Spectra","text":"<p>Let's imagine you want to plot the optical and near-infrared spectrum of a star with a particular effective temperature, surface gravity, and metallicity. The following commands show how to use <code>chromatic</code> to retrieve the model flux of a star like the Sun, making use of the <code>get_phoenix_photons</code> function.</p> In\u00a0[1]: Copied! <pre>from chromatic import get_phoenix_photons, get_planck_photons, version\nimport matplotlib.pyplot as plt, numpy as np\nimport astropy.units as u, astropy.constants as con\n</pre> from chromatic import get_phoenix_photons, get_planck_photons, version import matplotlib.pyplot as plt, numpy as np import astropy.units as u, astropy.constants as con In\u00a0[2]: Copied! <pre>version()\n</pre> version() Out[2]: <pre>'0.5.0'</pre> In\u00a0[3]: Copied! <pre># retrieve a wavelength array and a flux array\nwavelength, surface_flux = get_phoenix_photons(\n    temperature=5780, logg=4.43, metallicity=0.0, R=1000\n)\n\n# plot the wavelength array\nplt.figure(figsize=(8, 3), dpi=300)\nplt.plot(wavelength, surface_flux, label=\"PHOENIX model\")\n\n# for comparison, plot a Planck spectrum\nw_planck, f_planck = get_planck_photons(temperature=5780)\nplt.plot(w_planck, f_planck, label=\"Planck function\")\n\n# add some labels\nplt.xlabel(f\"Wavelength ({wavelength.unit.to_string('latex_inline')})\")\nplt.ylabel(f\"Surface Flux ({surface_flux.unit.to_string('latex_inline')})\")\nplt.legend(frameon=False);\n</pre> # retrieve a wavelength array and a flux array wavelength, surface_flux = get_phoenix_photons(     temperature=5780, logg=4.43, metallicity=0.0, R=1000 )  # plot the wavelength array plt.figure(figsize=(8, 3), dpi=300) plt.plot(wavelength, surface_flux, label=\"PHOENIX model\")  # for comparison, plot a Planck spectrum w_planck, f_planck = get_planck_photons(temperature=5780) plt.plot(w_planck, f_planck, label=\"Planck function\")  # add some labels plt.xlabel(f\"Wavelength ({wavelength.unit.to_string('latex_inline')})\") plt.ylabel(f\"Surface Flux ({surface_flux.unit.to_string('latex_inline')})\") plt.legend(frameon=False); <p>The solar spectrum looks approximately like the Planck thermal emission spectrum, but with some absorption features and redistribution of flux due radiative transfer through the stellar atmosphere.</p> In\u00a0[4]: Copied! <pre>plt.figure(figsize=(8, 3), dpi=300)\nfor R in [10000, 1000, 100, 10]:\n    w, f = get_phoenix_photons(R=R)\n    plt.plot(w, f, alpha=0.5, label=f\"R={R}\")\nplt.xlabel(f\"Wavelength ({wavelength.unit.to_string('latex_inline')})\")\nplt.ylabel(f\"Surface Flux ({surface_flux.unit.to_string('latex_inline')})\")\nplt.legend(frameon=False);\n</pre> plt.figure(figsize=(8, 3), dpi=300) for R in [10000, 1000, 100, 10]:     w, f = get_phoenix_photons(R=R)     plt.plot(w, f, alpha=0.5, label=f\"R={R}\") plt.xlabel(f\"Wavelength ({wavelength.unit.to_string('latex_inline')})\") plt.ylabel(f\"Surface Flux ({surface_flux.unit.to_string('latex_inline')})\") plt.legend(frameon=False); In\u00a0[5]: Copied! <pre>plt.figure(figsize=(8, 3), dpi=300)\nw, f = get_phoenix_photons(wavelength=np.linspace(0.4, 0.7, 1000) * u.micron)\nplt.plot(w, f)\nplt.xlabel(f\"Wavelength ({wavelength.unit.to_string('latex_inline')})\")\nplt.ylabel(f\"Surface Flux ({surface_flux.unit.to_string('latex_inline')})\");\n</pre> plt.figure(figsize=(8, 3), dpi=300) w, f = get_phoenix_photons(wavelength=np.linspace(0.4, 0.7, 1000) * u.micron) plt.plot(w, f) plt.xlabel(f\"Wavelength ({wavelength.unit.to_string('latex_inline')})\") plt.ylabel(f\"Surface Flux ({surface_flux.unit.to_string('latex_inline')})\"); <pre>\n            Downloading pre-processed grid for R=3000, metallicity=0.0 from\n            https://casa.colorado.edu/~bertathompson/chromatic/phoenix_photons_metallicity=0.0_R=3000.npy\n            Because the resolution is R&gt;1000, this might be annoyingly slow.\n            \n</pre> <pre>\ud83c\udf08\ud83e\udd16 The progress bar for this download is not being shown\nbecause `astropy.utils.data.download_file` is being run\nfrom a jupyter notebook instead of from the terminal.\n\n</pre> In\u00a0[6]: Copied! <pre>wavelength.unit\n</pre> wavelength.unit Out[6]:  $\\mathrm{\\mu m}$  In\u00a0[7]: Copied! <pre>surface_flux.unit\n</pre> surface_flux.unit Out[7]:  $\\mathrm{\\frac{ph}{nm\\,s\\,m^{2}}}$  <p>Because <code>chromatic</code> primarily uses these model spectra for photon-noise calculations, the flux is provided in photons/second rather than energy units like joules/second (= watts) or ergs/second.</p> In\u00a0[8]: Copied! <pre># retrieve a wavelength array and a flux array\nwavelength, surface_flux = get_phoenix_photons(\n    temperature=5780, logg=4.43, metallicity=0.0, R=1000\n)\n\n# calculate the energy per photon for each wavelength [J/photon]\nenergy_per_photon = (con.h * con.c / wavelength) / u.photon\n\n# calculate the surface flux in power units [W/(m**2 nm)]\nsurface_flux_power = energy_per_photon * surface_flux\n\n# integrate over wavelength to get bolometric surface flux [W/m**2]\nok = np.isfinite(surface_flux_power)\nbolometric_surface_flux_power = np.trapz(surface_flux_power[ok], wavelength[ok])\n\n# calculate the surface area of the Sun [m**2]\nsurface_area = 4 * np.pi * (1 * u.Rsun) ** 2\n\n# calculate the luminosity as bolometric flux * area\nluminosity = bolometric_surface_flux_power * surface_area\nluminosity.to(u.Lsun)\n</pre> # retrieve a wavelength array and a flux array wavelength, surface_flux = get_phoenix_photons(     temperature=5780, logg=4.43, metallicity=0.0, R=1000 )  # calculate the energy per photon for each wavelength [J/photon] energy_per_photon = (con.h * con.c / wavelength) / u.photon  # calculate the surface flux in power units [W/(m**2 nm)] surface_flux_power = energy_per_photon * surface_flux  # integrate over wavelength to get bolometric surface flux [W/m**2] ok = np.isfinite(surface_flux_power) bolometric_surface_flux_power = np.trapz(surface_flux_power[ok], wavelength[ok])  # calculate the surface area of the Sun [m**2] surface_area = 4 * np.pi * (1 * u.Rsun) ** 2  # calculate the luminosity as bolometric flux * area luminosity = bolometric_surface_flux_power * surface_area luminosity.to(u.Lsun) <pre>/var/folders/_4/bcdg2gg10bl4p015nxzlhq_40000gp/T/ipykernel_36993/1302982006.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n  bolometric_surface_flux_power = np.trapz(surface_flux_power[ok], wavelength[ok])\n</pre> Out[8]:  $0.99032735 \\; \\mathrm{L_{\\odot}}$  <p>To review, we converted from photons to energy, integrated over wavelength, and multiplied by the surface area of the Sun. Our estimate for the luminosity of the Sun came out pretty close, especially considering we didn't actually integrate the wavelengths from $\\lambda=0$ to $\\lambda=\\infty$.</p> In\u00a0[9]: Copied! <pre>fi, ax = plt.subplots(1, 2, figsize=(8, 3), dpi=300, constrained_layout=True)\n\n# loop over two different wavelengths\nfor a, i in zip(ax, [10, -10]):\n\n    # point to the appropriate plotting axes\n    plt.sca(a)\n\n    # plot flux for the original grid points\n    grid_temperatures = np.arange(4000, 4400, 100)\n    grid_fluxes = []\n    for T in grid_temperatures:\n        w, f = get_phoenix_photons(temperature=T, logg=4.5)\n        grid_fluxes.append(f)\n    plt.plot(grid_temperatures, np.array(grid_fluxes)[:, i], marker=\"o\", label=\"grid\")\n\n    # plot flux for interpolated temperatures\n    interpolated_temperatures = np.arange(4000, 4300, 11)\n    interpolated_fluxes = []\n    for T in interpolated_temperatures:\n        w, f = get_phoenix_photons(temperature=T, logg=4.5)\n        interpolated_fluxes.append(f)\n    plt.plot(\n        interpolated_temperatures,\n        np.array(interpolated_fluxes)[:, i],\n        marker=\".\",\n        label=\"interpolated\",\n    )\n\n    # add some labels\n    plt.title(f\"$\\lambda$={w[i]:.3f}\")\n    plt.xlabel(\"Temperature (K)\")\nplt.sca(ax[0])\nplt.ylabel(f\"Monochromatic Flux\\n({f.unit.to_string('latex_inline')})\")\nplt.legend(frameon=False);\n</pre> fi, ax = plt.subplots(1, 2, figsize=(8, 3), dpi=300, constrained_layout=True)  # loop over two different wavelengths for a, i in zip(ax, [10, -10]):      # point to the appropriate plotting axes     plt.sca(a)      # plot flux for the original grid points     grid_temperatures = np.arange(4000, 4400, 100)     grid_fluxes = []     for T in grid_temperatures:         w, f = get_phoenix_photons(temperature=T, logg=4.5)         grid_fluxes.append(f)     plt.plot(grid_temperatures, np.array(grid_fluxes)[:, i], marker=\"o\", label=\"grid\")      # plot flux for interpolated temperatures     interpolated_temperatures = np.arange(4000, 4300, 11)     interpolated_fluxes = []     for T in interpolated_temperatures:         w, f = get_phoenix_photons(temperature=T, logg=4.5)         interpolated_fluxes.append(f)     plt.plot(         interpolated_temperatures,         np.array(interpolated_fluxes)[:, i],         marker=\".\",         label=\"interpolated\",     )      # add some labels     plt.title(f\"$\\lambda$={w[i]:.3f}\")     plt.xlabel(\"Temperature (K)\") plt.sca(ax[0]) plt.ylabel(f\"Monochromatic Flux\\n({f.unit.to_string('latex_inline')})\") plt.legend(frameon=False); <pre>&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/_4/bcdg2gg10bl4p015nxzlhq_40000gp/T/ipykernel_36993/1642702033.py:31: SyntaxWarning: invalid escape sequence '\\l'\n  plt.title(f\"$\\lambda$={w[i]:.3f}\")\n</pre> In\u00a0[10]: Copied! <pre>from chromatic import phoenix_library\n</pre> from chromatic import phoenix_library In\u00a0[11]: Copied! <pre>phoenix_library.get_cache_dir()\n</pre> phoenix_library.get_cache_dir() Out[11]: <pre>'/Users/zabe0091/.chromatic/cache'</pre> In\u00a0[12]: Copied! <pre>phoenix_library.get_cache_size()\n</pre> phoenix_library.get_cache_size() Out[12]:  $3.623118 \\; \\mathrm{Gbyte}$  In\u00a0[13]: Copied! <pre>phoenix_library.plot_time_required();\n</pre> phoenix_library.plot_time_required(); <pre>\ud83c\udf08\ud83e\udd16 The progress bar for this download is not being shown\nbecause `astropy.utils.data.download_file` is being run\nfrom a jupyter notebook instead of from the terminal.\n\n</pre> <pre>\n            Downloading pre-processed grid for R=3, metallicity=0.0 from\n            https://casa.colorado.edu/~bertathompson/chromatic/phoenix_photons_metallicity=0.0_R=3.npy\n            Because the resolution is R&lt;=1000, this should be pretty quick.\n            \n\n            Downloading pre-processed grid for R=30, metallicity=0.0 from\n            https://casa.colorado.edu/~bertathompson/chromatic/phoenix_photons_metallicity=0.0_R=30.npy\n            Because the resolution is R&lt;=1000, this should be pretty quick.\n            \n</pre> <pre>\ud83c\udf08\ud83e\udd16 The progress bar for this download is not being shown\nbecause `astropy.utils.data.download_file` is being run\nfrom a jupyter notebook instead of from the terminal.\n\n</pre> <pre>\n            Downloading pre-processed grid for R=30000, metallicity=0.0 from\n            https://casa.colorado.edu/~bertathompson/chromatic/phoenix_photons_metallicity=0.0_R=30000.npy\n            Because the resolution is R&gt;1000, this might be annoyingly slow.\n            \n</pre> <pre>\ud83c\udf08\ud83e\udd16 The progress bar for this download is not being shown\nbecause `astropy.utils.data.download_file` is being run\nfrom a jupyter notebook instead of from the terminal.\n\n</pre>"},{"location":"tools/spectra/#retrieving-model-spectra","title":"Retrieving Model Spectra\u00b6","text":"<p>Often, we want to know the spectrum of light emanating from the surface of a star, a planet, or some other object. For <code>chromatic</code>, we added a wrapper around the Husser et al. (2013) library of high-resolution stellar spectra to make it easier to retrieve model spectra at a variety of resolutions. The tools used to achieve this is the <code>get_phoenix_photons</code> function, as demonstrated below.</p>"},{"location":"tools/spectra/#how-do-we-set-the-spectral-resolution","title":"How do we set the spectral resolution?\u00b6","text":"<p>If you specify a spectral resolution $R = \\lambda/d\\lambda$, the full extent of the model wavelength range (approximately $0.05-5 \\mu m$) will be returned, binned to that resolution. Different resolutions are stored in different files that are loaded as needed. Your code will run a lot faster if you load the lowest resolution that will meet your needs. For a single metallicity, files range from a very manageable 400 kilobytes for $R=10$ to an annoyingling large 3 gigabytes for $R=100000$. Trying to calculate an extremely high resolution model pre-existing metallicity grid point will try to load in all metallicities, which can cause slow-downs because of memory issues.</p>"},{"location":"tools/spectra/#should-we-specify-custom-wavelengths","title":"Should we specify custom wavelengths?\u00b6","text":"<p>For many applications, you may need to generate lots of spectra over a very narrow wavelength range. In those cases, it'd be inefficient to interpolate the a very large spectral range only to trim down to a few specific wavelengths. If you know exactly the wavelengths you want and you're generating more than one spectrum on the same wavelength grid, your code will run likely faster if you specify the wavelengths you need directly to <code>get_phoenix_photons</code>, as shown below.</p>"},{"location":"tools/spectra/#what-are-the-units","title":"What are the units?\u00b6","text":"<p>The variables <code>wavelength</code> and <code>surface_flux</code> returned by <code>get_phoenix_photons</code> have astropy units attached to them, to reduce the risk of unit mistakes in subsequent calculations.</p>"},{"location":"tools/spectra/#what-does-surface-flux-mean","title":"What does \"surface flux\" mean?\u00b6","text":"<p>In general, you could characterize the brightness of a star at lots of different locations: the surface of the star, at a fixed orbital distance from the star, or at some even larger astronomical distance from the star. The flux returned by <code>get_phoenix_photons</code> is a surface flux, in the sense that it represents the rate of photons flowing out of a star's photosphere per unit area. To show how this might be used, the following example calculation integrates a model spectrum to estimate the bolometric luminosity of the Sun.</p>"},{"location":"tools/spectra/#what-kind-of-interpolation-is-happening","title":"What kind of interpolation is happening?\u00b6","text":"<p>The PHOENIX model grid provides spectra at fixed intervals of temperature $T_{\\rm eff}$ (100-200K), surface gravity $\\log{g}$ (0.5 dex), and metallicity $[\\mathrm{Fe/H}]$ (0.5 dex). If you request a model that exists in that grid, it will be returned exactly. If you request a model somewhere between these grid points, it will be interpolated from the closest available grid points. The interpolation is linear in the quantities $\\log T_{\\rm eff}$, $\\log{g}$, and $[\\mathrm{Fe/H}]$. We interpolate in the logarithm of temperature because per-wavelength fluxes tend to grow $\\propto T_{\\rm eff}^{x}$ where $x\\ne 1$.</p>"},{"location":"tools/spectra/#where-are-the-spectra-stored","title":"Where are the spectra stored?\u00b6","text":"<p>The <code>get_phoenix_photons</code> function automatically downloads the files it needs and caches them on your local computer. You can find out where those files are stored and how much space their taking up by interacting with the <code>phoenix_library</code> object (of which <code>get_phoenix_photons</code> is a method).</p>"},{"location":"tools/spectra/#how-long-do-model-retrievals-take","title":"How long do model retrievals take?\u00b6","text":"<p>We put some effort into speeding up the process of retrieving multiple different spectral models on similar wavelength grids. The following plot shows approximately how long different steps take on a 2020 MacBook Pro with M1 chip.</p>"},{"location":"tools/transits/","title":"Making Model Transits","text":"In\u00a0[1]: Copied! <pre>from chromatic import exoplanet_transit, trapezoidal_transit\nimport numpy as np, matplotlib.pyplot as plt\n</pre> from chromatic import exoplanet_transit, trapezoidal_transit import numpy as np, matplotlib.pyplot as plt <p>The first argument to each of these model function is the array of times for which the flux should be computed, and the remaining keyword arguments allow you to change the planet parameters.</p> In\u00a0[2]: Copied! <pre>t = np.linspace(-0.1, 0.1, 1000)\nplt.figure(figsize=(8, 3))\nfor rp, c in zip([0.09, 0.1, 0.11], [\"orange\", \"red\", \"purple\"]):\n    limb_darkened_model = exoplanet_transit(t, rp=rp)\n    plt.plot(t, limb_darkened_model, color=c, label=f\"$R_p$ = {rp:.2f}\")\n    trapezoid_model = trapezoidal_transit(t, delta=rp**2)\n    plt.plot(t, trapezoid_model, color=c, linestyle=\"--\", label=f\"$R_p$ = {rp:.2f}\")\nplt.xlabel(\"Time (days)\")\nplt.ylabel(\"Relative Flux\")\nplt.legend(frameon=False);\n</pre> t = np.linspace(-0.1, 0.1, 1000) plt.figure(figsize=(8, 3)) for rp, c in zip([0.09, 0.1, 0.11], [\"orange\", \"red\", \"purple\"]):     limb_darkened_model = exoplanet_transit(t, rp=rp)     plt.plot(t, limb_darkened_model, color=c, label=f\"$R_p$ = {rp:.2f}\")     trapezoid_model = trapezoidal_transit(t, delta=rp**2)     plt.plot(t, trapezoid_model, color=c, linestyle=\"--\", label=f\"$R_p$ = {rp:.2f}\") plt.xlabel(\"Time (days)\") plt.ylabel(\"Relative Flux\") plt.legend(frameon=False); <p>These two functions take different keywords, as explained in their docstrings.</p> In\u00a0[3]: Copied! <pre>exoplanet_transit?\n</pre> exoplanet_transit? In\u00a0[4]: Copied! <pre>trapezoidal_transit?\n</pre> trapezoidal_transit? <p>Have fun!</p>"},{"location":"tools/transits/#making-model-transits","title":"Making Model Transits\u00b6","text":"<p>When making simulated datasets, the <code>.inject_transit</code> action can use two different functions for making transits. We make those functions directly available, in case you want to use them for your own wonderful purposes.  The options are:</p> <ul> <li><code>exoplanet_transit</code> = limb-darkened transits generated with <code>exoplanet_core</code></li> <li><code>trapezoidal_transit</code> = a very simple non-limb-darkened trapezoidal transit as in Winn (2010)</li> </ul> <p>You should probably use <code>exoplanet_transit</code>, unless you have a good reason to want to use a cartoon trapezoid instead.</p>"},{"location":"tools/transits/","title":"\u00b6","text":""}]}